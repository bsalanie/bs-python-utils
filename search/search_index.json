{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"bs-python-utils \u00b6 Bernard Salani\u00e9's Python utilities.","title":"Home"},{"location":"index.html#bs-python-utils","text":"Bernard Salani\u00e9's Python utilities.","title":"bs-python-utils"},{"location":"Timer.html","text":"Timer module \u00b6 Utilities to time code: a Timer class that can be used as a context manager a timeit decorator for functions. Timer \u00b6 A timer that can be started, stopped, and reset as needed by the user. It keeps track of the total elapsed time in the elapsed attribute:: Examples: >>> with Timer () as t : >>> .... >>> print ( f \"... took { t . elapsed } seconds\" ) use Timer(time.process_time) to get only CPU time. can also do: Examples: >>> t = Timer () >>> t . start () >>> t . stop () >>> t . start () # will add to the same counter >>> t . stop () >>> print ( f \" { t . elapsed } seconds total\" ) Source code in bs_python_utils/Timer.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class Timer : \"\"\" A timer that can be started, stopped, and reset as needed by the user. It keeps track of the total elapsed time in the `elapsed` attribute:: Examples: >>> with Timer() as t: >>> .... >>> print(f\"... took {t.elapsed} seconds\") use `Timer(time.process_time)` to get only CPU time. can also do: Examples: >>> t = Timer() >>> t.start() >>> t.stop() >>> t.start() # will add to the same counter >>> t.stop() >>> print(f\"{t.elapsed} seconds total\") \"\"\" def __init__ ( self , func : Callable = time . perf_counter ) -> None : self . elapsed = 0.0 self . _func = func self . _start = None def start ( self ) -> None : if self . _start is not None : raise RuntimeError ( \"Already started\" ) self . _start = self . _func () def stop ( self ) -> None : if self . _start is None : raise RuntimeError ( \"Not started\" ) end = self . _func () self . elapsed += end - self . _start self . _start = None def reset ( self ) -> None : self . elapsed = 0.0 @property def running ( self ) -> bool : return self . _start is not None def __enter__ ( self ) -> Any : self . start () return self def __exit__ ( self , * args : Iterable ) -> None : self . stop () timeit ( func ) \u00b6 Decorator to time a function Source code in bs_python_utils/Timer.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def timeit ( func : Callable ) -> Callable : \"\"\" Decorator to time a function \"\"\" @wraps ( func ) def wrapper ( * args : Iterable , ** kwargs : dict ) -> Any : start = time . perf_counter () result = func ( * args , ** kwargs ) end = time . perf_counter () print ( f \" { func . __name__ } executed in { end - start : .3f } seconds\" ) return result return wrapper","title":"timing routines"},{"location":"Timer.html#timer-module","text":"Utilities to time code: a Timer class that can be used as a context manager a timeit decorator for functions.","title":"Timer module"},{"location":"Timer.html#bs_python_utils.Timer.Timer","text":"A timer that can be started, stopped, and reset as needed by the user. It keeps track of the total elapsed time in the elapsed attribute:: Examples: >>> with Timer () as t : >>> .... >>> print ( f \"... took { t . elapsed } seconds\" ) use Timer(time.process_time) to get only CPU time. can also do: Examples: >>> t = Timer () >>> t . start () >>> t . stop () >>> t . start () # will add to the same counter >>> t . stop () >>> print ( f \" { t . elapsed } seconds total\" ) Source code in bs_python_utils/Timer.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class Timer : \"\"\" A timer that can be started, stopped, and reset as needed by the user. It keeps track of the total elapsed time in the `elapsed` attribute:: Examples: >>> with Timer() as t: >>> .... >>> print(f\"... took {t.elapsed} seconds\") use `Timer(time.process_time)` to get only CPU time. can also do: Examples: >>> t = Timer() >>> t.start() >>> t.stop() >>> t.start() # will add to the same counter >>> t.stop() >>> print(f\"{t.elapsed} seconds total\") \"\"\" def __init__ ( self , func : Callable = time . perf_counter ) -> None : self . elapsed = 0.0 self . _func = func self . _start = None def start ( self ) -> None : if self . _start is not None : raise RuntimeError ( \"Already started\" ) self . _start = self . _func () def stop ( self ) -> None : if self . _start is None : raise RuntimeError ( \"Not started\" ) end = self . _func () self . elapsed += end - self . _start self . _start = None def reset ( self ) -> None : self . elapsed = 0.0 @property def running ( self ) -> bool : return self . _start is not None def __enter__ ( self ) -> Any : self . start () return self def __exit__ ( self , * args : Iterable ) -> None : self . stop ()","title":"Timer"},{"location":"Timer.html#bs_python_utils.Timer.timeit","text":"Decorator to time a function Source code in bs_python_utils/Timer.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def timeit ( func : Callable ) -> Callable : \"\"\" Decorator to time a function \"\"\" @wraps ( func ) def wrapper ( * args : Iterable , ** kwargs : dict ) -> Any : start = time . perf_counter () result = func ( * args , ** kwargs ) end = time . perf_counter () print ( f \" { func . __name__ } executed in { end - start : .3f } seconds\" ) return result return wrapper","title":"timeit()"},{"location":"bivariate_quantiles.html","text":"bivariate_quantiles module \u00b6 This takes in observations of a bivariate random variable y and computes vector quantiles and vector ranks \u00e0 la Chernozhukov-Galichon-Hallin-Henry ( Ann. Stats. 2017) . Note if the math looks strange in the documentation, just reload the page. The sequence of steps is as follows: choose a number of Chebyshev nodes for numerical integration and optimize the weights: v = solve_for_v(y, n_nodes) to obtain the \\((u_1,u_2)\\) quantiles for \\((u_1, u_2)\\in [0,1]\\) , run qtiles_y = bivariate_quantiles_v(y, v, u1, u2) to compute the vector ranks for all points in the sample (the barycenters of the cells in the power diagram): ranks_y = bivariate_ranks_v(y, v, n_nodes) Steps 1 and 2 can be combined: qtiles_y = bivariate_quantiles(y, v, u1, u2, n_nodes) Steps 1 and 3 can be combined: ranks_y = bivariate_ranks(y, n_nodes) bivariate_quantiles ( y , u , n_nodes = 32 , verbose = False ) \u00b6 computes the bivariate quantiles of y at the quantiles u Parameters: Name Type Description Default y np . ndarray the observations, an (n, 2) matrix required u np . ndarray the quantiles at which to compute the bivariate quantiles, an (m, 2) matrix required n_nodes int the number of nodes to use for the quadrature 32 verbose bool if True , print some information False Returns: Type Description np . ndarray an (m, 2) matrix of bivariate quantiles Source code in bs_python_utils/bivariate_quantiles.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def bivariate_quantiles ( y : np . ndarray , u : np . ndarray , n_nodes : int = 32 , verbose : bool = False ) -> np . ndarray : \"\"\"computes the bivariate quantiles of `y` at the quantiles `u` Args: y: the observations, an `(n, 2)` matrix u: the quantiles at which to compute the bivariate quantiles, an `(m, 2)` matrix n_nodes: the number of nodes to use for the quadrature verbose: if `True`, print some information Returns: an `(m, 2)` matrix of bivariate quantiles \"\"\" v = solve_for_v_ ( y , n_nodes , verbose ) return bivariate_quantiles_v ( y , u , v ) bivariate_quantiles_v ( y , u , v ) \u00b6 computes the vector quantiles of y at values u , given the converged v Parameters: Name Type Description Default y np . ndarray the observations, an (n,2) matrix required u np . ndarray the values where we want the quantiles, an (m,2) matrix in \\([0,1]\\) required v np . ndarray the converged values of the weights, an n -vector required Returns: Type Description np . ndarray an (m,2) matrix with the quantiles of y at the values u Source code in bs_python_utils/bivariate_quantiles.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def bivariate_quantiles_v ( y : np . ndarray , u : np . ndarray , v : np . ndarray ) -> np . ndarray : \"\"\"computes the vector quantiles of `y` at values `u`, given the converged `v` Args: y: the observations, an `(n,2)` matrix u: the values where we want the quantiles, an `(m,2)` matrix in $[0,1]$ v: the converged values of the weights, an `n`-vector Returns: an `(m,2)` matrix with the quantiles of `y` at the values `u` \"\"\" net_val = u @ y . T - v k_max = np . argmax ( net_val , 1 ) return cast ( np . ndarray , y [ k_max ]) bivariate_ranks ( y , n_nodes = 32 , verbose = False ) \u00b6 computes the bivariate ranks of y Parameters: Name Type Description Default y np . ndarray the observations, an (n, 2) matrix required n_nodes int the number of nodes to use for the quadrature 32 verbose bool if True , print some information False Returns: Type Description np . ndarray the (n, 2) matrix of bivariate average ranks Source code in bs_python_utils/bivariate_quantiles.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def bivariate_ranks ( y : np . ndarray , n_nodes : int = 32 , verbose : bool = False ) -> np . ndarray : \"\"\"computes the bivariate ranks of `y` Args: y: the observations, an `(n, 2)` matrix n_nodes: the number of nodes to use for the quadrature verbose: if `True`, print some information Returns: the `(n, 2)` matrix of bivariate average ranks \"\"\" v = solve_for_v_ ( y , n_nodes , verbose ) return bivariate_ranks_v ( y , v , n_nodes ) bivariate_ranks_v ( y , v , n_nodes = 32 , presorted = False ) \u00b6 computes the vector ranks of y , given the converged v Parameters: Name Type Description Default y np . ndarray the observations, an (n,2) matrix required v np . ndarray the converged values of the weights, an n -vector required n_nodes int the number of nodes for Chebyshev integration 32 presorted bool if True , then y and v are sorted by increasing y[:, 1] . False Returns: Type Description np . ndarray an (n,2) matrix with the average ranks of y Source code in bs_python_utils/bivariate_quantiles.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def bivariate_ranks_v ( y : np . ndarray , v : np . ndarray , n_nodes : int = 32 , presorted : bool = False ) -> np . ndarray : \"\"\"computes the vector ranks of `y`, given the converged `v` Args: y: the observations, an `(n,2)` matrix v: the converged values of the weights, an `n`-vector n_nodes: the number of nodes for Chebyshev integration presorted: if `True`, then `y` and `v` are sorted by increasing `y[:, 1]`. Returns: an `(n,2)` matrix with the average ranks of `y` \"\"\" n , d = y . shape if d != 2 : bs_error_abort ( f \"only works for 2-dimensional y, not for { d } \" ) interval01 = Interval ( 0.0 , 1.0 ) u1_nodes , u1_weights = cheb_get_nodes_1d ( interval01 , n_nodes ) if presorted : sort_order = np . arange ( n ) y_sorted = y v_sorted = v else : sort_order = np . argsort ( y [:, 1 ]) y_sorted = y [ sort_order , :] v_sorted = v [ sort_order ] a_mat , b_mat = _compute_ab ( y_sorted , v_sorted ) average_ranks = np . zeros (( n , 2 )) for k in range ( n ): left_bounds , right_bounds = _compute_u2_bounds ( k , u1_nodes , a_mat , b_mat ) pos_diffs = np . maximum ( right_bounds - left_bounds , 0.0 ) pos_diffs_sq = np . maximum ( right_bounds * right_bounds - left_bounds * left_bounds , 0.0 ) prob_k = pos_diffs @ u1_weights average_ranks [ sort_order [ k ], 0 ] = (( u1_nodes * pos_diffs ) @ u1_weights ) / prob_k average_ranks [ sort_order [ k ], 1 ] = (( pos_diffs_sq @ u1_weights ) / 2.0 ) / prob_k return average_ranks","title":"bivariate quantiles and ranks"},{"location":"bivariate_quantiles.html#bivariate_quantiles-module","text":"This takes in observations of a bivariate random variable y and computes vector quantiles and vector ranks \u00e0 la Chernozhukov-Galichon-Hallin-Henry ( Ann. Stats. 2017) . Note if the math looks strange in the documentation, just reload the page. The sequence of steps is as follows: choose a number of Chebyshev nodes for numerical integration and optimize the weights: v = solve_for_v(y, n_nodes) to obtain the \\((u_1,u_2)\\) quantiles for \\((u_1, u_2)\\in [0,1]\\) , run qtiles_y = bivariate_quantiles_v(y, v, u1, u2) to compute the vector ranks for all points in the sample (the barycenters of the cells in the power diagram): ranks_y = bivariate_ranks_v(y, v, n_nodes) Steps 1 and 2 can be combined: qtiles_y = bivariate_quantiles(y, v, u1, u2, n_nodes) Steps 1 and 3 can be combined: ranks_y = bivariate_ranks(y, n_nodes)","title":"bivariate_quantiles module"},{"location":"bivariate_quantiles.html#bs_python_utils.bivariate_quantiles.bivariate_quantiles","text":"computes the bivariate quantiles of y at the quantiles u Parameters: Name Type Description Default y np . ndarray the observations, an (n, 2) matrix required u np . ndarray the quantiles at which to compute the bivariate quantiles, an (m, 2) matrix required n_nodes int the number of nodes to use for the quadrature 32 verbose bool if True , print some information False Returns: Type Description np . ndarray an (m, 2) matrix of bivariate quantiles Source code in bs_python_utils/bivariate_quantiles.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def bivariate_quantiles ( y : np . ndarray , u : np . ndarray , n_nodes : int = 32 , verbose : bool = False ) -> np . ndarray : \"\"\"computes the bivariate quantiles of `y` at the quantiles `u` Args: y: the observations, an `(n, 2)` matrix u: the quantiles at which to compute the bivariate quantiles, an `(m, 2)` matrix n_nodes: the number of nodes to use for the quadrature verbose: if `True`, print some information Returns: an `(m, 2)` matrix of bivariate quantiles \"\"\" v = solve_for_v_ ( y , n_nodes , verbose ) return bivariate_quantiles_v ( y , u , v )","title":"bivariate_quantiles()"},{"location":"bivariate_quantiles.html#bs_python_utils.bivariate_quantiles.bivariate_quantiles_v","text":"computes the vector quantiles of y at values u , given the converged v Parameters: Name Type Description Default y np . ndarray the observations, an (n,2) matrix required u np . ndarray the values where we want the quantiles, an (m,2) matrix in \\([0,1]\\) required v np . ndarray the converged values of the weights, an n -vector required Returns: Type Description np . ndarray an (m,2) matrix with the quantiles of y at the values u Source code in bs_python_utils/bivariate_quantiles.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def bivariate_quantiles_v ( y : np . ndarray , u : np . ndarray , v : np . ndarray ) -> np . ndarray : \"\"\"computes the vector quantiles of `y` at values `u`, given the converged `v` Args: y: the observations, an `(n,2)` matrix u: the values where we want the quantiles, an `(m,2)` matrix in $[0,1]$ v: the converged values of the weights, an `n`-vector Returns: an `(m,2)` matrix with the quantiles of `y` at the values `u` \"\"\" net_val = u @ y . T - v k_max = np . argmax ( net_val , 1 ) return cast ( np . ndarray , y [ k_max ])","title":"bivariate_quantiles_v()"},{"location":"bivariate_quantiles.html#bs_python_utils.bivariate_quantiles.bivariate_ranks","text":"computes the bivariate ranks of y Parameters: Name Type Description Default y np . ndarray the observations, an (n, 2) matrix required n_nodes int the number of nodes to use for the quadrature 32 verbose bool if True , print some information False Returns: Type Description np . ndarray the (n, 2) matrix of bivariate average ranks Source code in bs_python_utils/bivariate_quantiles.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def bivariate_ranks ( y : np . ndarray , n_nodes : int = 32 , verbose : bool = False ) -> np . ndarray : \"\"\"computes the bivariate ranks of `y` Args: y: the observations, an `(n, 2)` matrix n_nodes: the number of nodes to use for the quadrature verbose: if `True`, print some information Returns: the `(n, 2)` matrix of bivariate average ranks \"\"\" v = solve_for_v_ ( y , n_nodes , verbose ) return bivariate_ranks_v ( y , v , n_nodes )","title":"bivariate_ranks()"},{"location":"bivariate_quantiles.html#bs_python_utils.bivariate_quantiles.bivariate_ranks_v","text":"computes the vector ranks of y , given the converged v Parameters: Name Type Description Default y np . ndarray the observations, an (n,2) matrix required v np . ndarray the converged values of the weights, an n -vector required n_nodes int the number of nodes for Chebyshev integration 32 presorted bool if True , then y and v are sorted by increasing y[:, 1] . False Returns: Type Description np . ndarray an (n,2) matrix with the average ranks of y Source code in bs_python_utils/bivariate_quantiles.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def bivariate_ranks_v ( y : np . ndarray , v : np . ndarray , n_nodes : int = 32 , presorted : bool = False ) -> np . ndarray : \"\"\"computes the vector ranks of `y`, given the converged `v` Args: y: the observations, an `(n,2)` matrix v: the converged values of the weights, an `n`-vector n_nodes: the number of nodes for Chebyshev integration presorted: if `True`, then `y` and `v` are sorted by increasing `y[:, 1]`. Returns: an `(n,2)` matrix with the average ranks of `y` \"\"\" n , d = y . shape if d != 2 : bs_error_abort ( f \"only works for 2-dimensional y, not for { d } \" ) interval01 = Interval ( 0.0 , 1.0 ) u1_nodes , u1_weights = cheb_get_nodes_1d ( interval01 , n_nodes ) if presorted : sort_order = np . arange ( n ) y_sorted = y v_sorted = v else : sort_order = np . argsort ( y [:, 1 ]) y_sorted = y [ sort_order , :] v_sorted = v [ sort_order ] a_mat , b_mat = _compute_ab ( y_sorted , v_sorted ) average_ranks = np . zeros (( n , 2 )) for k in range ( n ): left_bounds , right_bounds = _compute_u2_bounds ( k , u1_nodes , a_mat , b_mat ) pos_diffs = np . maximum ( right_bounds - left_bounds , 0.0 ) pos_diffs_sq = np . maximum ( right_bounds * right_bounds - left_bounds * left_bounds , 0.0 ) prob_k = pos_diffs @ u1_weights average_ranks [ sort_order [ k ], 0 ] = (( u1_nodes * pos_diffs ) @ u1_weights ) / prob_k average_ranks [ sort_order [ k ], 1 ] = (( pos_diffs_sq @ u1_weights ) / 2.0 ) / prob_k return average_ranks","title":"bivariate_ranks_v()"},{"location":"bs_altair.html","text":"bs_altair module \u00b6 Some Altair plots. alt_lineplot , alt_superposed_lineplot , alt_superposed_faceted_lineplot alt_plot_fun : plots a function alt_density , alt_faceted_densities : plots the density of x , or of x conditional on a category alt_superposed_faceted_densities : plots the density of x superposed by f and faceted by g alt_scatterplot , alt_scatterplot_with_histo , alt-linked_scatterplots : variants of scatter plots alt_histogram_by , alt_histogram_continuous : histograms of x by y , and of a continuous x alt_stacked_area , alt_stacked_area_facets : stacked area plots plot_parameterized_estimates : plots densities of estimates of coefficients, with the true values, as a function of a parameter plot_true_sim_facets, plot_true_sim2_facets : plot two simulated values and the true values of statistics as a function of a parameter alt_tick_plots : vertically arranged tick plots of variables. alt_density ( df , str_x , save = None ) \u00b6 Plots the density of df[str_x] . Parameters: Name Type Description Default df pd . DataFrame the data with the str_x variable required str_x str the name of a continuous column required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def alt_density ( df : pd . DataFrame , str_x : str , save : str | None = None ) -> alt . Chart : \"\"\"Plots the density of `df[str_x]`. Args: df: the data with the `str_x` variable str_x: the name of a continuous column save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" ch = ( alt . Chart ( df ) . transform_density ( str_x , as_ = [ str_x , \"Density\" ], ) . mark_area ( opacity = 0.4 ) . encode ( x = f \" { str_x } :Q\" , y = \"Density:Q\" , ) ) _maybe_save ( ch , save ) return ch alt_faceted_densities ( df , str_x , str_f , legend_title = None , save = None , max_cols = 4 ) \u00b6 Plots the density of df[str_x] by df[str_f] in column facets Parameters: Name Type Description Default df pd . DataFrame the data with the str_x and str_f variables required str_x str the name of a continuous column required str_f str the name of a categorical column required legend_title str | None a title for the legend None save str | None the name of a file to save to (HTML extension will be added) None max_cols int | None we wrap after that number of columns 4 Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def alt_faceted_densities ( df : pd . DataFrame , str_x : str , str_f : str , legend_title : str | None = None , save : str | None = None , max_cols : int | None = 4 , ) -> alt . Chart : \"\"\" Plots the density of `df[str_x]` by `df[str_f]` in column facets Args: df: the data with the `str_x` and `str_f` variables str_x: the name of a continuous column str_f: the name of a categorical column legend_title: a title for the legend save: the name of a file to save to (HTML extension will be added) max_cols: we wrap after that number of columns Returns: the `alt.Chart` object. \"\"\" our_legend_title = str_f if legend_title is None else legend_title ch = ( alt . Chart ( df ) . transform_density ( str_x , groupby = [ str_f ], as_ = [ str_x , \"Density\" ], ) . mark_area ( opacity = 0.4 ) . encode ( x = f \" { str_x } :Q\" , y = \"Density:Q\" , color = alt . Color ( f \" { str_f } :N\" , title = our_legend_title ), ) . facet ( f \" { str_f } :N\" , columns = max_cols ) ) _maybe_save ( ch , save ) return ch alt_histogram_by ( df , str_x , str_y , str_agg = 'mean' , save = None ) \u00b6 Plots a histogram of a statistic of str_y by str_x Parameters: Name Type Description Default df pd . DataFrame a dataframe with columns str_x and str_y required str_x str a categorical variable required str_y str a continuous variable required str_agg str | None how we aggregate the values of str_y by str_x 'mean' save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the Altair chart. Source code in bs_python_utils/bs_altair.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 def alt_histogram_by ( df : pd . DataFrame , str_x : str , str_y : str , str_agg : str | None = \"mean\" , save : str | None = None , ) -> alt . Chart : \"\"\" Plots a histogram of a statistic of `str_y` by `str_x` Args: df: a dataframe with columns `str_x` and `str_y` str_x: a categorical variable str_y: a continuous variable str_agg: how we aggregate the values of `str_y` by `str_x` save: the name of a file to save to (HTML extension will be added) Returns: the Altair chart. \"\"\" ch = ( alt . Chart ( df ) . mark_bar () . encode ( x = str_x , y = f \" { str_agg } ( { str_y } ):Q\" ) . properties ( height = 300 , width = 400 ) ) _maybe_save ( ch , save ) return ch alt_histogram_continuous ( df , str_x , save = None ) \u00b6 Histogram of a continuous variable df[str_x] Parameters: Name Type Description Default df pd . DataFrame the data with the str_x , str_y , and str_f variables required str_x str the name of a continuous column required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 def alt_histogram_continuous ( df : pd . DataFrame , str_x : str , save : str | None = None ) -> alt . Chart : \"\"\" Histogram of a continuous variable `df[str_x]` Args: df: the data with the `str_x`, `str_y`, and `str_f` variables str_x: the name of a continuous column save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" ch = alt . Chart ( df ) . mark_bar () . encode ( alt . X ( str_x , bin = True ), y = \"count()\" ) _maybe_save ( ch , save ) return ch alt_lineplot ( df , str_x , str_y , time_series = False , save = None , aggreg = None , ** kwargs ) \u00b6 Scatterplot of df[str_x] vs df[str_y] Parameters: Name Type Description Default df pd . DataFrame the data with columns str_x and str_y required str_x str the name of a continuous column required str_y str the name of a continuous column required time_series bool True if x is a time series False save str | None the name of a file to save to (HTML extension will be added) None aggreg str | None the name of an aggregating function for y None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def alt_lineplot ( df : pd . DataFrame , str_x : str , str_y : str , time_series : bool = False , save : str | None = None , aggreg : str | None = None , ** kwargs , ) -> alt . Chart : \"\"\" Scatterplot of `df[str_x]` vs `df[str_y]` Args: df: the data with columns `str_x` and `str_y` str_x: the name of a continuous column str_y: the name of a continuous column time_series: `True` if x is a time series save: the name of a file to save to (HTML extension will be added) aggreg: the name of an aggregating function for `y` Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" var_y = f \" { aggreg } ( { str_y } ):Q\" if aggreg is not None else str_y ch = alt . Chart ( df ) . mark_line () . encode ( x = f \" { str_x } : { type_x } \" , y = var_y ) if \"title\" in kwargs : ch = ch . properties ( title = kwargs [ \"title\" ]) _maybe_save ( ch , save ) return ch alt_linked_scatterplots ( df , str_x1 , str_x2 , str_y , str_f , save = None ) \u00b6 Creates two scatterplots: of df[str_x1] vs df[str_y] and of df[str_x2] vs df[str_y] , both with color as per df[str_f] . Selecting an interval in one shows up in the other. Parameters: Name Type Description Default df pd . DataFrame required str_x1 str the name of a continuous column required str_x2 str the name of a continuous column required str_y str the name of a continuous column required str_f str the name of a categorical column required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def alt_linked_scatterplots ( df : pd . DataFrame , str_x1 : str , str_x2 : str , str_y : str , str_f : str , save : str | None = None , ) -> alt . Chart : \"\"\" Creates two scatterplots: of `df[str_x1]` vs `df[str_y]` and of `df[str_x2]` vs `df[str_y]`, both with color as per `df[str_f]`. Selecting an interval in one shows up in the other. Args: df: str_x1: the name of a continuous column str_x2: the name of a continuous column str_y: the name of a continuous column str_f: the name of a categorical column save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" interval = alt . selection_interval () base = ( alt . Chart ( df ) . mark_point () . encode ( y = f \" { str_y } :Q\" , color = alt . condition ( interval , str_f , alt . value ( \"lightgray\" )) ) . properties ( selection = interval ) ) ch = base . encode ( x = f \" { str_x1 } :Q\" ) | base . encode ( x = f \" { str_x2 } :Q\" ) _maybe_save ( ch , save ) return ch alt_plot_fun ( f , start , end , npoints = 100 , save = None ) \u00b6 Plots the function f from start to end . Parameters: Name Type Description Default f Callable returns a Numpy array from a Numpy array required start float first point on x axis required end float last point on x axis required npoints int number of points 100 save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def alt_plot_fun ( f : Callable , start : float , end : float , npoints : int = 100 , save : str | None = None , ) -> alt . Chart : \"\"\" Plots the function `f` from `start` to `end`. Args: f: returns a Numpy array from a Numpy array start: first point on `x` axis end: last point on `x` axis npoints: number of points save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" step = ( end - start ) / npoints points = np . arange ( start , end + step , step ) fun_data = pd . DataFrame ({ \"x\" : points , \"y\" : f ( points )}) ch = ( alt . Chart ( fun_data ) . mark_line () . encode ( x = \"x:Q\" , y = \"y:Q\" , ) ) _maybe_save ( ch , save ) return ch alt_scatterplot ( df , str_x , str_y , time_series = False , save = None , xlabel = None , ylabel = None , size = 30 , title = None , color = None , aggreg = None , selection = False ) \u00b6 Scatterplot of df[str_x] vs df[str_y] . Parameters: Name Type Description Default df pd . DataFrame the data with columns for x, y required str_x str the name of a continuous x column required str_y str the name of a continuous y column required time_series bool True if x is a time series False xlabel str | None label for the horizontal axis None ylabel str | None label for the vertical axis None title str | None title for the graph None size int | None radius of the circles 30 color str | None variable that determines the color of the circles None selection bool if True , the user can select interactively from the color legend, if any False save str | None the name of a file to save to (HTML extension will be added) None aggreg str | None the name of an aggregating function for y None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def alt_scatterplot ( df : pd . DataFrame , str_x : str , str_y : str , time_series : bool = False , save : str | None = None , xlabel : str | None = None , ylabel : str | None = None , size : int | None = 30 , title : str | None = None , color : str | None = None , aggreg : str | None = None , selection : bool = False , ) -> alt . Chart : \"\"\" Scatterplot of `df[str_x]` vs `df[str_y]`. Args: df: the data with columns for x, y str_x: the name of a continuous x column str_y: the name of a continuous y column time_series: `True` if x is a time series xlabel: label for the horizontal axis ylabel: label for the vertical axis title: title for the graph size: radius of the circles color: variable that determines the color of the circles selection: if `True`, the user can select interactively from the `color` legend, if any save: the name of a file to save to (HTML extension will be added) aggreg: the name of an aggregating function for `y` Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" var_x = alt . X ( f \" { str_x } : { type_x } \" ) if xlabel is not None : if isinstance ( xlabel , str ): var_x = alt . X ( f \" { str_x } : { type_x } \" , axis = alt . Axis ( title = xlabel )) else : bs_error_abort ( f \"xlabel must be a string, not { xlabel } \" ) var_y = f \" { aggreg } ( { str_y } ):Q\" if aggreg is not None else str_y if ylabel is not None : if isinstance ( ylabel , str ): var_y = alt . Y ( var_y , axis = alt . Axis ( title = ylabel )) else : bs_error_abort ( f \"ylabel must be a string, not { ylabel } \" ) if isinstance ( size , int ): circles_size = size else : bs_error_abort ( f \"size must be an integer, not { size } \" ) if color is not None : if isinstance ( color , str ): if selection : selection_criterion = alt . selection_multi ( fields = [ color ], bind = \"legend\" ) ch = ( alt . Chart ( df ) . mark_circle ( size = circles_size ) . encode ( x = var_x , y = var_y , color = color , opacity = alt . condition ( selection_criterion , alt . value ( 1 ), alt . value ( 0.1 ) ), ) . add_selection ( selection_criterion ) ) else : ch = ( alt . Chart ( df ) . mark_circle ( size = circles_size ) . encode ( x = var_x , y = var_y , color = color ) ) else : bs_error_abort ( f \"color must be a string, not { color } \" ) else : ch = alt . Chart ( df ) . mark_circle ( size = circles_size ) . encode ( x = var_x , y = var_y ) ch = _add_title ( ch , title ) _maybe_save ( ch , save ) return ch alt_scatterplot_with_histo ( df , str_x , str_y , str_f , save = None ) \u00b6 Scatterplot of df[str_x] vs df[str_y] with colors as per df[str_f] allows to select an interval and histograns the counts of df[str_f] in the interval. Parameters: Name Type Description Default df pd . DataFrame the data with the str_x and str_f variables required str_x str the name of a continuous column required str_y str the name of a continuous column required str_f str the name of a categorical column required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def alt_scatterplot_with_histo ( df : pd . DataFrame , str_x : str , str_y : str , str_f : str , save : str | None = None ) -> alt . Chart : \"\"\" Scatterplot of `df[str_x]` vs `df[str_y]` with colors as per `df[str_f]` allows to select an interval and histograns the counts of `df[str_f]` in the interval. Args: df: the data with the `str_x` and `str_f` variables str_x: the name of a continuous column str_y: the name of a continuous column str_f: the name of a categorical column save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" interval = alt . selection_interval () points = ( alt . Chart ( df ) . mark_point () . encode ( x = f \" { str_x } :Q\" , y = f \" { str_y } :Q\" , color = alt . condition ( interval , str_f , alt . value ( \"lightgray\" )), ) . properties ( selection = interval ) ) histogram = ( alt . Chart ( df ) . mark_bar () . encode ( x = \"count()\" , y = str_f , color = str_f , ) . transform_filter ( interval ) ) ch = points & histogram _maybe_save ( ch , save ) return ch alt_stacked_area ( df , str_x , str_y , str_f , time_series = False , title = None , save = None ) \u00b6 Normalized stacked lineplots of df[str_x] vs df[str_y] by df[str_f] Parameters: Name Type Description Default df pd . DataFrame the data with columns for str_x , str_y , and str_f required str_x str the name of a continuous column required str_y str the name of a continuous column required str_f str the name of a categorical column required time_series bool True if str_x is a time series False title str | None a title for the plot None save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 def alt_stacked_area ( df : pd . DataFrame , str_x : str , str_y : str , str_f : str , time_series : bool = False , title : str | None = None , save : str | None = None , ) -> alt . Chart : \"\"\" Normalized stacked lineplots of `df[str_x]` vs `df[str_y]` by `df[str_f]` Args: df: the data with columns for `str_x`, `str_y`, and `str_f` str_x: the name of a continuous column str_y: the name of a continuous column str_f: the name of a categorical column time_series: `True` if `str_x` is a time series title: a title for the plot save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" ch = ( alt . Chart ( df ) . mark_area () . encode ( x = f \" { str_x } : { type_x } \" , y = alt . Y ( f \" { str_y } :Q\" , stack = \"normalize\" ), color = f \" { str_f } :N\" , ) ) if title is not None : ch = ch . properties ( title = title ) _maybe_save ( ch , save ) return ch alt_stacked_area_facets ( df , str_x , str_y , str_f , str_g , time_series = False , max_cols = 5 , title = None , save = None ) \u00b6 Normalized stacked lineplots of df[str_x] vs df[str_y] by df[str_f] , faceted by df[str_g] Parameters: Name Type Description Default df pd . DataFrame the data with columns for str_x , str_y , and str_f required str_x str the name of a continuous column required str_y str the name of a continuous column required str_f str the name of a categorical column required str_g str the name of a categorical column required time_series bool True if str_x is a time series False title str | None a title for the plot None save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 def alt_stacked_area_facets ( df : pd . DataFrame , str_x : str , str_y : str , str_f : str , str_g : str , time_series : bool = False , max_cols : int | None = 5 , title : str | None = None , save : str | None = None , ) -> alt . Chart : \"\"\" Normalized stacked lineplots of `df[str_x]` vs `df[str_y]` by `df[str_f]`, faceted by `df[str_g]` Args: df: the data with columns for `str_x`, `str_y`, and `str_f` str_x: the name of a continuous column str_y: the name of a continuous column str_f: the name of a categorical column str_g: the name of a categorical column time_series: `True` if `str_x` is a time series title: a title for the plot save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" ch = ( alt . Chart ( df ) . mark_area () . encode ( x = f \" { str_x } : { type_x } \" , y = alt . Y ( f \" { str_y } :Q\" , stack = \"normalize\" ), color = f \" { str_f } :N\" , facet = alt . Facet ( f \" { str_g } :N\" , columns = max_cols ), ) ) _maybe_save ( ch , save ) return ch alt_superposed_faceted_densities ( df , str_x , str_f , str_g , max_cols = 4 , save = None ) \u00b6 Creates density plots of df[str_x] by df[str_f] and df[str_g] with color as per df[str_f] and faceted by df[str_g] . that is: facets by str_g , with densities conditional on str_f superposed. Parameters: Name Type Description Default df pd . DataFrame a Pandas dataframe wity columns str_x , str_f , str_g required str_x str the name of a continuous column required str_f str the name of a categorical column required str_g str the name of a categorical column required max_cols int | None the number of columns after whcih we wrap 4 save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def alt_superposed_faceted_densities ( df : pd . DataFrame , str_x : str , str_f : str , str_g : str , max_cols : int | None = 4 , save : str | None = None , ) -> alt . Chart : \"\"\" Creates density plots of `df[str_x]` by `df[str_f]` and `df[str_g]` with color as per `df[str_f]` and faceted by `df[str_g]`. that is: facets by `str_g`, with densities conditional on `str_f` superposed. Args: df: a Pandas dataframe wity columns `str_x`, `str_f`, `str_g` str_x: the name of a continuous column str_f: the name of a categorical column str_g: the name of a categorical column max_cols: the number of columns after whcih we wrap save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" densities = ( alt . Chart ( df ) . transform_density ( str_x , groupby = [ str_f , str_g ], as_ = [ str_x , \"Density\" ], ) . mark_line () . encode ( x = f \" { str_x } :Q\" , y = \"Density:Q\" , color = f \" { str_f } :N\" , ) . facet ( column = f \" { str_g } :N\" , columns = max_cols ) . resolve_scale ( x = \"independent\" , y = \"independent\" ) ) _maybe_save ( densities , save ) return densities alt_superposed_faceted_lineplot ( df , str_x , str_y , str_f , str_g , time_series = False , legend_title = None , max_cols = 5 , save = None ) \u00b6 Plots df[str_x] vs df[str_y] superposed by df[str_f] and faceted by df[str_g] Parameters: Name Type Description Default df pd . DataFrame the data with the str_x , str_y , and str_f variables required str_x str the name of a continuous column required str_y str the name of a continuous column required str_f str the name of a categorical column required str_g str the name of a categorical column required time_series bool True if str_x is a time series False legend_title str | None a title for the legend None save str | None the name of a file to save to (HTML extension will be added) None max_cols int | None we wrap after that number of columns 5 Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 def alt_superposed_faceted_lineplot ( df : pd . DataFrame , str_x : str , str_y : str , str_f : str , str_g : str , time_series : bool = False , legend_title : str | None = None , max_cols : int | None = 5 , save : str | None = None , ) -> alt . Chart : \"\"\" Plots `df[str_x]` vs `df[str_y]` superposed by `df[str_f]` and faceted by `df[str_g]` Args: df: the data with the `str_x`, `str_y`, and `str_f` variables str_x: the name of a continuous column str_y: the name of a continuous column str_f: the name of a categorical column str_g: the name of a categorical column time_series: `True` if `str_x` is a time series legend_title: a title for the legend save: the name of a file to save to (HTML extension will be added) max_cols: we wrap after that number of columns Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" our_title = str_f if legend_title is None else legend_title ch = ( alt . Chart ( df ) . mark_line () . encode ( x = f \" { str_x } : { type_x } \" , y = f \" { str_y } :Q\" , color = alt . Color ( f \" { str_f } :N\" , title = our_title ), facet = alt . Facet ( f \" { str_g } :N\" , columns = max_cols ), ) ) _maybe_save ( ch , save ) return ch alt_superposed_lineplot ( df , str_x , str_y , str_f , time_series = False , legend_title = None , save = None ) \u00b6 Plots df[str_x] vs df[str_y] by df[str_f] on one plot Parameters: Name Type Description Default df pd . DataFrame the data with the str_x , str_y , and str_f variables required str_x str the name of a continuous x column required str_y str the name of a continuous y column required str_f str the name of a categorical f column required time_series bool True if str_x is a time series False legend_title str | None a title for the legend None save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def alt_superposed_lineplot ( df : pd . DataFrame , str_x : str , str_y : str , str_f : str , time_series : bool = False , legend_title : str | None = None , save : str | None = None , ) -> alt . Chart : \"\"\" Plots `df[str_x]` vs `df[str_y]` by `df[str_f]` on one plot Args: df: the data with the `str_x`, `str_y`, and `str_f` variables str_x: the name of a continuous `x` column str_y: the name of a continuous `y` column str_f: the name of a categorical `f` column time_series: `True` if `str_x` is a time series legend_title: a title for the legend save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" our_legend_title = str_f if legend_title is None else legend_title ch = ( alt . Chart ( df ) . mark_line () . encode ( x = f \" { str_x } : { type_x } \" , y = f \" { str_y } :Q\" , color = alt . Color ( f \" { str_f } :N\" , title = our_legend_title ), ) ) _maybe_save ( ch , save ) return ch alt_tick_plots ( df , list_vars , save = None ) \u00b6 Creates a tick plot of the variables in list_vars of df , arranged vertically. Parameters: Name Type Description Default df pd . DataFrame a dataframe with the variables in list_vars required list_vars str | list [ str ] the name of a column of df , or a list of names required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 def alt_tick_plots ( df : pd . DataFrame , list_vars : str | list [ str ], save : str | None = None ) -> alt . Chart : \"\"\" Creates a tick plot of the variables in `list_vars` of`df`, arranged vertically. Args: df: a dataframe with the variables in `list_vars` list_vars: the name of a column of `df`, or a list of names save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" if isinstance ( list_vars , str ): varname = list_vars ch = alt . Chart ( df ) . encode ( x = varname ) . mark_tick () else : ch = ( alt . Chart ( df ) . encode ( alt . X ( alt . repeat ( \"row\" ), type = \"quantitative\" )) . mark_tick () . repeat ( row = list_vars ) . resolve_scale ( y = \"independent\" ) ) _maybe_save ( ch , save ) return ch plot_parameterized_estimates ( parameter_name , parameter_values , coeff_names , true_values , estimate_names , estimates , colors , save = None ) \u00b6 Plots estimates of coefficients, with the true values, as a function of a parameter; one facet per coefficient Parameters: Name Type Description Default parameter_name str the name of the parameter required parameter_values np . ndarray a vector of n_vals values for the parameter required coeff_names str | list [ str ] the names of the n_coeffs coefficients required true_values np . ndarray their true values, depending on the parameter or not required estimate_names str | list [ str ] names of the estimates required estimates np . ndarray their values required colors list [ str ] colors for the various estimates required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 def plot_parameterized_estimates ( parameter_name : str , parameter_values : np . ndarray , coeff_names : str | list [ str ], true_values : np . ndarray , estimate_names : str | list [ str ], estimates : np . ndarray , colors : list [ str ], save : str | None = None , ) -> alt . Chart : \"\"\" Plots estimates of coefficients, with the true values, as a function of a parameter; one facet per coefficient Args: parameter_name: the name of the parameter parameter_values: a vector of `n_vals` values for the parameter coeff_names: the names of the `n_coeffs` coefficients true_values: their true values, depending on the parameter or not estimate_names: names of the estimates estimates: their values colors: colors for the various estimates save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" n_vals = check_vector ( parameter_values ) n_coeffs = 1 if isinstance ( coeff_names , str ) else len ( coeff_names ) if n_coeffs == 1 : n_true = check_vector ( true_values , \"plot_parameterized_estimates\" ) if n_true != n_vals : bs_error_abort ( f \"plot_parameterized_estimates: we have { n_true } values and\" f \" { n_vals } parameter values.\" ) df = pd . DataFrame ({ parameter_name : parameter_values , \"True value\" : true_values }) df1 , ordered_estimates = _stack_estimates ( estimate_names , estimates , df ) df1m = pd . melt ( df1 , parameter_name , var_name = \"Estimate\" ) ch = ( alt . Chart ( df1m ) . mark_line () . encode ( x = f \" { parameter_name } :Q\" , y = \"value:Q\" , strokeDash = alt . StrokeDash ( \"Estimate:N\" , sort = ordered_estimates ), color = alt . Color ( \"Estimate:N\" , sort = estimate_names , scale = alt . Scale ( domain = ordered_estimates , range = colors ), ), ) ) else : n_true , n_c = check_matrix ( true_values , \"plot_parameterized_estimates\" ) if n_true != n_vals : bs_error_abort ( f \"plot_parameterized_estimates: we have { n_true } true values and\" f \" { n_vals } parameter values.\" ) if n_c != n_coeffs : bs_error_abort ( f \"plot_parameterized_estimates: we have { n_c } columns of true values\" f \" and { n_coeffs } coefficients.\" ) df1 = [ None ] * n_coeffs for i_coeff , coeff in enumerate ( coeff_names ): df_i = pd . DataFrame ( { parameter_name : parameter_values , \"True value\" : true_values [:, i_coeff ], } ) df1 [ i_coeff ], ordered_estimates = _stack_estimates ( estimate_names , estimates [ ... , i_coeff ], df_i ) df1 [ i_coeff ][ \"Coefficient\" ] = coeff df2 = pd . concat ( df1 [ i_coeff ] for i_coeff in range ( n_coeffs )) ordered_colors = colors df2m = pd . melt ( df2 , [ parameter_name , \"Coefficient\" ], var_name = \"Estimate\" ) ch = ( alt . Chart ( df2m ) . mark_line () . encode ( x = f \" { parameter_name } :Q\" , y = \"value:Q\" , strokeDash = alt . StrokeDash ( \"Estimate:N\" , sort = ordered_estimates ), color = alt . Color ( \"Estimate:N\" , sort = ordered_estimates , scale = alt . Scale ( domain = ordered_estimates , range = ordered_colors ), ), ) . facet ( alt . Facet ( \"Coefficient:N\" , sort = coeff_names )) . resolve_scale ( y = \"independent\" ) ) _maybe_save ( ch , save ) return ch plot_true_sim2_facets ( parameter_name , parameter_values , stat_names , stat_true , stat_sim1 , stat_sim2 , colors , stat_title = 'Statistic' , subtitle = 'True vs estimated' , ncols = 3 , save = None ) \u00b6 Plots simulated values for two methods and true values of statistics as a function of a parameter; one facet per coefficient Parameters: Name Type Description Default parameter_name str the name of the parameter required parameter_values np . ndarray a vector of n_vals values for the parameter required stat_names list [ str ] the names of the n statistics required stat_true np . ndarray their true values, (n_vals, n) required stat_sim1 np . ndarray their simulated values, method 1 required stat_sim2 np . ndarray their simulated values, method 2 required colors list [ str ] colors for the various estimates required stat_title str | None main title 'Statistic' subtitle str | None subtitle 'True vs estimated' ncols int | None wrap after ncols columns 3 save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 def plot_true_sim2_facets ( parameter_name : str , parameter_values : np . ndarray , stat_names : list [ str ], stat_true : np . ndarray , stat_sim1 : np . ndarray , stat_sim2 : np . ndarray , colors : list [ str ], stat_title : str | None = \"Statistic\" , subtitle : str | None = \"True vs estimated\" , ncols : int | None = 3 , save : str | None = None , ) -> alt . Chart : \"\"\" Plots simulated values for two methods and true values of statistics as a function of a parameter; one facet per coefficient Args: parameter_name: the name of the parameter parameter_values: a vector of `n_vals` values for the parameter stat_names: the names of the `n` statistics stat_true: their true values, `(n_vals, n)` stat_sim1: their simulated values, method 1 stat_sim2: their simulated values, method 2 colors: colors for the various estimates stat_title: main title subtitle: subtitle ncols: wrap after `ncols` columns save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" n_stats = len ( stat_names ) nvals = check_vector ( parameter_values , \"plot_true_sim2_facets\" ) nv_true , n_stat_true = check_matrix ( stat_true , \"plot_true_sim2_facets\" ) if nv_true != nvals : bs_error_abort ( f \"we have { nvals } parameter values and { nv_true } for stat_true.\" ) if n_stat_true != n_stats : bs_error_abort ( f \"we have { n_stats } names for { n_stat_true } true statistics.\" ) nv_est1 , n_stat_est1 = check_matrix ( stat_sim1 , \"plot_true_sim2_facets\" ) if nv_est1 != nvals : bs_error_abort ( f \"we have { nvals } parameter values and { nv_est1 } for stat_sim1.\" ) if n_stat_est1 != n_stats : bs_error_abort ( f \"we have { n_stats } names for { n_stat_est1 } estimated statistics.\" ) nv_est2 , n_stat_est2 = check_matrix ( stat_sim2 , \"plot_true_sim2_facets\" ) if nv_est2 != nvals : bs_error_abort ( f \"we have { nvals } parameter values and { nv_est2 } for stat_sim2.\" ) if n_stat_est2 != n_stats : bs_error_abort ( f \"we have { n_stats } names for { n_stat_est2 } estimated statistics.\" ) df = pd . DataFrame ( { parameter_name : parameter_values , \"True value\" : stat_true [:, 0 ], \"Estimated1\" : stat_sim1 [:, 0 ], \"Estimated2\" : stat_sim2 [:, 0 ], stat_title : stat_names [ 0 ], } ) for i_stat in range ( 1 , n_stats ): df_i = pd . DataFrame ( { parameter_name : parameter_values , \"True value\" : stat_true [:, i_stat ], \"Estimated1\" : stat_sim1 [:, i_stat ], \"Estimated2\" : stat_sim2 [:, i_stat ], stat_title : stat_names [ i_stat ], } ) df = pd . concat (( df , df_i )) sub_order = [ \"True value\" , \"Estimated1\" , \"Estimated2\" ] dfm = pd . melt ( df , [ parameter_name , stat_title ], var_name = subtitle ) ch = ( alt . Chart ( dfm ) . mark_line () . encode ( x = f \" { parameter_name } :Q\" , y = \"value:Q\" , strokeDash = alt . StrokeDash ( f \" { subtitle } :N\" , sort = sub_order ), color = alt . Color ( f \" { subtitle } :N\" , sort = sub_order , scale = alt . Scale ( domain = sub_order , range = colors ), ), facet = alt . Facet ( f \" { stat_title } :N\" , sort = stat_names , columns = ncols ), ) . resolve_scale ( y = \"independent\" ) ) _maybe_save ( ch , save ) return ch plot_true_sim_facets ( parameter_name , parameter_values , stat_names , stat_true , stat_sim , colors , stat_title = 'Statistic' , subtitle = 'True vs estimated' , ncols = 3 , save = None ) \u00b6 Plots simulated and true values of statistics as a function of a parameter; one facet per coefficient Parameters: Name Type Description Default parameter_name str the name of the parameter required parameter_values np . ndarray a vector of n_vals values for the parameter required stat_names list [ str ] the names of the n statistics required stat_true np . ndarray their true values, (n_vals, n) required stat_sim np . ndarray their simulated values required colors list [ str ] colors for the various estimates required stat_title str | None main title 'Statistic' subtitle str | None subtitle 'True vs estimated' ncols int | None wrap after ncols columns 3 save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 def plot_true_sim_facets ( parameter_name : str , parameter_values : np . ndarray , stat_names : list [ str ], stat_true : np . ndarray , stat_sim : np . ndarray , colors : list [ str ], stat_title : str | None = \"Statistic\" , subtitle : str | None = \"True vs estimated\" , ncols : int | None = 3 , save : str | None = None , ) -> alt . Chart : \"\"\" Plots simulated and true values of statistics as a function of a parameter; one facet per coefficient Args: parameter_name: the name of the parameter parameter_values: a vector of `n_vals` values for the parameter stat_names: the names of the `n` statistics stat_true: their true values, `(n_vals, n)` stat_sim: their simulated values colors: colors for the various estimates stat_title: main title subtitle: subtitle ncols: wrap after `ncols` columns save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" n_stats = len ( stat_names ) nvals = check_vector ( parameter_values , \"plot_true_sim_facets\" ) nv_true , n_stat_true = check_matrix ( stat_true , \"plot_true_sim_facets\" ) if nv_true != nvals : bs_error_abort ( f \"plot_true_sim_facets: we have { nvals } parameter values and { nv_true } for\" \" stat_true.\" ) nv_est , n_stat_est = check_matrix ( stat_sim , \"plot_true_sim_facets\" ) if nv_est != nvals : bs_error_abort ( f \"plot_true_sim_facets: we have { nvals } parameter values and { nv_est } for\" \" stat_sim.\" ) if n_stat_true != n_stats : bs_error_abort ( f \"plot_true_sim_facets: we have { n_stats } names for { n_stat_true } true\" \" statistics.\" ) if n_stat_est != n_stats : bs_error_abort ( f \"plot_true_sim_facets: we have { n_stats } names for { n_stat_est } estimated\" \" statistics.\" ) df = pd . DataFrame ( { parameter_name : parameter_values , \"True value\" : stat_true [:, 0 ], \"Estimated\" : stat_sim [:, 0 ], stat_title : stat_names [ 0 ], } ) for i_stat in range ( 1 , n_stats ): df_i = pd . DataFrame ( { parameter_name : parameter_values , \"True value\" : stat_true [:, i_stat ], \"Estimated\" : stat_sim [:, i_stat ], stat_title : stat_names [ i_stat ], } ) df = pd . concat (( df , df_i )) sub_order = [ \"True value\" , \"Estimated\" ] dfm = pd . melt ( df , [ parameter_name , stat_title ], var_name = subtitle ) ch = ( alt . Chart ( dfm ) . mark_line () . encode ( x = f \" { parameter_name } :Q\" , y = \"value:Q\" , strokeDash = alt . StrokeDash ( f \" { subtitle } :N\" , sort = sub_order ), color = alt . Color ( f \" { subtitle } :N\" , sort = sub_order , scale = alt . Scale ( domain = sub_order , range = colors ), ), facet = alt . Facet ( f \" { stat_title } :N\" , sort = stat_names , columns = ncols ), ) . resolve_scale ( y = \"independent\" ) ) _maybe_save ( ch , save ) return ch","title":"Altair"},{"location":"bs_altair.html#bs_altair-module","text":"Some Altair plots. alt_lineplot , alt_superposed_lineplot , alt_superposed_faceted_lineplot alt_plot_fun : plots a function alt_density , alt_faceted_densities : plots the density of x , or of x conditional on a category alt_superposed_faceted_densities : plots the density of x superposed by f and faceted by g alt_scatterplot , alt_scatterplot_with_histo , alt-linked_scatterplots : variants of scatter plots alt_histogram_by , alt_histogram_continuous : histograms of x by y , and of a continuous x alt_stacked_area , alt_stacked_area_facets : stacked area plots plot_parameterized_estimates : plots densities of estimates of coefficients, with the true values, as a function of a parameter plot_true_sim_facets, plot_true_sim2_facets : plot two simulated values and the true values of statistics as a function of a parameter alt_tick_plots : vertically arranged tick plots of variables.","title":"bs_altair module"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_density","text":"Plots the density of df[str_x] . Parameters: Name Type Description Default df pd . DataFrame the data with the str_x variable required str_x str the name of a continuous column required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def alt_density ( df : pd . DataFrame , str_x : str , save : str | None = None ) -> alt . Chart : \"\"\"Plots the density of `df[str_x]`. Args: df: the data with the `str_x` variable str_x: the name of a continuous column save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" ch = ( alt . Chart ( df ) . transform_density ( str_x , as_ = [ str_x , \"Density\" ], ) . mark_area ( opacity = 0.4 ) . encode ( x = f \" { str_x } :Q\" , y = \"Density:Q\" , ) ) _maybe_save ( ch , save ) return ch","title":"alt_density()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_faceted_densities","text":"Plots the density of df[str_x] by df[str_f] in column facets Parameters: Name Type Description Default df pd . DataFrame the data with the str_x and str_f variables required str_x str the name of a continuous column required str_f str the name of a categorical column required legend_title str | None a title for the legend None save str | None the name of a file to save to (HTML extension will be added) None max_cols int | None we wrap after that number of columns 4 Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def alt_faceted_densities ( df : pd . DataFrame , str_x : str , str_f : str , legend_title : str | None = None , save : str | None = None , max_cols : int | None = 4 , ) -> alt . Chart : \"\"\" Plots the density of `df[str_x]` by `df[str_f]` in column facets Args: df: the data with the `str_x` and `str_f` variables str_x: the name of a continuous column str_f: the name of a categorical column legend_title: a title for the legend save: the name of a file to save to (HTML extension will be added) max_cols: we wrap after that number of columns Returns: the `alt.Chart` object. \"\"\" our_legend_title = str_f if legend_title is None else legend_title ch = ( alt . Chart ( df ) . transform_density ( str_x , groupby = [ str_f ], as_ = [ str_x , \"Density\" ], ) . mark_area ( opacity = 0.4 ) . encode ( x = f \" { str_x } :Q\" , y = \"Density:Q\" , color = alt . Color ( f \" { str_f } :N\" , title = our_legend_title ), ) . facet ( f \" { str_f } :N\" , columns = max_cols ) ) _maybe_save ( ch , save ) return ch","title":"alt_faceted_densities()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_histogram_by","text":"Plots a histogram of a statistic of str_y by str_x Parameters: Name Type Description Default df pd . DataFrame a dataframe with columns str_x and str_y required str_x str a categorical variable required str_y str a continuous variable required str_agg str | None how we aggregate the values of str_y by str_x 'mean' save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the Altair chart. Source code in bs_python_utils/bs_altair.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 def alt_histogram_by ( df : pd . DataFrame , str_x : str , str_y : str , str_agg : str | None = \"mean\" , save : str | None = None , ) -> alt . Chart : \"\"\" Plots a histogram of a statistic of `str_y` by `str_x` Args: df: a dataframe with columns `str_x` and `str_y` str_x: a categorical variable str_y: a continuous variable str_agg: how we aggregate the values of `str_y` by `str_x` save: the name of a file to save to (HTML extension will be added) Returns: the Altair chart. \"\"\" ch = ( alt . Chart ( df ) . mark_bar () . encode ( x = str_x , y = f \" { str_agg } ( { str_y } ):Q\" ) . properties ( height = 300 , width = 400 ) ) _maybe_save ( ch , save ) return ch","title":"alt_histogram_by()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_histogram_continuous","text":"Histogram of a continuous variable df[str_x] Parameters: Name Type Description Default df pd . DataFrame the data with the str_x , str_y , and str_f variables required str_x str the name of a continuous column required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 def alt_histogram_continuous ( df : pd . DataFrame , str_x : str , save : str | None = None ) -> alt . Chart : \"\"\" Histogram of a continuous variable `df[str_x]` Args: df: the data with the `str_x`, `str_y`, and `str_f` variables str_x: the name of a continuous column save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" ch = alt . Chart ( df ) . mark_bar () . encode ( alt . X ( str_x , bin = True ), y = \"count()\" ) _maybe_save ( ch , save ) return ch","title":"alt_histogram_continuous()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_lineplot","text":"Scatterplot of df[str_x] vs df[str_y] Parameters: Name Type Description Default df pd . DataFrame the data with columns str_x and str_y required str_x str the name of a continuous column required str_y str the name of a continuous column required time_series bool True if x is a time series False save str | None the name of a file to save to (HTML extension will be added) None aggreg str | None the name of an aggregating function for y None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def alt_lineplot ( df : pd . DataFrame , str_x : str , str_y : str , time_series : bool = False , save : str | None = None , aggreg : str | None = None , ** kwargs , ) -> alt . Chart : \"\"\" Scatterplot of `df[str_x]` vs `df[str_y]` Args: df: the data with columns `str_x` and `str_y` str_x: the name of a continuous column str_y: the name of a continuous column time_series: `True` if x is a time series save: the name of a file to save to (HTML extension will be added) aggreg: the name of an aggregating function for `y` Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" var_y = f \" { aggreg } ( { str_y } ):Q\" if aggreg is not None else str_y ch = alt . Chart ( df ) . mark_line () . encode ( x = f \" { str_x } : { type_x } \" , y = var_y ) if \"title\" in kwargs : ch = ch . properties ( title = kwargs [ \"title\" ]) _maybe_save ( ch , save ) return ch","title":"alt_lineplot()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_linked_scatterplots","text":"Creates two scatterplots: of df[str_x1] vs df[str_y] and of df[str_x2] vs df[str_y] , both with color as per df[str_f] . Selecting an interval in one shows up in the other. Parameters: Name Type Description Default df pd . DataFrame required str_x1 str the name of a continuous column required str_x2 str the name of a continuous column required str_y str the name of a continuous column required str_f str the name of a categorical column required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def alt_linked_scatterplots ( df : pd . DataFrame , str_x1 : str , str_x2 : str , str_y : str , str_f : str , save : str | None = None , ) -> alt . Chart : \"\"\" Creates two scatterplots: of `df[str_x1]` vs `df[str_y]` and of `df[str_x2]` vs `df[str_y]`, both with color as per `df[str_f]`. Selecting an interval in one shows up in the other. Args: df: str_x1: the name of a continuous column str_x2: the name of a continuous column str_y: the name of a continuous column str_f: the name of a categorical column save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" interval = alt . selection_interval () base = ( alt . Chart ( df ) . mark_point () . encode ( y = f \" { str_y } :Q\" , color = alt . condition ( interval , str_f , alt . value ( \"lightgray\" )) ) . properties ( selection = interval ) ) ch = base . encode ( x = f \" { str_x1 } :Q\" ) | base . encode ( x = f \" { str_x2 } :Q\" ) _maybe_save ( ch , save ) return ch","title":"alt_linked_scatterplots()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_plot_fun","text":"Plots the function f from start to end . Parameters: Name Type Description Default f Callable returns a Numpy array from a Numpy array required start float first point on x axis required end float last point on x axis required npoints int number of points 100 save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def alt_plot_fun ( f : Callable , start : float , end : float , npoints : int = 100 , save : str | None = None , ) -> alt . Chart : \"\"\" Plots the function `f` from `start` to `end`. Args: f: returns a Numpy array from a Numpy array start: first point on `x` axis end: last point on `x` axis npoints: number of points save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" step = ( end - start ) / npoints points = np . arange ( start , end + step , step ) fun_data = pd . DataFrame ({ \"x\" : points , \"y\" : f ( points )}) ch = ( alt . Chart ( fun_data ) . mark_line () . encode ( x = \"x:Q\" , y = \"y:Q\" , ) ) _maybe_save ( ch , save ) return ch","title":"alt_plot_fun()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_scatterplot","text":"Scatterplot of df[str_x] vs df[str_y] . Parameters: Name Type Description Default df pd . DataFrame the data with columns for x, y required str_x str the name of a continuous x column required str_y str the name of a continuous y column required time_series bool True if x is a time series False xlabel str | None label for the horizontal axis None ylabel str | None label for the vertical axis None title str | None title for the graph None size int | None radius of the circles 30 color str | None variable that determines the color of the circles None selection bool if True , the user can select interactively from the color legend, if any False save str | None the name of a file to save to (HTML extension will be added) None aggreg str | None the name of an aggregating function for y None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def alt_scatterplot ( df : pd . DataFrame , str_x : str , str_y : str , time_series : bool = False , save : str | None = None , xlabel : str | None = None , ylabel : str | None = None , size : int | None = 30 , title : str | None = None , color : str | None = None , aggreg : str | None = None , selection : bool = False , ) -> alt . Chart : \"\"\" Scatterplot of `df[str_x]` vs `df[str_y]`. Args: df: the data with columns for x, y str_x: the name of a continuous x column str_y: the name of a continuous y column time_series: `True` if x is a time series xlabel: label for the horizontal axis ylabel: label for the vertical axis title: title for the graph size: radius of the circles color: variable that determines the color of the circles selection: if `True`, the user can select interactively from the `color` legend, if any save: the name of a file to save to (HTML extension will be added) aggreg: the name of an aggregating function for `y` Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" var_x = alt . X ( f \" { str_x } : { type_x } \" ) if xlabel is not None : if isinstance ( xlabel , str ): var_x = alt . X ( f \" { str_x } : { type_x } \" , axis = alt . Axis ( title = xlabel )) else : bs_error_abort ( f \"xlabel must be a string, not { xlabel } \" ) var_y = f \" { aggreg } ( { str_y } ):Q\" if aggreg is not None else str_y if ylabel is not None : if isinstance ( ylabel , str ): var_y = alt . Y ( var_y , axis = alt . Axis ( title = ylabel )) else : bs_error_abort ( f \"ylabel must be a string, not { ylabel } \" ) if isinstance ( size , int ): circles_size = size else : bs_error_abort ( f \"size must be an integer, not { size } \" ) if color is not None : if isinstance ( color , str ): if selection : selection_criterion = alt . selection_multi ( fields = [ color ], bind = \"legend\" ) ch = ( alt . Chart ( df ) . mark_circle ( size = circles_size ) . encode ( x = var_x , y = var_y , color = color , opacity = alt . condition ( selection_criterion , alt . value ( 1 ), alt . value ( 0.1 ) ), ) . add_selection ( selection_criterion ) ) else : ch = ( alt . Chart ( df ) . mark_circle ( size = circles_size ) . encode ( x = var_x , y = var_y , color = color ) ) else : bs_error_abort ( f \"color must be a string, not { color } \" ) else : ch = alt . Chart ( df ) . mark_circle ( size = circles_size ) . encode ( x = var_x , y = var_y ) ch = _add_title ( ch , title ) _maybe_save ( ch , save ) return ch","title":"alt_scatterplot()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_scatterplot_with_histo","text":"Scatterplot of df[str_x] vs df[str_y] with colors as per df[str_f] allows to select an interval and histograns the counts of df[str_f] in the interval. Parameters: Name Type Description Default df pd . DataFrame the data with the str_x and str_f variables required str_x str the name of a continuous column required str_y str the name of a continuous column required str_f str the name of a categorical column required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def alt_scatterplot_with_histo ( df : pd . DataFrame , str_x : str , str_y : str , str_f : str , save : str | None = None ) -> alt . Chart : \"\"\" Scatterplot of `df[str_x]` vs `df[str_y]` with colors as per `df[str_f]` allows to select an interval and histograns the counts of `df[str_f]` in the interval. Args: df: the data with the `str_x` and `str_f` variables str_x: the name of a continuous column str_y: the name of a continuous column str_f: the name of a categorical column save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" interval = alt . selection_interval () points = ( alt . Chart ( df ) . mark_point () . encode ( x = f \" { str_x } :Q\" , y = f \" { str_y } :Q\" , color = alt . condition ( interval , str_f , alt . value ( \"lightgray\" )), ) . properties ( selection = interval ) ) histogram = ( alt . Chart ( df ) . mark_bar () . encode ( x = \"count()\" , y = str_f , color = str_f , ) . transform_filter ( interval ) ) ch = points & histogram _maybe_save ( ch , save ) return ch","title":"alt_scatterplot_with_histo()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_stacked_area","text":"Normalized stacked lineplots of df[str_x] vs df[str_y] by df[str_f] Parameters: Name Type Description Default df pd . DataFrame the data with columns for str_x , str_y , and str_f required str_x str the name of a continuous column required str_y str the name of a continuous column required str_f str the name of a categorical column required time_series bool True if str_x is a time series False title str | None a title for the plot None save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 def alt_stacked_area ( df : pd . DataFrame , str_x : str , str_y : str , str_f : str , time_series : bool = False , title : str | None = None , save : str | None = None , ) -> alt . Chart : \"\"\" Normalized stacked lineplots of `df[str_x]` vs `df[str_y]` by `df[str_f]` Args: df: the data with columns for `str_x`, `str_y`, and `str_f` str_x: the name of a continuous column str_y: the name of a continuous column str_f: the name of a categorical column time_series: `True` if `str_x` is a time series title: a title for the plot save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" ch = ( alt . Chart ( df ) . mark_area () . encode ( x = f \" { str_x } : { type_x } \" , y = alt . Y ( f \" { str_y } :Q\" , stack = \"normalize\" ), color = f \" { str_f } :N\" , ) ) if title is not None : ch = ch . properties ( title = title ) _maybe_save ( ch , save ) return ch","title":"alt_stacked_area()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_stacked_area_facets","text":"Normalized stacked lineplots of df[str_x] vs df[str_y] by df[str_f] , faceted by df[str_g] Parameters: Name Type Description Default df pd . DataFrame the data with columns for str_x , str_y , and str_f required str_x str the name of a continuous column required str_y str the name of a continuous column required str_f str the name of a categorical column required str_g str the name of a categorical column required time_series bool True if str_x is a time series False title str | None a title for the plot None save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 def alt_stacked_area_facets ( df : pd . DataFrame , str_x : str , str_y : str , str_f : str , str_g : str , time_series : bool = False , max_cols : int | None = 5 , title : str | None = None , save : str | None = None , ) -> alt . Chart : \"\"\" Normalized stacked lineplots of `df[str_x]` vs `df[str_y]` by `df[str_f]`, faceted by `df[str_g]` Args: df: the data with columns for `str_x`, `str_y`, and `str_f` str_x: the name of a continuous column str_y: the name of a continuous column str_f: the name of a categorical column str_g: the name of a categorical column time_series: `True` if `str_x` is a time series title: a title for the plot save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" ch = ( alt . Chart ( df ) . mark_area () . encode ( x = f \" { str_x } : { type_x } \" , y = alt . Y ( f \" { str_y } :Q\" , stack = \"normalize\" ), color = f \" { str_f } :N\" , facet = alt . Facet ( f \" { str_g } :N\" , columns = max_cols ), ) ) _maybe_save ( ch , save ) return ch","title":"alt_stacked_area_facets()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_superposed_faceted_densities","text":"Creates density plots of df[str_x] by df[str_f] and df[str_g] with color as per df[str_f] and faceted by df[str_g] . that is: facets by str_g , with densities conditional on str_f superposed. Parameters: Name Type Description Default df pd . DataFrame a Pandas dataframe wity columns str_x , str_f , str_g required str_x str the name of a continuous column required str_f str the name of a categorical column required str_g str the name of a categorical column required max_cols int | None the number of columns after whcih we wrap 4 save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def alt_superposed_faceted_densities ( df : pd . DataFrame , str_x : str , str_f : str , str_g : str , max_cols : int | None = 4 , save : str | None = None , ) -> alt . Chart : \"\"\" Creates density plots of `df[str_x]` by `df[str_f]` and `df[str_g]` with color as per `df[str_f]` and faceted by `df[str_g]`. that is: facets by `str_g`, with densities conditional on `str_f` superposed. Args: df: a Pandas dataframe wity columns `str_x`, `str_f`, `str_g` str_x: the name of a continuous column str_f: the name of a categorical column str_g: the name of a categorical column max_cols: the number of columns after whcih we wrap save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" densities = ( alt . Chart ( df ) . transform_density ( str_x , groupby = [ str_f , str_g ], as_ = [ str_x , \"Density\" ], ) . mark_line () . encode ( x = f \" { str_x } :Q\" , y = \"Density:Q\" , color = f \" { str_f } :N\" , ) . facet ( column = f \" { str_g } :N\" , columns = max_cols ) . resolve_scale ( x = \"independent\" , y = \"independent\" ) ) _maybe_save ( densities , save ) return densities","title":"alt_superposed_faceted_densities()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_superposed_faceted_lineplot","text":"Plots df[str_x] vs df[str_y] superposed by df[str_f] and faceted by df[str_g] Parameters: Name Type Description Default df pd . DataFrame the data with the str_x , str_y , and str_f variables required str_x str the name of a continuous column required str_y str the name of a continuous column required str_f str the name of a categorical column required str_g str the name of a categorical column required time_series bool True if str_x is a time series False legend_title str | None a title for the legend None save str | None the name of a file to save to (HTML extension will be added) None max_cols int | None we wrap after that number of columns 5 Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 def alt_superposed_faceted_lineplot ( df : pd . DataFrame , str_x : str , str_y : str , str_f : str , str_g : str , time_series : bool = False , legend_title : str | None = None , max_cols : int | None = 5 , save : str | None = None , ) -> alt . Chart : \"\"\" Plots `df[str_x]` vs `df[str_y]` superposed by `df[str_f]` and faceted by `df[str_g]` Args: df: the data with the `str_x`, `str_y`, and `str_f` variables str_x: the name of a continuous column str_y: the name of a continuous column str_f: the name of a categorical column str_g: the name of a categorical column time_series: `True` if `str_x` is a time series legend_title: a title for the legend save: the name of a file to save to (HTML extension will be added) max_cols: we wrap after that number of columns Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" our_title = str_f if legend_title is None else legend_title ch = ( alt . Chart ( df ) . mark_line () . encode ( x = f \" { str_x } : { type_x } \" , y = f \" { str_y } :Q\" , color = alt . Color ( f \" { str_f } :N\" , title = our_title ), facet = alt . Facet ( f \" { str_g } :N\" , columns = max_cols ), ) ) _maybe_save ( ch , save ) return ch","title":"alt_superposed_faceted_lineplot()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_superposed_lineplot","text":"Plots df[str_x] vs df[str_y] by df[str_f] on one plot Parameters: Name Type Description Default df pd . DataFrame the data with the str_x , str_y , and str_f variables required str_x str the name of a continuous x column required str_y str the name of a continuous y column required str_f str the name of a categorical f column required time_series bool True if str_x is a time series False legend_title str | None a title for the legend None save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def alt_superposed_lineplot ( df : pd . DataFrame , str_x : str , str_y : str , str_f : str , time_series : bool = False , legend_title : str | None = None , save : str | None = None , ) -> alt . Chart : \"\"\" Plots `df[str_x]` vs `df[str_y]` by `df[str_f]` on one plot Args: df: the data with the `str_x`, `str_y`, and `str_f` variables str_x: the name of a continuous `x` column str_y: the name of a continuous `y` column str_f: the name of a categorical `f` column time_series: `True` if `str_x` is a time series legend_title: a title for the legend save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" type_x = \"T\" if time_series else \"Q\" our_legend_title = str_f if legend_title is None else legend_title ch = ( alt . Chart ( df ) . mark_line () . encode ( x = f \" { str_x } : { type_x } \" , y = f \" { str_y } :Q\" , color = alt . Color ( f \" { str_f } :N\" , title = our_legend_title ), ) ) _maybe_save ( ch , save ) return ch","title":"alt_superposed_lineplot()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.alt_tick_plots","text":"Creates a tick plot of the variables in list_vars of df , arranged vertically. Parameters: Name Type Description Default df pd . DataFrame a dataframe with the variables in list_vars required list_vars str | list [ str ] the name of a column of df , or a list of names required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 def alt_tick_plots ( df : pd . DataFrame , list_vars : str | list [ str ], save : str | None = None ) -> alt . Chart : \"\"\" Creates a tick plot of the variables in `list_vars` of`df`, arranged vertically. Args: df: a dataframe with the variables in `list_vars` list_vars: the name of a column of `df`, or a list of names save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" if isinstance ( list_vars , str ): varname = list_vars ch = alt . Chart ( df ) . encode ( x = varname ) . mark_tick () else : ch = ( alt . Chart ( df ) . encode ( alt . X ( alt . repeat ( \"row\" ), type = \"quantitative\" )) . mark_tick () . repeat ( row = list_vars ) . resolve_scale ( y = \"independent\" ) ) _maybe_save ( ch , save ) return ch","title":"alt_tick_plots()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.plot_parameterized_estimates","text":"Plots estimates of coefficients, with the true values, as a function of a parameter; one facet per coefficient Parameters: Name Type Description Default parameter_name str the name of the parameter required parameter_values np . ndarray a vector of n_vals values for the parameter required coeff_names str | list [ str ] the names of the n_coeffs coefficients required true_values np . ndarray their true values, depending on the parameter or not required estimate_names str | list [ str ] names of the estimates required estimates np . ndarray their values required colors list [ str ] colors for the various estimates required save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 def plot_parameterized_estimates ( parameter_name : str , parameter_values : np . ndarray , coeff_names : str | list [ str ], true_values : np . ndarray , estimate_names : str | list [ str ], estimates : np . ndarray , colors : list [ str ], save : str | None = None , ) -> alt . Chart : \"\"\" Plots estimates of coefficients, with the true values, as a function of a parameter; one facet per coefficient Args: parameter_name: the name of the parameter parameter_values: a vector of `n_vals` values for the parameter coeff_names: the names of the `n_coeffs` coefficients true_values: their true values, depending on the parameter or not estimate_names: names of the estimates estimates: their values colors: colors for the various estimates save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" n_vals = check_vector ( parameter_values ) n_coeffs = 1 if isinstance ( coeff_names , str ) else len ( coeff_names ) if n_coeffs == 1 : n_true = check_vector ( true_values , \"plot_parameterized_estimates\" ) if n_true != n_vals : bs_error_abort ( f \"plot_parameterized_estimates: we have { n_true } values and\" f \" { n_vals } parameter values.\" ) df = pd . DataFrame ({ parameter_name : parameter_values , \"True value\" : true_values }) df1 , ordered_estimates = _stack_estimates ( estimate_names , estimates , df ) df1m = pd . melt ( df1 , parameter_name , var_name = \"Estimate\" ) ch = ( alt . Chart ( df1m ) . mark_line () . encode ( x = f \" { parameter_name } :Q\" , y = \"value:Q\" , strokeDash = alt . StrokeDash ( \"Estimate:N\" , sort = ordered_estimates ), color = alt . Color ( \"Estimate:N\" , sort = estimate_names , scale = alt . Scale ( domain = ordered_estimates , range = colors ), ), ) ) else : n_true , n_c = check_matrix ( true_values , \"plot_parameterized_estimates\" ) if n_true != n_vals : bs_error_abort ( f \"plot_parameterized_estimates: we have { n_true } true values and\" f \" { n_vals } parameter values.\" ) if n_c != n_coeffs : bs_error_abort ( f \"plot_parameterized_estimates: we have { n_c } columns of true values\" f \" and { n_coeffs } coefficients.\" ) df1 = [ None ] * n_coeffs for i_coeff , coeff in enumerate ( coeff_names ): df_i = pd . DataFrame ( { parameter_name : parameter_values , \"True value\" : true_values [:, i_coeff ], } ) df1 [ i_coeff ], ordered_estimates = _stack_estimates ( estimate_names , estimates [ ... , i_coeff ], df_i ) df1 [ i_coeff ][ \"Coefficient\" ] = coeff df2 = pd . concat ( df1 [ i_coeff ] for i_coeff in range ( n_coeffs )) ordered_colors = colors df2m = pd . melt ( df2 , [ parameter_name , \"Coefficient\" ], var_name = \"Estimate\" ) ch = ( alt . Chart ( df2m ) . mark_line () . encode ( x = f \" { parameter_name } :Q\" , y = \"value:Q\" , strokeDash = alt . StrokeDash ( \"Estimate:N\" , sort = ordered_estimates ), color = alt . Color ( \"Estimate:N\" , sort = ordered_estimates , scale = alt . Scale ( domain = ordered_estimates , range = ordered_colors ), ), ) . facet ( alt . Facet ( \"Coefficient:N\" , sort = coeff_names )) . resolve_scale ( y = \"independent\" ) ) _maybe_save ( ch , save ) return ch","title":"plot_parameterized_estimates()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.plot_true_sim2_facets","text":"Plots simulated values for two methods and true values of statistics as a function of a parameter; one facet per coefficient Parameters: Name Type Description Default parameter_name str the name of the parameter required parameter_values np . ndarray a vector of n_vals values for the parameter required stat_names list [ str ] the names of the n statistics required stat_true np . ndarray their true values, (n_vals, n) required stat_sim1 np . ndarray their simulated values, method 1 required stat_sim2 np . ndarray their simulated values, method 2 required colors list [ str ] colors for the various estimates required stat_title str | None main title 'Statistic' subtitle str | None subtitle 'True vs estimated' ncols int | None wrap after ncols columns 3 save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 def plot_true_sim2_facets ( parameter_name : str , parameter_values : np . ndarray , stat_names : list [ str ], stat_true : np . ndarray , stat_sim1 : np . ndarray , stat_sim2 : np . ndarray , colors : list [ str ], stat_title : str | None = \"Statistic\" , subtitle : str | None = \"True vs estimated\" , ncols : int | None = 3 , save : str | None = None , ) -> alt . Chart : \"\"\" Plots simulated values for two methods and true values of statistics as a function of a parameter; one facet per coefficient Args: parameter_name: the name of the parameter parameter_values: a vector of `n_vals` values for the parameter stat_names: the names of the `n` statistics stat_true: their true values, `(n_vals, n)` stat_sim1: their simulated values, method 1 stat_sim2: their simulated values, method 2 colors: colors for the various estimates stat_title: main title subtitle: subtitle ncols: wrap after `ncols` columns save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" n_stats = len ( stat_names ) nvals = check_vector ( parameter_values , \"plot_true_sim2_facets\" ) nv_true , n_stat_true = check_matrix ( stat_true , \"plot_true_sim2_facets\" ) if nv_true != nvals : bs_error_abort ( f \"we have { nvals } parameter values and { nv_true } for stat_true.\" ) if n_stat_true != n_stats : bs_error_abort ( f \"we have { n_stats } names for { n_stat_true } true statistics.\" ) nv_est1 , n_stat_est1 = check_matrix ( stat_sim1 , \"plot_true_sim2_facets\" ) if nv_est1 != nvals : bs_error_abort ( f \"we have { nvals } parameter values and { nv_est1 } for stat_sim1.\" ) if n_stat_est1 != n_stats : bs_error_abort ( f \"we have { n_stats } names for { n_stat_est1 } estimated statistics.\" ) nv_est2 , n_stat_est2 = check_matrix ( stat_sim2 , \"plot_true_sim2_facets\" ) if nv_est2 != nvals : bs_error_abort ( f \"we have { nvals } parameter values and { nv_est2 } for stat_sim2.\" ) if n_stat_est2 != n_stats : bs_error_abort ( f \"we have { n_stats } names for { n_stat_est2 } estimated statistics.\" ) df = pd . DataFrame ( { parameter_name : parameter_values , \"True value\" : stat_true [:, 0 ], \"Estimated1\" : stat_sim1 [:, 0 ], \"Estimated2\" : stat_sim2 [:, 0 ], stat_title : stat_names [ 0 ], } ) for i_stat in range ( 1 , n_stats ): df_i = pd . DataFrame ( { parameter_name : parameter_values , \"True value\" : stat_true [:, i_stat ], \"Estimated1\" : stat_sim1 [:, i_stat ], \"Estimated2\" : stat_sim2 [:, i_stat ], stat_title : stat_names [ i_stat ], } ) df = pd . concat (( df , df_i )) sub_order = [ \"True value\" , \"Estimated1\" , \"Estimated2\" ] dfm = pd . melt ( df , [ parameter_name , stat_title ], var_name = subtitle ) ch = ( alt . Chart ( dfm ) . mark_line () . encode ( x = f \" { parameter_name } :Q\" , y = \"value:Q\" , strokeDash = alt . StrokeDash ( f \" { subtitle } :N\" , sort = sub_order ), color = alt . Color ( f \" { subtitle } :N\" , sort = sub_order , scale = alt . Scale ( domain = sub_order , range = colors ), ), facet = alt . Facet ( f \" { stat_title } :N\" , sort = stat_names , columns = ncols ), ) . resolve_scale ( y = \"independent\" ) ) _maybe_save ( ch , save ) return ch","title":"plot_true_sim2_facets()"},{"location":"bs_altair.html#bs_python_utils.bs_altair.plot_true_sim_facets","text":"Plots simulated and true values of statistics as a function of a parameter; one facet per coefficient Parameters: Name Type Description Default parameter_name str the name of the parameter required parameter_values np . ndarray a vector of n_vals values for the parameter required stat_names list [ str ] the names of the n statistics required stat_true np . ndarray their true values, (n_vals, n) required stat_sim np . ndarray their simulated values required colors list [ str ] colors for the various estimates required stat_title str | None main title 'Statistic' subtitle str | None subtitle 'True vs estimated' ncols int | None wrap after ncols columns 3 save str | None the name of a file to save to (HTML extension will be added) None Returns: Type Description alt . Chart the alt.Chart object. Source code in bs_python_utils/bs_altair.py 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 def plot_true_sim_facets ( parameter_name : str , parameter_values : np . ndarray , stat_names : list [ str ], stat_true : np . ndarray , stat_sim : np . ndarray , colors : list [ str ], stat_title : str | None = \"Statistic\" , subtitle : str | None = \"True vs estimated\" , ncols : int | None = 3 , save : str | None = None , ) -> alt . Chart : \"\"\" Plots simulated and true values of statistics as a function of a parameter; one facet per coefficient Args: parameter_name: the name of the parameter parameter_values: a vector of `n_vals` values for the parameter stat_names: the names of the `n` statistics stat_true: their true values, `(n_vals, n)` stat_sim: their simulated values colors: colors for the various estimates stat_title: main title subtitle: subtitle ncols: wrap after `ncols` columns save: the name of a file to save to (HTML extension will be added) Returns: the `alt.Chart` object. \"\"\" n_stats = len ( stat_names ) nvals = check_vector ( parameter_values , \"plot_true_sim_facets\" ) nv_true , n_stat_true = check_matrix ( stat_true , \"plot_true_sim_facets\" ) if nv_true != nvals : bs_error_abort ( f \"plot_true_sim_facets: we have { nvals } parameter values and { nv_true } for\" \" stat_true.\" ) nv_est , n_stat_est = check_matrix ( stat_sim , \"plot_true_sim_facets\" ) if nv_est != nvals : bs_error_abort ( f \"plot_true_sim_facets: we have { nvals } parameter values and { nv_est } for\" \" stat_sim.\" ) if n_stat_true != n_stats : bs_error_abort ( f \"plot_true_sim_facets: we have { n_stats } names for { n_stat_true } true\" \" statistics.\" ) if n_stat_est != n_stats : bs_error_abort ( f \"plot_true_sim_facets: we have { n_stats } names for { n_stat_est } estimated\" \" statistics.\" ) df = pd . DataFrame ( { parameter_name : parameter_values , \"True value\" : stat_true [:, 0 ], \"Estimated\" : stat_sim [:, 0 ], stat_title : stat_names [ 0 ], } ) for i_stat in range ( 1 , n_stats ): df_i = pd . DataFrame ( { parameter_name : parameter_values , \"True value\" : stat_true [:, i_stat ], \"Estimated\" : stat_sim [:, i_stat ], stat_title : stat_names [ i_stat ], } ) df = pd . concat (( df , df_i )) sub_order = [ \"True value\" , \"Estimated\" ] dfm = pd . melt ( df , [ parameter_name , stat_title ], var_name = subtitle ) ch = ( alt . Chart ( dfm ) . mark_line () . encode ( x = f \" { parameter_name } :Q\" , y = \"value:Q\" , strokeDash = alt . StrokeDash ( f \" { subtitle } :N\" , sort = sub_order ), color = alt . Color ( f \" { subtitle } :N\" , sort = sub_order , scale = alt . Scale ( domain = sub_order , range = colors ), ), facet = alt . Facet ( f \" { stat_title } :N\" , sort = stat_names , columns = ncols ), ) . resolve_scale ( y = \"independent\" ) ) _maybe_save ( ch , save ) return ch","title":"plot_true_sim_facets()"},{"location":"bs_logging.html","text":"bs_logging module \u00b6 Utilities for logging: init_logger initializes and returns a customized logger log_execution ia a decorator to log entry into and ext from a function. init_logger ( logger_name , log_level_for_console = 'info' , log_level_for_file = 'debug' , save_dir = None ) \u00b6 Initialize a logger Parameters: Name Type Description Default logger_name str name for the logger required log_level_for_console str minimum level of messages logged to the console logging 'info' log_level_for_file str 'debug' save_dir str None Returns: Type Description logging . Logger the logger Examples: >>> logger_dir = \"logs\" >>> logger_name = \"check_log\" >>> logger = init_logger ( logger_name , save_dir = logger_dir ) This will create two logs: one printed to console where we run the code (the StreamHandler ), and one that will be saved to file save_dir/logger_name.txt (the FileHandler ). 'logger.propagate = False' makes sure that the logs sent to file will not be printed to console. We use the Formatter class to define the format of the logs. Here: The time of the log in a human-readable format, asctime levelname is the level of the log, one out of INFO, DEBUG, WARNING, ERROR, CRITICAL . The name of the file, filename , from which the log was generated, and the line number, lineno . Lastly, the message itself \u2014 message . The default has only INFO logs and above (i.e., also WARNING, ERROR and CRITICAL ) displayed in the console; the file will also include DEBUG logs. Source code in bs_python_utils/bs_logging.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def init_logger ( logger_name : str , log_level_for_console : str = \"info\" , log_level_for_file : str = \"debug\" , save_dir : str = None , ) -> logging . Logger : \"\"\" Initialize a logger Args: logger_name: name for the logger log_level_for_console: minimum level of messages logged to the console logging log_level_for_file: save_dir: Returns: the logger Examples: >>> logger_dir = \"logs\" >>> logger_name = \"check_log\" >>> logger = init_logger(logger_name, save_dir=logger_dir) This will create two logs: * one printed to console where we run the code (the `StreamHandler`), * and one that will be saved to file `save_dir/logger_name.txt` (the `FileHandler`). `'logger.propagate = False'` makes sure that the logs sent to file will not be printed to console. We use the `Formatter` class to define the format of the logs. Here: * The time of the log in a human-readable format, `asctime` * `levelname` is the level of the log, one out of `INFO, DEBUG, WARNING, ERROR, CRITICAL`. * The name of the file, `filename`, from which the log was generated, and the line number, `lineno`. * Lastly, the message itself \u2014 `message`. The default has only `INFO` logs and above (i.e., also `WARNING, ERROR` and `CRITICAL`) displayed in the console; the file will also include `DEBUG` logs. \"\"\" logger = logging . getLogger () logger . setLevel ( level = logging . DEBUG ) logger . propagate = False formatter = logging . Formatter ( \" %(asctime)s [ %(levelname)s ] %(filename)s %(lineno)d - %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ch = logging . StreamHandler () ch . setLevel ( log_level_for_console . upper ()) ch . setFormatter ( formatter ) logger . addHandler ( ch ) if save_dir is not None : Path ( save_dir ) . mkdir ( exist_ok = True , parents = True ) fh = logging . FileHandler ( save_dir + f \"/ { logger_name } .txt\" ) fh . setLevel ( log_level_for_file . upper ()) fh . setFormatter ( formatter ) logger . addHandler ( fh ) return logger log_execution ( func ) \u00b6 Decorator to log the execution of a function. Only records entry to and exit from the function, to the console. Source code in bs_python_utils/bs_logging.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def log_execution ( func : Callable ) -> Callable : \"\"\"Decorator to log the execution of a function. Only records entry to and exit from the function, to the console. \"\"\" loglevel = logging . info @functools . wraps ( func ) def wrapper ( * args , ** kwargs ): loglevel ( f \"Executing { func . __name__ } \" ) result = func ( * args , ** kwargs ) loglevel ( f \"Finished executing { func . __name__ } \" ) return result return wrapper","title":"logging"},{"location":"bs_logging.html#bs_logging-module","text":"Utilities for logging: init_logger initializes and returns a customized logger log_execution ia a decorator to log entry into and ext from a function.","title":"bs_logging module"},{"location":"bs_logging.html#bs_python_utils.bs_logging.init_logger","text":"Initialize a logger Parameters: Name Type Description Default logger_name str name for the logger required log_level_for_console str minimum level of messages logged to the console logging 'info' log_level_for_file str 'debug' save_dir str None Returns: Type Description logging . Logger the logger Examples: >>> logger_dir = \"logs\" >>> logger_name = \"check_log\" >>> logger = init_logger ( logger_name , save_dir = logger_dir ) This will create two logs: one printed to console where we run the code (the StreamHandler ), and one that will be saved to file save_dir/logger_name.txt (the FileHandler ). 'logger.propagate = False' makes sure that the logs sent to file will not be printed to console. We use the Formatter class to define the format of the logs. Here: The time of the log in a human-readable format, asctime levelname is the level of the log, one out of INFO, DEBUG, WARNING, ERROR, CRITICAL . The name of the file, filename , from which the log was generated, and the line number, lineno . Lastly, the message itself \u2014 message . The default has only INFO logs and above (i.e., also WARNING, ERROR and CRITICAL ) displayed in the console; the file will also include DEBUG logs. Source code in bs_python_utils/bs_logging.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def init_logger ( logger_name : str , log_level_for_console : str = \"info\" , log_level_for_file : str = \"debug\" , save_dir : str = None , ) -> logging . Logger : \"\"\" Initialize a logger Args: logger_name: name for the logger log_level_for_console: minimum level of messages logged to the console logging log_level_for_file: save_dir: Returns: the logger Examples: >>> logger_dir = \"logs\" >>> logger_name = \"check_log\" >>> logger = init_logger(logger_name, save_dir=logger_dir) This will create two logs: * one printed to console where we run the code (the `StreamHandler`), * and one that will be saved to file `save_dir/logger_name.txt` (the `FileHandler`). `'logger.propagate = False'` makes sure that the logs sent to file will not be printed to console. We use the `Formatter` class to define the format of the logs. Here: * The time of the log in a human-readable format, `asctime` * `levelname` is the level of the log, one out of `INFO, DEBUG, WARNING, ERROR, CRITICAL`. * The name of the file, `filename`, from which the log was generated, and the line number, `lineno`. * Lastly, the message itself \u2014 `message`. The default has only `INFO` logs and above (i.e., also `WARNING, ERROR` and `CRITICAL`) displayed in the console; the file will also include `DEBUG` logs. \"\"\" logger = logging . getLogger () logger . setLevel ( level = logging . DEBUG ) logger . propagate = False formatter = logging . Formatter ( \" %(asctime)s [ %(levelname)s ] %(filename)s %(lineno)d - %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ch = logging . StreamHandler () ch . setLevel ( log_level_for_console . upper ()) ch . setFormatter ( formatter ) logger . addHandler ( ch ) if save_dir is not None : Path ( save_dir ) . mkdir ( exist_ok = True , parents = True ) fh = logging . FileHandler ( save_dir + f \"/ { logger_name } .txt\" ) fh . setLevel ( log_level_for_file . upper ()) fh . setFormatter ( formatter ) logger . addHandler ( fh ) return logger","title":"init_logger()"},{"location":"bs_logging.html#bs_python_utils.bs_logging.log_execution","text":"Decorator to log the execution of a function. Only records entry to and exit from the function, to the console. Source code in bs_python_utils/bs_logging.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def log_execution ( func : Callable ) -> Callable : \"\"\"Decorator to log the execution of a function. Only records entry to and exit from the function, to the console. \"\"\" loglevel = logging . info @functools . wraps ( func ) def wrapper ( * args , ** kwargs ): loglevel ( f \"Executing { func . __name__ } \" ) result = func ( * args , ** kwargs ) loglevel ( f \"Finished executing { func . __name__ } \" ) return result return wrapper","title":"log_execution()"},{"location":"bs_mathstr.html","text":"bs_mathstr module \u00b6 Some useful strings for math formulae. Note if the math looks strange in the documentation, just reload the page. Attributes: Name Type Description * str_beta0 the LaTeX string \\(\\beta_0\\) * str_beta1 the LaTeX string \\(\\beta_1\\) * str_pi the LaTeX string \\(\\pi\\) * str_sigma the LaTeX string \\(\\sigma\\) * str_sigma2 the LaTeX string \\(\\sigma^2\\) * uni_beta0 the Unicode string \\(\\beta_0\\) * uni_beta1 the Unicode string \\(\\beta_1\\) * uni_pi the Unicode string \\(\\pi\\) * uni_sigma the Unicode string \\(\\sigma\\) * uni_sigma2 the Unicode string \\(\\sigma^2\\) * uni_s2 the Unicode string \\(s^2\\) * uni_R2 the Unicode string \\(R^2\\) * sub_sub_scripts a dictionary of unicodes for subscripts and superscripts; e.g \\(a^b\\) would be \"a\" + sub_sup_scripts['b'][0]","title":"math strings"},{"location":"bs_mathstr.html#bs_mathstr-module","text":"Some useful strings for math formulae. Note if the math looks strange in the documentation, just reload the page. Attributes: Name Type Description * str_beta0 the LaTeX string \\(\\beta_0\\) * str_beta1 the LaTeX string \\(\\beta_1\\) * str_pi the LaTeX string \\(\\pi\\) * str_sigma the LaTeX string \\(\\sigma\\) * str_sigma2 the LaTeX string \\(\\sigma^2\\) * uni_beta0 the Unicode string \\(\\beta_0\\) * uni_beta1 the Unicode string \\(\\beta_1\\) * uni_pi the Unicode string \\(\\pi\\) * uni_sigma the Unicode string \\(\\sigma\\) * uni_sigma2 the Unicode string \\(\\sigma^2\\) * uni_s2 the Unicode string \\(s^2\\) * uni_R2 the Unicode string \\(R^2\\) * sub_sub_scripts a dictionary of unicodes for subscripts and superscripts; e.g \\(a^b\\) would be \"a\" + sub_sup_scripts['b'][0]","title":"bs_mathstr module"},{"location":"bs_mem.html","text":"bs_mem module \u00b6 Reports on memory usage: mem_usage : prints the top n largest global items in memory memory_display_top : prints the top n largest memory allocations since tracing started memory_display_top_diffs : prints the top n largest memory allocations since the last snapshot. memory_display_top ( snapshot , key_type = 'lineno' , limit = 5 ) \u00b6 prints out the lines with the top limit allocations of memory since tracemalloc.start() Parameters: Name Type Description Default snapshot tracemalloc . Snapshot obtained from tracemalloc.take_snapshot() required key_type str 'lineno' gives file and line number; 'traceback' gives all 'lineno' limit int | None how many top allocations we want 5 Returns: Type Description None just prints. Examples: >>> tracemalloc . start () >>> .... execute ... >>> snapshot = tracemalloc . take_snapshot () >>> memory_display_top ( snapshot ) Source code in bs_python_utils/bs_mem.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def memory_display_top ( snapshot : tracemalloc . Snapshot , key_type : str = \"lineno\" , limit : int | None = 5 ) -> None : \"\"\" prints out the lines with the top `limit` allocations of memory since `tracemalloc.start()` Args: snapshot: obtained from tracemalloc.take_snapshot() key_type: 'lineno' gives file and line number; 'traceback' gives all limit: how many top allocations we want Returns: just prints. Examples: >>> tracemalloc.start() >>> .... execute ... >>> snapshot = tracemalloc.take_snapshot() >>> memory_display_top(snapshot) \"\"\" top_stats = snapshot . statistics ( key_type ) print_stars ( f \"Top { limit } memory allocations\" ) for index , stat in enumerate ( top_stats [: limit ], 1 ): frame = stat . traceback [ 0 ] print ( \"# %s : %s : %s : %.1f KiB\" % ( index , frame . filename , frame . lineno , stat . size / 1024 ) ) line = linecache . getline ( frame . filename , frame . lineno ) . strip () if line : print ( \" %s \" % line ) other = top_stats [ limit :] if other : size = sum ( stat . size for stat in other ) print ( f \" { len ( other ) } other: { size / 1024 : .1f } KiB\" ) total = sum ( stat . size for stat in top_stats ) print ( \"Total allocated size: %.1f KiB\" % ( total / 1024 )) memory_display_top_diffs ( snapshot1 , snapshot2 , key_type = 'lineno' , limit = 5 ) \u00b6 prints out the lines with the top limit allocations between the two snapshots Parameters: Name Type Description Default snapshot1 tracemalloc . Snapshot previous snapshot required snapshot2 tracemalloc . Snapshot new snapshot required key_type str 'lineno' gives file and line number; 'traceback' gives all 'lineno' limit int how many top allocations we want 5 Returns: Type Description None just prints. Examples: >>> tracemalloc . start () >>> .... execute ... >>> snapshot1 = tracemalloc . take_snapshot () >>> .... execute ... >>> snapshot2 = tracemalloc . take_snapshot () >>> memory_display_top_diffs ( snapshot1 , snapshot2 ) Source code in bs_python_utils/bs_mem.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def memory_display_top_diffs ( snapshot1 : tracemalloc . Snapshot , snapshot2 : tracemalloc . Snapshot , key_type : str = \"lineno\" , limit : int = 5 , ) -> None : \"\"\" prints out the lines with the top `limit` allocations between the two snapshots Args: snapshot1: previous snapshot snapshot2: new snapshot key_type: 'lineno' gives file and line number; 'traceback' gives all limit: how many top allocations we want Returns: just prints. Examples: >>> tracemalloc.start() >>> .... execute ... >>> snapshot1 = tracemalloc.take_snapshot() >>> .... execute ... >>> snapshot2 = tracemalloc.take_snapshot() >>> memory_display_top_diffs(snapshot1, snapshot2) \"\"\" top_stats = snapshot2 . compare_to ( snapshot1 , key_type ) print_stars ( f \"Top { limit } new memory allocations\" ) for index , stat in enumerate ( top_stats [: limit ], 1 ): frame = stat . traceback [ 0 ] print ( \"# %s : %s : %s : %.1f KiB\" % ( index , frame . filename , frame . lineno , stat . size / 1024 ) ) line = linecache . getline ( frame . filename , frame . lineno ) . strip () if line : print ( \" %s \" % line ) other = top_stats [ limit :] if other : size = sum ( stat . size for stat in other ) print ( f \" { len ( other ) } other: { size / 1024 : .1f } KiB\" ) total = sum ( stat . size for stat in top_stats ) print ( \"Total allocated size: %.1f KiB\" % ( total / 1024 )) memory_usage ( n = 10 ) \u00b6 prints the top n largest global items in memory Parameters: Name Type Description Default n int | None we report the size of the largest n global items 10 Returns: Type Description None nothing. Source code in bs_python_utils/bs_mem.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def memory_usage ( n : int | None = 10 ) -> None : \"\"\" prints the top `n` largest global items in memory Args: n: we report the size of the largest `n` global items Returns: nothing. \"\"\" memory_usage_by_variable = pd . DataFrame ( { k : sys . getsizeof ( v ) for ( k , v ) in globals () . items ()}, index = [ \"Size\" ] ) memory_usage_by_variable = memory_usage_by_variable . T total_usage = _obj_size_fmt ( memory_usage_by_variable [ \"Size\" ] . sum ()) memory_usage_by_variable = memory_usage_by_variable . sort_values ( by = \"Size\" , ascending = False ) . head ( n ) memory_usage_by_variable [ \"Size\" ] = memory_usage_by_variable [ \"Size\" ] . apply ( lambda x : _obj_size_fmt ( x ) ) print_stars ( f \"Currently used memory = { total_usage } \\n\\t\\t\\t\\t Top { n } global objects:\" ) print ( memory_usage_by_variable ) return","title":"memory usage"},{"location":"bs_mem.html#bs_mem-module","text":"Reports on memory usage: mem_usage : prints the top n largest global items in memory memory_display_top : prints the top n largest memory allocations since tracing started memory_display_top_diffs : prints the top n largest memory allocations since the last snapshot.","title":"bs_mem module"},{"location":"bs_mem.html#bs_python_utils.bs_mem.memory_display_top","text":"prints out the lines with the top limit allocations of memory since tracemalloc.start() Parameters: Name Type Description Default snapshot tracemalloc . Snapshot obtained from tracemalloc.take_snapshot() required key_type str 'lineno' gives file and line number; 'traceback' gives all 'lineno' limit int | None how many top allocations we want 5 Returns: Type Description None just prints. Examples: >>> tracemalloc . start () >>> .... execute ... >>> snapshot = tracemalloc . take_snapshot () >>> memory_display_top ( snapshot ) Source code in bs_python_utils/bs_mem.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def memory_display_top ( snapshot : tracemalloc . Snapshot , key_type : str = \"lineno\" , limit : int | None = 5 ) -> None : \"\"\" prints out the lines with the top `limit` allocations of memory since `tracemalloc.start()` Args: snapshot: obtained from tracemalloc.take_snapshot() key_type: 'lineno' gives file and line number; 'traceback' gives all limit: how many top allocations we want Returns: just prints. Examples: >>> tracemalloc.start() >>> .... execute ... >>> snapshot = tracemalloc.take_snapshot() >>> memory_display_top(snapshot) \"\"\" top_stats = snapshot . statistics ( key_type ) print_stars ( f \"Top { limit } memory allocations\" ) for index , stat in enumerate ( top_stats [: limit ], 1 ): frame = stat . traceback [ 0 ] print ( \"# %s : %s : %s : %.1f KiB\" % ( index , frame . filename , frame . lineno , stat . size / 1024 ) ) line = linecache . getline ( frame . filename , frame . lineno ) . strip () if line : print ( \" %s \" % line ) other = top_stats [ limit :] if other : size = sum ( stat . size for stat in other ) print ( f \" { len ( other ) } other: { size / 1024 : .1f } KiB\" ) total = sum ( stat . size for stat in top_stats ) print ( \"Total allocated size: %.1f KiB\" % ( total / 1024 ))","title":"memory_display_top()"},{"location":"bs_mem.html#bs_python_utils.bs_mem.memory_display_top_diffs","text":"prints out the lines with the top limit allocations between the two snapshots Parameters: Name Type Description Default snapshot1 tracemalloc . Snapshot previous snapshot required snapshot2 tracemalloc . Snapshot new snapshot required key_type str 'lineno' gives file and line number; 'traceback' gives all 'lineno' limit int how many top allocations we want 5 Returns: Type Description None just prints. Examples: >>> tracemalloc . start () >>> .... execute ... >>> snapshot1 = tracemalloc . take_snapshot () >>> .... execute ... >>> snapshot2 = tracemalloc . take_snapshot () >>> memory_display_top_diffs ( snapshot1 , snapshot2 ) Source code in bs_python_utils/bs_mem.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def memory_display_top_diffs ( snapshot1 : tracemalloc . Snapshot , snapshot2 : tracemalloc . Snapshot , key_type : str = \"lineno\" , limit : int = 5 , ) -> None : \"\"\" prints out the lines with the top `limit` allocations between the two snapshots Args: snapshot1: previous snapshot snapshot2: new snapshot key_type: 'lineno' gives file and line number; 'traceback' gives all limit: how many top allocations we want Returns: just prints. Examples: >>> tracemalloc.start() >>> .... execute ... >>> snapshot1 = tracemalloc.take_snapshot() >>> .... execute ... >>> snapshot2 = tracemalloc.take_snapshot() >>> memory_display_top_diffs(snapshot1, snapshot2) \"\"\" top_stats = snapshot2 . compare_to ( snapshot1 , key_type ) print_stars ( f \"Top { limit } new memory allocations\" ) for index , stat in enumerate ( top_stats [: limit ], 1 ): frame = stat . traceback [ 0 ] print ( \"# %s : %s : %s : %.1f KiB\" % ( index , frame . filename , frame . lineno , stat . size / 1024 ) ) line = linecache . getline ( frame . filename , frame . lineno ) . strip () if line : print ( \" %s \" % line ) other = top_stats [ limit :] if other : size = sum ( stat . size for stat in other ) print ( f \" { len ( other ) } other: { size / 1024 : .1f } KiB\" ) total = sum ( stat . size for stat in top_stats ) print ( \"Total allocated size: %.1f KiB\" % ( total / 1024 ))","title":"memory_display_top_diffs()"},{"location":"bs_mem.html#bs_python_utils.bs_mem.memory_usage","text":"prints the top n largest global items in memory Parameters: Name Type Description Default n int | None we report the size of the largest n global items 10 Returns: Type Description None nothing. Source code in bs_python_utils/bs_mem.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def memory_usage ( n : int | None = 10 ) -> None : \"\"\" prints the top `n` largest global items in memory Args: n: we report the size of the largest `n` global items Returns: nothing. \"\"\" memory_usage_by_variable = pd . DataFrame ( { k : sys . getsizeof ( v ) for ( k , v ) in globals () . items ()}, index = [ \"Size\" ] ) memory_usage_by_variable = memory_usage_by_variable . T total_usage = _obj_size_fmt ( memory_usage_by_variable [ \"Size\" ] . sum ()) memory_usage_by_variable = memory_usage_by_variable . sort_values ( by = \"Size\" , ascending = False ) . head ( n ) memory_usage_by_variable [ \"Size\" ] = memory_usage_by_variable [ \"Size\" ] . apply ( lambda x : _obj_size_fmt ( x ) ) print_stars ( f \"Currently used memory = { total_usage } \\n\\t\\t\\t\\t Top { n } global objects:\" ) print ( memory_usage_by_variable ) return","title":"memory_usage()"},{"location":"bs_opt.html","text":"bs_opt module \u00b6 Interface to scipy.optimize : ScalarFunctionAndGradient , ProximalFunction type aliases an OptimizeParams class check_gradient_scalar_function checks whether an analytical gradient is correct acc_grad_descent : accelerated gradient descent for convex, possibly non-smooth functions minimize_some_fixed : minimizes a function with some parameter values possibly fixed and some possibly within bounds, using L-BFGS-B minimize_free : minimizes a function with some parameter values possibly within bounds dfp_update, bfgs_update : compute updates to the inverese Hessian armijo_alpha, barzilai_borwein_alpha : two ways of computing the step length print_optimization_results , print_constrained_optimization_results format the results. ProximalFunction = Callable [[ np . ndarray , float , Iterable ], np . ndarray ] module-attribute \u00b6 Type of h(x, t, pars) that returns a scalar value. ScalarFunctionAndGradient = Callable [[ np . ndarray , Iterable , Optional [ bool ]], Union [ float , tuple [ float , np . ndarray ]]] module-attribute \u00b6 Type of f(v, args, gr) that returns a scalar value and also a gradient if gr is True . OptimizeParams dataclass \u00b6 used for optimization; combines values, bounds and initial values for a parameter vector Source code in bs_python_utils/bs_opt.py 36 37 38 39 40 41 42 43 @dataclass class OptimizeParams : \"\"\"used for optimization; combines values, bounds and initial values for a parameter vector \"\"\" params_values : np . ndarray | None params_bounds : list [ tuple ] | None params_init : np . ndarray | None acc_grad_descent ( grad_f , x_init , other_params , prox_h = None , print_result = False , verbose = False , tol = 1e-09 , alpha = 1.01 , beta = 0.5 , maxiter = 10000 ) \u00b6 Minimizes (f+h) by Accelerated Gradient Descent where f is smooth and convex and h is convex. By default h is zero. The convergence criterion is that the largest component of the absolute value of the gradient must be smaller than tol . Parameters: Name Type Description Default grad_f Callable grad_f of f ; should return an (n) array from an (n) array and the other_ params object required x_init np . ndarray initial guess, shape (n) required prox_h ProximalFunction | None proximal projector of h , if any None other_params Iterable an iterable with additional parameters required verbose bool if True , print remaining gradient error every 10 iterations False tol float convergence criterion on grad_f 1e-09 alpha float ceiling on step multiplier 1.01 beta float floor on step multiplier 0.5 maxiter int max number of iterations 10000 Returns: Type Description tuple [ np . ndarray , int ] the candidate solution, and a convergence code (0 if successful, 1 if not). Source code in bs_python_utils/bs_opt.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 @timeit def acc_grad_descent ( grad_f : Callable , x_init : np . ndarray , other_params : Iterable , prox_h : ProximalFunction | None = None , print_result : bool = False , verbose : bool = False , tol : float = 1e-9 , alpha : float = 1.01 , beta : float = 0.5 , maxiter : int = 10000 , ) -> tuple [ np . ndarray , int ]: \"\"\" Minimizes `(f+h)` by Accelerated Gradient Descent where `f` is smooth and convex and `h` is convex. By default `h` is zero. The convergence criterion is that the largest component of the absolute value of the gradient must be smaller than `tol`. Args: grad_f: grad_f of `f`; should return an `(n)` array from an `(n)` array and the `other_ params` object x_init: initial guess, shape `(n)` prox_h: proximal projector of `h`, if any other_params: an iterable with additional parameters verbose: if `True`, print remaining gradient error every 10 iterations tol: convergence criterion on grad_f alpha: ceiling on step multiplier beta: floor on step multiplier maxiter: max number of iterations Returns: the candidate solution, and a convergence code (0 if successful, 1 if not). \"\"\" # no proximal projection if no h local_prox_h : ProximalFunction = prox_h if prox_h else lambda x , t , p : x x = x_init . copy () y = x_init . copy () # for stepsize we use Barzilai-Borwein t , g = barzilai_borwein_alpha ( grad_f , y , other_params ) grad_err_init = npmaxabs ( g ) if verbose : print ( f \"agd: grad_err_init= { grad_err_init } \" ) n_iter = 0 theta = 1.0 while n_iter < maxiter : grad_err = npmaxabs ( g ) if grad_err < tol : break xi = x yi = y x = y - t * g x = local_prox_h ( x , t , other_params ) theta = 2.0 / ( 1.0 + sqrt ( 1.0 + 4.0 / theta / theta )) if np . dot ( y - x , x - xi ) > 0 : # wrong direction, we restart x = xi y = x theta = 1.0 else : y = x + ( 1.0 - theta ) * ( x - xi ) gi = g g = grad_f ( y , other_params ) ndy = spla . norm ( y - yi ) t_hat = 0.5 * ndy * ndy / abs ( np . dot ( y - yi , gi - g )) t = min ( alpha * t , max ( beta * t , t_hat )) n_iter += 1 if verbose and n_iter % 10 == 0 : print ( f \" AGD with grad_err = { grad_err } after { n_iter } iterations\" ) x_conv = y ret_code = 0 if grad_err < tol else 1 if verbose or print_result : if ret_code == 0 : print_stars ( f \" AGD converged with grad_err = { grad_err } after { n_iter } iterations\" ) else : print_stars ( f \" Problem in AGD: grad_err = { grad_err } after { n_iter } iterations\" ) return x_conv , ret_code armijo_alpha ( f , x , d , args , alpha_init = 1.0 , beta = 0.5 , max_iter = 100 , tol = 0.0 ) \u00b6 Given a function f we are minimizing, computes the step size alpha to take in the direction d using the Armijo rule. Parameters: Name Type Description Default f Callable the function required x np . ndarray the current point required d np . ndarray the direction we are taking required args Iterable other arguments passed to f required alpha_init float the initial step size 1.0 beta float the step size reduction factor 0.5 max_iter int the maximum number of iterations 100 tol float a tolerance 0.0 Returns: Type Description float the step size alpha . Source code in bs_python_utils/bs_opt.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def armijo_alpha ( f : Callable , x : np . ndarray , d : np . ndarray , args : Iterable , alpha_init : float = 1.0 , beta : float = 0.5 , max_iter : int = 100 , tol : float = 0.0 , ) -> float : \"\"\"Given a function `f` we are minimizing, computes the step size `alpha` to take in the direction `d` using the Armijo rule. Args: f: the function x: the current point d: the direction we are taking args: other arguments passed to `f` alpha_init: the initial step size beta: the step size reduction factor max_iter: the maximum number of iterations tol: a tolerance Returns: the step size `alpha`. \"\"\" f0 = f ( x , args ) alpha = alpha_init for _ in range ( max_iter ): x1 = x + alpha * d f1 = f ( x1 , args ) if f1 < f0 + tol : return alpha alpha *= beta else : bs_error_abort ( \"Too many iterations\" ) return alpha barzilai_borwein_alpha ( grad_f , x , args ) \u00b6 Given a function f we are minimizing, computes the step size alpha to take in the opposite direction of the gradient using the Barzilai-Borwein rule. Parameters: Name Type Description Default grad_f Callable the gradient of the function required x np . ndarray the current point required args Iterable other arguments passed to f required Returns: Type Description tuple [ float , np . ndarray ] the step size alpha and the gradient g at the point x . Source code in bs_python_utils/bs_opt.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def barzilai_borwein_alpha ( grad_f : Callable , x : np . ndarray , args : Iterable ) -> tuple [ float , np . ndarray ]: \"\"\"Given a function `f` we are minimizing, computes the step size `alpha` to take in the opposite direction of the gradient using the Barzilai-Borwein rule. Args: grad_f: the gradient of the function x: the current point args: other arguments passed to `f` Returns: the step size `alpha` and the gradient `g` at the point `x`. \"\"\" g = grad_f ( x , args ) alpha = 1.0 / spla . norm ( g ) x_hat = x - alpha * g g_hat = grad_f ( x_hat , args ) norm_dg = spla . norm ( g - g_hat ) norm_dg2 = norm_dg * norm_dg alpha = np . abs ( np . dot ( x - x_hat , g - g_hat )) / norm_dg2 return alpha , g bfgs_update ( hess_inv , gradient_diff , x_diff ) \u00b6 Runs a BFGS update for the inverse Hessian. Parameters: Name Type Description Default hess_inv np . ndarray the current inverse Hessian required gradient_diff np . ndarray the update in the gradient required x_diff np . ndarray the update in x required Returns: Type Description np . ndarray the updated inverse Hessian. Source code in bs_python_utils/bs_opt.py 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 def bfgs_update ( hess_inv : np . ndarray , gradient_diff : np . ndarray , x_diff : np . ndarray ) -> np . ndarray : \"\"\"Runs a BFGS update for the inverse Hessian. Args: hess_inv: the current inverse Hessian gradient_diff: the update in the gradient x_diff: the update in x Returns: the updated inverse Hessian. \"\"\" xdt = x_diff . T xpg = xdt @ gradient_diff hdg = hess_inv @ gradient_diff dgp_hdg = gradient_diff . T @ hdg u = x_diff / xpg - hdg / dgp_hdg hess_inv_new = dfp_update ( hess_inv , gradient_diff , x_diff ) + dgp_hdg * ( u @ u . T ) return cast ( np . ndarray , hess_inv_new ) check_gradient_scalar_function ( fg , p , args , mode = 'central' , EPS = 1e-06 ) \u00b6 Checks the gradient of a scalar function. Parameters: Name Type Description Default fg ScalarFunctionAndGradient should return the scalar value, and the gradient if its gr argument is True required p np . ndarray where we are checking the gradient required args Iterable other arguments passed to fg required mode str \"central\" or \"forward\" derivatives 'central' EPS float the step for forward or central derivatives 1e-06 Returns: Type Description TwoArrays the analytic and numeric gradients. Source code in bs_python_utils/bs_opt.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def check_gradient_scalar_function ( fg : ScalarFunctionAndGradient , p : np . ndarray , args : Iterable , mode : str = \"central\" , EPS : float = 1e-6 , ) -> TwoArrays : \"\"\"Checks the gradient of a scalar function. Args: fg: should return the scalar value, and the gradient if its `gr` argument is `True` p: where we are checking the gradient args: other arguments passed to `fg` mode: \"central\" or \"forward\" derivatives EPS: the step for forward or central derivatives Returns: the analytic and numeric gradients. \"\"\" f0 , f_grad = fg ( p , args , gr = True ) # type: ignore f0 = cast ( float , f0 ) print_stars ( \"checking the gradient: analytic, numeric\" ) g = np . zeros_like ( p ) if mode == \"central\" : for i , x in enumerate ( p ): p1 = p . copy () p1 [ i ] = x + EPS f_plus = cast ( float , fg ( p1 , args , gr = False )) # type: ignore p1 [ i ] -= 2.0 * EPS f_minus = cast ( float , fg ( p1 , args , gr = False )) # type: ignore g [ i ] = ( f_plus - f_minus ) / ( 2.0 * EPS ) print ( f \" { i } : { f_grad [ i ] } , { g [ i ] } \" ) elif mode == \"forward\" : for i , x in enumerate ( p ): p1 = p . copy () p1 [ i ] = x + EPS f_plus = cast ( float , fg ( p1 , args , gr = False )) # type: ignore g [ i ] = ( f_plus - f0 ) / EPS print ( f \" { i } : { f_grad [ i ] } , { g [ i ] } \" ) else : bs_error_abort ( \"mode must be 'central' or 'forward'\" ) return f_grad , g dfp_update ( hess_inv , gradient_diff , x_diff ) \u00b6 Runs a DFP update for the inverse Hessian. Parameters: Name Type Description Default hess_inv np . ndarray the current inverse Hessian required gradient_diff np . ndarray the update in the gradient required x_diff np . ndarray the update in x required Returns: Type Description np . ndarray the updated inverse Hessian. Source code in bs_python_utils/bs_opt.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 def dfp_update ( hess_inv : np . ndarray , gradient_diff : np . ndarray , x_diff : np . ndarray ) -> np . ndarray : \"\"\"Runs a DFP update for the inverse Hessian. Args: hess_inv: the current inverse Hessian gradient_diff: the update in the gradient x_diff: the update in x Returns: the updated inverse Hessian. \"\"\" xdt = x_diff . T xxp = x_diff @ xdt xpg = xdt @ gradient_diff hdg = hess_inv @ gradient_diff dgp_hdg = gradient_diff . T @ hdg hess_inv_new = hess_inv + xxp / xpg - ( hdg @ hdg . T ) / dgp_hdg return cast ( np . ndarray , hess_inv_new ) minimize_free ( obj , grad_obj , x_init , args , options = None , bounds = None ) \u00b6 Minimize a function on all of its variables, using BFGS or L-BFGS-B. Parameters: Name Type Description Default obj Callable the original function required grad_obj Callable its gradient function required x_init np . ndarray the initial values of all variables required args Iterable other parameters required options dict | None any options passed on to scipy.optimize.minimize None bounds list [ tuple [ float , float ]] | None the bounds on all variables, if any None Returns: Type Description Any the result of optimization, on all variables. Source code in bs_python_utils/bs_opt.py 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 @timeit def minimize_free ( obj : Callable , grad_obj : Callable , x_init : np . ndarray , args : Iterable , options : dict | None = None , bounds : list [ tuple [ float , float ]] | None = None , ) -> Any : \"\"\" Minimize a function on all of its variables, using BFGS or L-BFGS-B. Args: obj: the original function grad_obj: its gradient function x_init: the initial values of all variables args: other parameters options: any options passed on to `scipy.optimize.minimize` bounds: the bounds on all variables, if any Returns: the result of optimization, on all variables. \"\"\" if bounds is None : resopt = spopt . minimize ( obj , x_init , method = \"BFGS\" , args = args , options = options , jac = grad_obj , ) else : resopt = spopt . minimize ( obj , x_init , method = \"L-BFGS-B\" , args = args , options = options , jac = grad_obj , bounds = bounds , ) return resopt minimize_some_fixed ( obj , grad_obj , x_init , args , fixed_vars , fixed_vals , options = None , bounds = None ) \u00b6 Minimize a function with some variables fixed, using L-BFGS-B. Parameters: Name Type Description Default obj Callable the original function required grad_obj Callable its gradient function required fixed_vars list [ int ] | None a list if the indices of variables whose values are fixed required fixed_vals np . ndarray | None their fixed values required x_init np . ndarray the initial values of all variables (those on fixed variables are not used) required args Iterable other parameters required options dict | None any options passed on to scipy.optimize.minimize None bounds list [ tuple [ float , float ]] | None the bounds on all variables (those on fixed variables are not used) None Returns: Type Description Any the result of optimization, on all variables. Source code in bs_python_utils/bs_opt.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 @timeit def minimize_some_fixed ( obj : Callable , grad_obj : Callable , x_init : np . ndarray , args : Iterable , fixed_vars : list [ int ] | None , fixed_vals : np . ndarray | None , options : dict | None = None , bounds : list [ tuple [ float , float ]] | None = None , ) -> Any : \"\"\" Minimize a function with some variables fixed, using L-BFGS-B. Args: obj: the original function grad_obj: its gradient function fixed_vars: a list if the indices of variables whose values are fixed fixed_vals: their fixed values x_init: the initial values of all variables (those on fixed variables are not used) args: other parameters options: any options passed on to `scipy.optimize.minimize` bounds: the bounds on all variables (those on fixed variables are not used) Returns: the result of optimization, on all variables. \"\"\" if fixed_vars is None : resopt = spopt . minimize ( obj , x_init , method = \"L-BFGS-B\" , args = args , options = options , jac = grad_obj , bounds = bounds , ) else : fixed_vars = cast ( list , fixed_vars ) n_fixed = check_vector ( fixed_vals ) fixed_vals = cast ( np . ndarray , fixed_vals ) if len ( fixed_vars ) != n_fixed : bs_error_abort ( f \"fixed_vars has { len ( fixed_vars ) } indices but fixed_vals has\" f \" { fixed_vals . size } elements.\" ) fixed_obj , fixed_grad_obj = _fix_some ( obj , grad_obj , fixed_vars , fixed_vals ) # drop fixed variables and the corresponding bounds n = len ( x_init ) not_fixed = np . ones ( n , dtype = bool ) not_fixed [ fixed_vars ] = False t_init = x_init [ not_fixed ] t_bounds = ( None if bounds is None else [ bounds [ i ] for i in range ( n ) if not_fixed [ i ]] ) resopt = spopt . minimize ( fixed_obj , t_init , method = \"L-BFGS-B\" , args = args , options = options , jac = fixed_grad_obj , bounds = t_bounds , ) # now re-fill the values of the variables t = resopt . x t_full = list ( t ) for i , i_coef in enumerate ( fixed_vars ): t_full . insert ( i_coef , fixed_vals [ i ]) resopt . x = t_full # and re-fill the values of the gradients g = grad_obj ( np . array ( t_full ), args ) resopt . jac = g return resopt print_constrained_optimization_results ( resus , title = 'Minimizing' , print_constr = False , print_multipliers = False ) \u00b6 print results from constrained optimization. Parameters: Name Type Description Default resus spopt . OptimizeResult results from optimization required title str a title 'Minimizing' print_constr bool if True , print the values of the constraints False print_multipliers bool if True , print the values of the multipliers False Returns: Type Description None just prints. Source code in bs_python_utils/bs_opt.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def print_constrained_optimization_results ( resus : spopt . OptimizeResult , title : str = \"Minimizing\" , print_constr : bool = False , print_multipliers : bool = False , ) -> None : \"\"\"print results from constrained optimization. Args: resus: results from optimization title: a title print_constr: if `True`, print the values of the constraints print_multipliers: if `True`, print the values of the multipliers Returns: just prints. \"\"\" print_stars ( title ) print ( resus . message ) if resus . success : print ( f \"Successful! in { resus . nit } iterations\" ) print ( f \" evaluated { resus . nfev } functions and { resus . njev } gradients\" ) print ( f \"Minimized value is { resus . fun } \" ) print ( f \"The Lagrangian norm is { resus . optimality } \" ) print ( f \"The largest constraint violation is { resus . constr_violation } \" ) if print_multipliers : print ( f \"The multipliers are { resus . v } \" ) if print_constr : print ( f \"The values of the constraints are { resus . constr } \" ) else : print_stars ( \"Constrained minimization failed!\" ) return print_optimization_results ( resus , title = 'Minimizing' ) \u00b6 print results from unconstrained optimization. Parameters: Name Type Description Default resus spopt . OptimizeResult results from optimization required title str a title 'Minimizing' Returns: Type Description None just prints. Source code in bs_python_utils/bs_opt.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def print_optimization_results ( resus : spopt . OptimizeResult , title : str = \"Minimizing\" ) -> None : \"\"\"print results from unconstrained optimization. Args: resus: results from optimization title: a title Returns: just prints. \"\"\" print_stars ( title ) print ( resus . message ) if resus . success : print ( f \"Successful! in { resus . nit } iterations\" ) print ( f \" evaluated { resus . nfev } functions functions and { resus . njev } gradients\" ) print ( \" \\n Minimizer and grad_f:\" ) print ( np . column_stack (( resus . x , resus . jac ))) print ( f \"Minimized value is { resus . fun } \" ) else : print_stars ( \"Minimization failed!\" ) return","title":"optimization"},{"location":"bs_opt.html#bs_opt-module","text":"Interface to scipy.optimize : ScalarFunctionAndGradient , ProximalFunction type aliases an OptimizeParams class check_gradient_scalar_function checks whether an analytical gradient is correct acc_grad_descent : accelerated gradient descent for convex, possibly non-smooth functions minimize_some_fixed : minimizes a function with some parameter values possibly fixed and some possibly within bounds, using L-BFGS-B minimize_free : minimizes a function with some parameter values possibly within bounds dfp_update, bfgs_update : compute updates to the inverese Hessian armijo_alpha, barzilai_borwein_alpha : two ways of computing the step length print_optimization_results , print_constrained_optimization_results format the results.","title":"bs_opt module"},{"location":"bs_opt.html#bs_python_utils.bs_opt.ProximalFunction","text":"Type of h(x, t, pars) that returns a scalar value.","title":"ProximalFunction"},{"location":"bs_opt.html#bs_python_utils.bs_opt.ScalarFunctionAndGradient","text":"Type of f(v, args, gr) that returns a scalar value and also a gradient if gr is True .","title":"ScalarFunctionAndGradient"},{"location":"bs_opt.html#bs_python_utils.bs_opt.OptimizeParams","text":"used for optimization; combines values, bounds and initial values for a parameter vector Source code in bs_python_utils/bs_opt.py 36 37 38 39 40 41 42 43 @dataclass class OptimizeParams : \"\"\"used for optimization; combines values, bounds and initial values for a parameter vector \"\"\" params_values : np . ndarray | None params_bounds : list [ tuple ] | None params_init : np . ndarray | None","title":"OptimizeParams"},{"location":"bs_opt.html#bs_python_utils.bs_opt.acc_grad_descent","text":"Minimizes (f+h) by Accelerated Gradient Descent where f is smooth and convex and h is convex. By default h is zero. The convergence criterion is that the largest component of the absolute value of the gradient must be smaller than tol . Parameters: Name Type Description Default grad_f Callable grad_f of f ; should return an (n) array from an (n) array and the other_ params object required x_init np . ndarray initial guess, shape (n) required prox_h ProximalFunction | None proximal projector of h , if any None other_params Iterable an iterable with additional parameters required verbose bool if True , print remaining gradient error every 10 iterations False tol float convergence criterion on grad_f 1e-09 alpha float ceiling on step multiplier 1.01 beta float floor on step multiplier 0.5 maxiter int max number of iterations 10000 Returns: Type Description tuple [ np . ndarray , int ] the candidate solution, and a convergence code (0 if successful, 1 if not). Source code in bs_python_utils/bs_opt.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 @timeit def acc_grad_descent ( grad_f : Callable , x_init : np . ndarray , other_params : Iterable , prox_h : ProximalFunction | None = None , print_result : bool = False , verbose : bool = False , tol : float = 1e-9 , alpha : float = 1.01 , beta : float = 0.5 , maxiter : int = 10000 , ) -> tuple [ np . ndarray , int ]: \"\"\" Minimizes `(f+h)` by Accelerated Gradient Descent where `f` is smooth and convex and `h` is convex. By default `h` is zero. The convergence criterion is that the largest component of the absolute value of the gradient must be smaller than `tol`. Args: grad_f: grad_f of `f`; should return an `(n)` array from an `(n)` array and the `other_ params` object x_init: initial guess, shape `(n)` prox_h: proximal projector of `h`, if any other_params: an iterable with additional parameters verbose: if `True`, print remaining gradient error every 10 iterations tol: convergence criterion on grad_f alpha: ceiling on step multiplier beta: floor on step multiplier maxiter: max number of iterations Returns: the candidate solution, and a convergence code (0 if successful, 1 if not). \"\"\" # no proximal projection if no h local_prox_h : ProximalFunction = prox_h if prox_h else lambda x , t , p : x x = x_init . copy () y = x_init . copy () # for stepsize we use Barzilai-Borwein t , g = barzilai_borwein_alpha ( grad_f , y , other_params ) grad_err_init = npmaxabs ( g ) if verbose : print ( f \"agd: grad_err_init= { grad_err_init } \" ) n_iter = 0 theta = 1.0 while n_iter < maxiter : grad_err = npmaxabs ( g ) if grad_err < tol : break xi = x yi = y x = y - t * g x = local_prox_h ( x , t , other_params ) theta = 2.0 / ( 1.0 + sqrt ( 1.0 + 4.0 / theta / theta )) if np . dot ( y - x , x - xi ) > 0 : # wrong direction, we restart x = xi y = x theta = 1.0 else : y = x + ( 1.0 - theta ) * ( x - xi ) gi = g g = grad_f ( y , other_params ) ndy = spla . norm ( y - yi ) t_hat = 0.5 * ndy * ndy / abs ( np . dot ( y - yi , gi - g )) t = min ( alpha * t , max ( beta * t , t_hat )) n_iter += 1 if verbose and n_iter % 10 == 0 : print ( f \" AGD with grad_err = { grad_err } after { n_iter } iterations\" ) x_conv = y ret_code = 0 if grad_err < tol else 1 if verbose or print_result : if ret_code == 0 : print_stars ( f \" AGD converged with grad_err = { grad_err } after { n_iter } iterations\" ) else : print_stars ( f \" Problem in AGD: grad_err = { grad_err } after { n_iter } iterations\" ) return x_conv , ret_code","title":"acc_grad_descent()"},{"location":"bs_opt.html#bs_python_utils.bs_opt.armijo_alpha","text":"Given a function f we are minimizing, computes the step size alpha to take in the direction d using the Armijo rule. Parameters: Name Type Description Default f Callable the function required x np . ndarray the current point required d np . ndarray the direction we are taking required args Iterable other arguments passed to f required alpha_init float the initial step size 1.0 beta float the step size reduction factor 0.5 max_iter int the maximum number of iterations 100 tol float a tolerance 0.0 Returns: Type Description float the step size alpha . Source code in bs_python_utils/bs_opt.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def armijo_alpha ( f : Callable , x : np . ndarray , d : np . ndarray , args : Iterable , alpha_init : float = 1.0 , beta : float = 0.5 , max_iter : int = 100 , tol : float = 0.0 , ) -> float : \"\"\"Given a function `f` we are minimizing, computes the step size `alpha` to take in the direction `d` using the Armijo rule. Args: f: the function x: the current point d: the direction we are taking args: other arguments passed to `f` alpha_init: the initial step size beta: the step size reduction factor max_iter: the maximum number of iterations tol: a tolerance Returns: the step size `alpha`. \"\"\" f0 = f ( x , args ) alpha = alpha_init for _ in range ( max_iter ): x1 = x + alpha * d f1 = f ( x1 , args ) if f1 < f0 + tol : return alpha alpha *= beta else : bs_error_abort ( \"Too many iterations\" ) return alpha","title":"armijo_alpha()"},{"location":"bs_opt.html#bs_python_utils.bs_opt.barzilai_borwein_alpha","text":"Given a function f we are minimizing, computes the step size alpha to take in the opposite direction of the gradient using the Barzilai-Borwein rule. Parameters: Name Type Description Default grad_f Callable the gradient of the function required x np . ndarray the current point required args Iterable other arguments passed to f required Returns: Type Description tuple [ float , np . ndarray ] the step size alpha and the gradient g at the point x . Source code in bs_python_utils/bs_opt.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def barzilai_borwein_alpha ( grad_f : Callable , x : np . ndarray , args : Iterable ) -> tuple [ float , np . ndarray ]: \"\"\"Given a function `f` we are minimizing, computes the step size `alpha` to take in the opposite direction of the gradient using the Barzilai-Borwein rule. Args: grad_f: the gradient of the function x: the current point args: other arguments passed to `f` Returns: the step size `alpha` and the gradient `g` at the point `x`. \"\"\" g = grad_f ( x , args ) alpha = 1.0 / spla . norm ( g ) x_hat = x - alpha * g g_hat = grad_f ( x_hat , args ) norm_dg = spla . norm ( g - g_hat ) norm_dg2 = norm_dg * norm_dg alpha = np . abs ( np . dot ( x - x_hat , g - g_hat )) / norm_dg2 return alpha , g","title":"barzilai_borwein_alpha()"},{"location":"bs_opt.html#bs_python_utils.bs_opt.bfgs_update","text":"Runs a BFGS update for the inverse Hessian. Parameters: Name Type Description Default hess_inv np . ndarray the current inverse Hessian required gradient_diff np . ndarray the update in the gradient required x_diff np . ndarray the update in x required Returns: Type Description np . ndarray the updated inverse Hessian. Source code in bs_python_utils/bs_opt.py 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 def bfgs_update ( hess_inv : np . ndarray , gradient_diff : np . ndarray , x_diff : np . ndarray ) -> np . ndarray : \"\"\"Runs a BFGS update for the inverse Hessian. Args: hess_inv: the current inverse Hessian gradient_diff: the update in the gradient x_diff: the update in x Returns: the updated inverse Hessian. \"\"\" xdt = x_diff . T xpg = xdt @ gradient_diff hdg = hess_inv @ gradient_diff dgp_hdg = gradient_diff . T @ hdg u = x_diff / xpg - hdg / dgp_hdg hess_inv_new = dfp_update ( hess_inv , gradient_diff , x_diff ) + dgp_hdg * ( u @ u . T ) return cast ( np . ndarray , hess_inv_new )","title":"bfgs_update()"},{"location":"bs_opt.html#bs_python_utils.bs_opt.check_gradient_scalar_function","text":"Checks the gradient of a scalar function. Parameters: Name Type Description Default fg ScalarFunctionAndGradient should return the scalar value, and the gradient if its gr argument is True required p np . ndarray where we are checking the gradient required args Iterable other arguments passed to fg required mode str \"central\" or \"forward\" derivatives 'central' EPS float the step for forward or central derivatives 1e-06 Returns: Type Description TwoArrays the analytic and numeric gradients. Source code in bs_python_utils/bs_opt.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def check_gradient_scalar_function ( fg : ScalarFunctionAndGradient , p : np . ndarray , args : Iterable , mode : str = \"central\" , EPS : float = 1e-6 , ) -> TwoArrays : \"\"\"Checks the gradient of a scalar function. Args: fg: should return the scalar value, and the gradient if its `gr` argument is `True` p: where we are checking the gradient args: other arguments passed to `fg` mode: \"central\" or \"forward\" derivatives EPS: the step for forward or central derivatives Returns: the analytic and numeric gradients. \"\"\" f0 , f_grad = fg ( p , args , gr = True ) # type: ignore f0 = cast ( float , f0 ) print_stars ( \"checking the gradient: analytic, numeric\" ) g = np . zeros_like ( p ) if mode == \"central\" : for i , x in enumerate ( p ): p1 = p . copy () p1 [ i ] = x + EPS f_plus = cast ( float , fg ( p1 , args , gr = False )) # type: ignore p1 [ i ] -= 2.0 * EPS f_minus = cast ( float , fg ( p1 , args , gr = False )) # type: ignore g [ i ] = ( f_plus - f_minus ) / ( 2.0 * EPS ) print ( f \" { i } : { f_grad [ i ] } , { g [ i ] } \" ) elif mode == \"forward\" : for i , x in enumerate ( p ): p1 = p . copy () p1 [ i ] = x + EPS f_plus = cast ( float , fg ( p1 , args , gr = False )) # type: ignore g [ i ] = ( f_plus - f0 ) / EPS print ( f \" { i } : { f_grad [ i ] } , { g [ i ] } \" ) else : bs_error_abort ( \"mode must be 'central' or 'forward'\" ) return f_grad , g","title":"check_gradient_scalar_function()"},{"location":"bs_opt.html#bs_python_utils.bs_opt.dfp_update","text":"Runs a DFP update for the inverse Hessian. Parameters: Name Type Description Default hess_inv np . ndarray the current inverse Hessian required gradient_diff np . ndarray the update in the gradient required x_diff np . ndarray the update in x required Returns: Type Description np . ndarray the updated inverse Hessian. Source code in bs_python_utils/bs_opt.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 def dfp_update ( hess_inv : np . ndarray , gradient_diff : np . ndarray , x_diff : np . ndarray ) -> np . ndarray : \"\"\"Runs a DFP update for the inverse Hessian. Args: hess_inv: the current inverse Hessian gradient_diff: the update in the gradient x_diff: the update in x Returns: the updated inverse Hessian. \"\"\" xdt = x_diff . T xxp = x_diff @ xdt xpg = xdt @ gradient_diff hdg = hess_inv @ gradient_diff dgp_hdg = gradient_diff . T @ hdg hess_inv_new = hess_inv + xxp / xpg - ( hdg @ hdg . T ) / dgp_hdg return cast ( np . ndarray , hess_inv_new )","title":"dfp_update()"},{"location":"bs_opt.html#bs_python_utils.bs_opt.minimize_free","text":"Minimize a function on all of its variables, using BFGS or L-BFGS-B. Parameters: Name Type Description Default obj Callable the original function required grad_obj Callable its gradient function required x_init np . ndarray the initial values of all variables required args Iterable other parameters required options dict | None any options passed on to scipy.optimize.minimize None bounds list [ tuple [ float , float ]] | None the bounds on all variables, if any None Returns: Type Description Any the result of optimization, on all variables. Source code in bs_python_utils/bs_opt.py 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 @timeit def minimize_free ( obj : Callable , grad_obj : Callable , x_init : np . ndarray , args : Iterable , options : dict | None = None , bounds : list [ tuple [ float , float ]] | None = None , ) -> Any : \"\"\" Minimize a function on all of its variables, using BFGS or L-BFGS-B. Args: obj: the original function grad_obj: its gradient function x_init: the initial values of all variables args: other parameters options: any options passed on to `scipy.optimize.minimize` bounds: the bounds on all variables, if any Returns: the result of optimization, on all variables. \"\"\" if bounds is None : resopt = spopt . minimize ( obj , x_init , method = \"BFGS\" , args = args , options = options , jac = grad_obj , ) else : resopt = spopt . minimize ( obj , x_init , method = \"L-BFGS-B\" , args = args , options = options , jac = grad_obj , bounds = bounds , ) return resopt","title":"minimize_free()"},{"location":"bs_opt.html#bs_python_utils.bs_opt.minimize_some_fixed","text":"Minimize a function with some variables fixed, using L-BFGS-B. Parameters: Name Type Description Default obj Callable the original function required grad_obj Callable its gradient function required fixed_vars list [ int ] | None a list if the indices of variables whose values are fixed required fixed_vals np . ndarray | None their fixed values required x_init np . ndarray the initial values of all variables (those on fixed variables are not used) required args Iterable other parameters required options dict | None any options passed on to scipy.optimize.minimize None bounds list [ tuple [ float , float ]] | None the bounds on all variables (those on fixed variables are not used) None Returns: Type Description Any the result of optimization, on all variables. Source code in bs_python_utils/bs_opt.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 @timeit def minimize_some_fixed ( obj : Callable , grad_obj : Callable , x_init : np . ndarray , args : Iterable , fixed_vars : list [ int ] | None , fixed_vals : np . ndarray | None , options : dict | None = None , bounds : list [ tuple [ float , float ]] | None = None , ) -> Any : \"\"\" Minimize a function with some variables fixed, using L-BFGS-B. Args: obj: the original function grad_obj: its gradient function fixed_vars: a list if the indices of variables whose values are fixed fixed_vals: their fixed values x_init: the initial values of all variables (those on fixed variables are not used) args: other parameters options: any options passed on to `scipy.optimize.minimize` bounds: the bounds on all variables (those on fixed variables are not used) Returns: the result of optimization, on all variables. \"\"\" if fixed_vars is None : resopt = spopt . minimize ( obj , x_init , method = \"L-BFGS-B\" , args = args , options = options , jac = grad_obj , bounds = bounds , ) else : fixed_vars = cast ( list , fixed_vars ) n_fixed = check_vector ( fixed_vals ) fixed_vals = cast ( np . ndarray , fixed_vals ) if len ( fixed_vars ) != n_fixed : bs_error_abort ( f \"fixed_vars has { len ( fixed_vars ) } indices but fixed_vals has\" f \" { fixed_vals . size } elements.\" ) fixed_obj , fixed_grad_obj = _fix_some ( obj , grad_obj , fixed_vars , fixed_vals ) # drop fixed variables and the corresponding bounds n = len ( x_init ) not_fixed = np . ones ( n , dtype = bool ) not_fixed [ fixed_vars ] = False t_init = x_init [ not_fixed ] t_bounds = ( None if bounds is None else [ bounds [ i ] for i in range ( n ) if not_fixed [ i ]] ) resopt = spopt . minimize ( fixed_obj , t_init , method = \"L-BFGS-B\" , args = args , options = options , jac = fixed_grad_obj , bounds = t_bounds , ) # now re-fill the values of the variables t = resopt . x t_full = list ( t ) for i , i_coef in enumerate ( fixed_vars ): t_full . insert ( i_coef , fixed_vals [ i ]) resopt . x = t_full # and re-fill the values of the gradients g = grad_obj ( np . array ( t_full ), args ) resopt . jac = g return resopt","title":"minimize_some_fixed()"},{"location":"bs_opt.html#bs_python_utils.bs_opt.print_constrained_optimization_results","text":"print results from constrained optimization. Parameters: Name Type Description Default resus spopt . OptimizeResult results from optimization required title str a title 'Minimizing' print_constr bool if True , print the values of the constraints False print_multipliers bool if True , print the values of the multipliers False Returns: Type Description None just prints. Source code in bs_python_utils/bs_opt.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def print_constrained_optimization_results ( resus : spopt . OptimizeResult , title : str = \"Minimizing\" , print_constr : bool = False , print_multipliers : bool = False , ) -> None : \"\"\"print results from constrained optimization. Args: resus: results from optimization title: a title print_constr: if `True`, print the values of the constraints print_multipliers: if `True`, print the values of the multipliers Returns: just prints. \"\"\" print_stars ( title ) print ( resus . message ) if resus . success : print ( f \"Successful! in { resus . nit } iterations\" ) print ( f \" evaluated { resus . nfev } functions and { resus . njev } gradients\" ) print ( f \"Minimized value is { resus . fun } \" ) print ( f \"The Lagrangian norm is { resus . optimality } \" ) print ( f \"The largest constraint violation is { resus . constr_violation } \" ) if print_multipliers : print ( f \"The multipliers are { resus . v } \" ) if print_constr : print ( f \"The values of the constraints are { resus . constr } \" ) else : print_stars ( \"Constrained minimization failed!\" ) return","title":"print_constrained_optimization_results()"},{"location":"bs_opt.html#bs_python_utils.bs_opt.print_optimization_results","text":"print results from unconstrained optimization. Parameters: Name Type Description Default resus spopt . OptimizeResult results from optimization required title str a title 'Minimizing' Returns: Type Description None just prints. Source code in bs_python_utils/bs_opt.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def print_optimization_results ( resus : spopt . OptimizeResult , title : str = \"Minimizing\" ) -> None : \"\"\"print results from unconstrained optimization. Args: resus: results from optimization title: a title Returns: just prints. \"\"\" print_stars ( title ) print ( resus . message ) if resus . success : print ( f \"Successful! in { resus . nit } iterations\" ) print ( f \" evaluated { resus . nfev } functions functions and { resus . njev } gradients\" ) print ( \" \\n Minimizer and grad_f:\" ) print ( np . column_stack (( resus . x , resus . jac ))) print ( f \"Minimized value is { resus . fun } \" ) else : print_stars ( \"Minimization failed!\" ) return","title":"print_optimization_results()"},{"location":"bs_plots.html","text":"bs_plots module \u00b6 This collects the plotting functions in bsmplutils , bs_seaborn , and bs_altair .","title":"plotting interface"},{"location":"bs_plots.html#bs_plots-module","text":"This collects the plotting functions in bsmplutils , bs_seaborn , and bs_altair .","title":"bs_plots module"},{"location":"bs_seaborn.html","text":"bs_seaborn module \u00b6 Some Seaborn plotting utilities: bs_sns_get_legend : get the Legend object of a Seaborn plot bs_sns_bar_x_byf : make a bar plot of x by f bs_sns_bar_x_byfg : make a bar plot of x by f and g bs_sns_density_estimates : plots the densities of estimates of several coefficients with several methods, superposed by methods and faceted by coefficients. bs_sns_bar_x_byf ( df , xstr , fstr , statistic = np . mean , label_x = None , label_f = None , title = None ) \u00b6 Make a bar plot of x by f . Parameters: Name Type Description Default df pd . DataFrame dataframe, should contain columns xstr and fstr required xstr str column name of x required fstr str column name of f required statistic Callable statistic to plot (by default, the mean) np . mean label_x str | None label of x None label_f str | None label of f None title str | None title of plot None Returns: Type Description SeabornGraph the plot. Source code in bs_python_utils/bs_seaborn.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def bs_sns_bar_x_byf ( df : pd . DataFrame , xstr : str , fstr : str , statistic : Callable = np . mean , label_x : str | None = None , label_f : str | None = None , title : str | None = None , ) -> SeabornGraph : \"\"\"Make a bar plot of `x` by `f`. Args: df: dataframe, should contain columns `xstr` and `fstr` xstr: column name of x fstr: column name of f statistic: statistic to plot (by default, the mean) label_x: label of x label_f: label of f title: title of plot Returns: the plot. \"\"\" fig , ax = plt . subplots () gbar = sns . barplot ( x = fstr , y = xstr , data = df , estimator = statistic , errcolor = \"r\" , errwidth = 0.75 , capsize = 0.2 , ax = ax , ) xlab = fstr if label_f is None else label_f ylab = xstr if label_x is None else label_x ax . set_xlabel ( xlab ) ax . set_ylabel ( ylab ) if title is not None : ax . set_title ( title ) return cast ( SeabornGraph , gbar ) bs_sns_bar_x_byfg ( df , xstr , fstr , gstr , statistic = np . mean , label_x = None , label_f = None , label_g = None , title = None ) \u00b6 Make a bar plot of x by f and g Parameters: Name Type Description Default df pd . DataFrame dataframe, should contain columns xstr , fstr , and gstr required xstr str column name of x required fstr str column name of f required gstr str column name of g required statistic Callable statistic to plot (by default, the mean) np . mean label_x str | None label of x None label_f str | None label of f None label_g str | None label of g in legend None title str | None title of plot None Returns: Type Description SeabornGraph the plot. Source code in bs_python_utils/bs_seaborn.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def bs_sns_bar_x_byfg ( df : pd . DataFrame , xstr : str , fstr : str , gstr : str , statistic : Callable = np . mean , label_x : str | None = None , label_f : str | None = None , label_g : str | None = None , title : str | None = None , ) -> SeabornGraph : \"\"\"Make a bar plot of x by f and g Args: df: dataframe, should contain columns `xstr`, `fstr`, and `gstr` xstr: column name of x fstr: column name of f gstr: column name of g statistic: statistic to plot (by default, the mean) label_x: label of x label_f: label of f label_g: label of g in legend title: title of plot Returns: the plot. \"\"\" _ , ax = plt . subplots () gbar = sns . barplot ( x = fstr , y = xstr , data = df , hue = gstr , estimator = statistic , errcolor = \"r\" , errwidth = 0.75 , capsize = 0.2 , ax = ax , ) xlab = fstr if label_f is None else label_f ylab = xstr if label_x is None else label_x ax . set_xlabel ( xlab ) ax . set_ylabel ( ylab ) if label_g is not None : gbar_legend = bs_sns_get_legend ( gbar ) gbar_legend . set_title ( label_g ) if title is not None : ax . set_title ( title ) return cast ( SeabornGraph , gbar ) bs_sns_density_estimates ( df , true_values , method_string = 'Estimator' , coeff_string = 'Parameter' , estimate_string = 'Estimate' , max_cols = 3 ) \u00b6 Plots the densities of estimates of several coefficients with several methods, superposed by methods and faceted by coefficients. Parameters: Name Type Description Default df pd . DataFrame contains columns method_string , coeff_name , estimate_value required true_values np . ndarray the true values of the coefficients required method_string str | None the name of the column that indicates the method 'Estimator' coeff_string str | None the name of the column that indicates the coefficient 'Parameter' estimate_string str | None the name of the column that gives the value of the estimate 'Estimate' max_cols int we wrap after that 3 Returns: Type Description sns . FacetGrid the FacetGrid plot. Source code in bs_python_utils/bs_seaborn.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def bs_sns_density_estimates ( df : pd . DataFrame , true_values : np . ndarray , method_string : str | None = \"Estimator\" , coeff_string : str | None = \"Parameter\" , estimate_string : str | None = \"Estimate\" , max_cols : int = 3 , ) -> sns . FacetGrid : \"\"\" Plots the densities of estimates of several coefficients with several methods, superposed by methods and faceted by coefficients. Args: df: contains columns `method_string`, `coeff_name`, `estimate_value` true_values: the true values of the coefficients method_string: the name of the column that indicates the method coeff_string: the name of the column that indicates the coefficient estimate_string: the name of the column that gives the value of the estimate max_cols: we wrap after that Returns: the `FacetGrid` plot. \"\"\" g = sns . FacetGrid ( data = df , sharex = False , sharey = False , hue = method_string , col = coeff_string , col_wrap = max_cols , ) g . map ( sns . kdeplot , estimate_string ) g . set_titles ( \" {col_name} \" ) for true_val , ax in zip ( true_values , g . axes . ravel (), strict = True ): ax . vlines ( true_val , * ax . get_ylim (), color = \"k\" , linestyles = \"dashed\" ) g . add_legend () return g bs_sns_get_legend ( g ) \u00b6 Get the Legend object of a Seaborn plot. Parameters: Name Type Description Default g mpl . axes . Axes the plot object required Returns: Name Type Description leg mpl . legend . Legend the associated Legend object. Source code in bs_python_utils/bs_seaborn.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def bs_sns_get_legend ( g : mpl . axes . Axes ) -> mpl . legend . Legend : \"\"\" Get the `Legend` object of a Seaborn plot. Args: g: the plot object Returns: leg: the associated `Legend` object. \"\"\" # check axes and find which one has a legend axs = g . axes if isinstance ( axs , mpl . axes . Axes ): # only one Axes leg = axs . get_legend () else : for ax in axs . flat (): leg = ax . get_legend () if leg is not None : break # or legend may be on a figure if leg is None : leg = g . _legend return leg","title":"Seaborn"},{"location":"bs_seaborn.html#bs_seaborn-module","text":"Some Seaborn plotting utilities: bs_sns_get_legend : get the Legend object of a Seaborn plot bs_sns_bar_x_byf : make a bar plot of x by f bs_sns_bar_x_byfg : make a bar plot of x by f and g bs_sns_density_estimates : plots the densities of estimates of several coefficients with several methods, superposed by methods and faceted by coefficients.","title":"bs_seaborn module"},{"location":"bs_seaborn.html#bs_python_utils.bs_seaborn.bs_sns_bar_x_byf","text":"Make a bar plot of x by f . Parameters: Name Type Description Default df pd . DataFrame dataframe, should contain columns xstr and fstr required xstr str column name of x required fstr str column name of f required statistic Callable statistic to plot (by default, the mean) np . mean label_x str | None label of x None label_f str | None label of f None title str | None title of plot None Returns: Type Description SeabornGraph the plot. Source code in bs_python_utils/bs_seaborn.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def bs_sns_bar_x_byf ( df : pd . DataFrame , xstr : str , fstr : str , statistic : Callable = np . mean , label_x : str | None = None , label_f : str | None = None , title : str | None = None , ) -> SeabornGraph : \"\"\"Make a bar plot of `x` by `f`. Args: df: dataframe, should contain columns `xstr` and `fstr` xstr: column name of x fstr: column name of f statistic: statistic to plot (by default, the mean) label_x: label of x label_f: label of f title: title of plot Returns: the plot. \"\"\" fig , ax = plt . subplots () gbar = sns . barplot ( x = fstr , y = xstr , data = df , estimator = statistic , errcolor = \"r\" , errwidth = 0.75 , capsize = 0.2 , ax = ax , ) xlab = fstr if label_f is None else label_f ylab = xstr if label_x is None else label_x ax . set_xlabel ( xlab ) ax . set_ylabel ( ylab ) if title is not None : ax . set_title ( title ) return cast ( SeabornGraph , gbar )","title":"bs_sns_bar_x_byf()"},{"location":"bs_seaborn.html#bs_python_utils.bs_seaborn.bs_sns_bar_x_byfg","text":"Make a bar plot of x by f and g Parameters: Name Type Description Default df pd . DataFrame dataframe, should contain columns xstr , fstr , and gstr required xstr str column name of x required fstr str column name of f required gstr str column name of g required statistic Callable statistic to plot (by default, the mean) np . mean label_x str | None label of x None label_f str | None label of f None label_g str | None label of g in legend None title str | None title of plot None Returns: Type Description SeabornGraph the plot. Source code in bs_python_utils/bs_seaborn.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def bs_sns_bar_x_byfg ( df : pd . DataFrame , xstr : str , fstr : str , gstr : str , statistic : Callable = np . mean , label_x : str | None = None , label_f : str | None = None , label_g : str | None = None , title : str | None = None , ) -> SeabornGraph : \"\"\"Make a bar plot of x by f and g Args: df: dataframe, should contain columns `xstr`, `fstr`, and `gstr` xstr: column name of x fstr: column name of f gstr: column name of g statistic: statistic to plot (by default, the mean) label_x: label of x label_f: label of f label_g: label of g in legend title: title of plot Returns: the plot. \"\"\" _ , ax = plt . subplots () gbar = sns . barplot ( x = fstr , y = xstr , data = df , hue = gstr , estimator = statistic , errcolor = \"r\" , errwidth = 0.75 , capsize = 0.2 , ax = ax , ) xlab = fstr if label_f is None else label_f ylab = xstr if label_x is None else label_x ax . set_xlabel ( xlab ) ax . set_ylabel ( ylab ) if label_g is not None : gbar_legend = bs_sns_get_legend ( gbar ) gbar_legend . set_title ( label_g ) if title is not None : ax . set_title ( title ) return cast ( SeabornGraph , gbar )","title":"bs_sns_bar_x_byfg()"},{"location":"bs_seaborn.html#bs_python_utils.bs_seaborn.bs_sns_density_estimates","text":"Plots the densities of estimates of several coefficients with several methods, superposed by methods and faceted by coefficients. Parameters: Name Type Description Default df pd . DataFrame contains columns method_string , coeff_name , estimate_value required true_values np . ndarray the true values of the coefficients required method_string str | None the name of the column that indicates the method 'Estimator' coeff_string str | None the name of the column that indicates the coefficient 'Parameter' estimate_string str | None the name of the column that gives the value of the estimate 'Estimate' max_cols int we wrap after that 3 Returns: Type Description sns . FacetGrid the FacetGrid plot. Source code in bs_python_utils/bs_seaborn.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def bs_sns_density_estimates ( df : pd . DataFrame , true_values : np . ndarray , method_string : str | None = \"Estimator\" , coeff_string : str | None = \"Parameter\" , estimate_string : str | None = \"Estimate\" , max_cols : int = 3 , ) -> sns . FacetGrid : \"\"\" Plots the densities of estimates of several coefficients with several methods, superposed by methods and faceted by coefficients. Args: df: contains columns `method_string`, `coeff_name`, `estimate_value` true_values: the true values of the coefficients method_string: the name of the column that indicates the method coeff_string: the name of the column that indicates the coefficient estimate_string: the name of the column that gives the value of the estimate max_cols: we wrap after that Returns: the `FacetGrid` plot. \"\"\" g = sns . FacetGrid ( data = df , sharex = False , sharey = False , hue = method_string , col = coeff_string , col_wrap = max_cols , ) g . map ( sns . kdeplot , estimate_string ) g . set_titles ( \" {col_name} \" ) for true_val , ax in zip ( true_values , g . axes . ravel (), strict = True ): ax . vlines ( true_val , * ax . get_ylim (), color = \"k\" , linestyles = \"dashed\" ) g . add_legend () return g","title":"bs_sns_density_estimates()"},{"location":"bs_seaborn.html#bs_python_utils.bs_seaborn.bs_sns_get_legend","text":"Get the Legend object of a Seaborn plot. Parameters: Name Type Description Default g mpl . axes . Axes the plot object required Returns: Name Type Description leg mpl . legend . Legend the associated Legend object. Source code in bs_python_utils/bs_seaborn.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def bs_sns_get_legend ( g : mpl . axes . Axes ) -> mpl . legend . Legend : \"\"\" Get the `Legend` object of a Seaborn plot. Args: g: the plot object Returns: leg: the associated `Legend` object. \"\"\" # check axes and find which one has a legend axs = g . axes if isinstance ( axs , mpl . axes . Axes ): # only one Axes leg = axs . get_legend () else : for ax in axs . flat (): leg = ax . get_legend () if leg is not None : break # or legend may be on a figure if leg is None : leg = g . _legend return leg","title":"bs_sns_get_legend()"},{"location":"bs_sparse_gaussian.html","text":"bs_sparse_gaussian module \u00b6 Sets up sparse integration over a Gaussian, given text files that contain rescaled Gauss-Hermite nodes and weights. These files must be named GHsparseGrid{ndims}prec{iprec}.txt , where ndims is the number of dimensions of integration and iprec is a precision level that must be 9, 13, or (most precise) 17. The file must have (ndims+1) columns, with the weights in the first column. The nodes and weights are rescaled so that f(nodes) @ weights approximates Ef(X) for X an N(0,I) variable. setup_sparse_gaussian ( ndims , iprec , GHsparsedir = None ) \u00b6 Get nodes and weights for sparse integration Ef(X) with X = N(0,1) in ndims dimensions. Examples: >>> nodes , weights = setup_sparse_gaussian ( mdims , iprec ) >>> integral_f = f ( nodes ) @ weights Parameters: Name Type Description Default ndims int number of dimensions (1 to 5) required iprec int precision (must be 9, 13, or 17) required GHsparsedir str | None the name of a directory that contains nodes and weights None Returns: Type Description TwoArrays a pair of arrays nodes and weights ; TwoArrays nodes has ndims-1 columns and weights is a vector with the same number of rows. Source code in bs_python_utils/bs_sparse_gaussian.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def setup_sparse_gaussian ( ndims : int , iprec : int , GHsparsedir : str | None = None ) -> TwoArrays : \"\"\" Get nodes and weights for sparse integration Ef(X) with X = N(0,1) in `ndims` dimensions. Examples: >>> nodes, weights = setup_sparse_gaussian(mdims, iprec) >>> integral_f = f(nodes) @ weights Args: ndims: number of dimensions (1 to 5) iprec: precision (must be 9, 13, or 17) GHsparsedir: the name of a directory that contains nodes and weights Returns: a pair of arrays `nodes` and `weights`; `nodes` has `ndims-1` columns and `weights` is a vector with the same number of rows. \"\"\" GHdir = ( Path . home () / \"Dropbox\" / \"GHsparseGrids\" if GHsparsedir is None else Path ( GHsparsedir ) ) if iprec not in [ 9 , 13 , 17 ]: bs_error_abort ( f \"We only do sparse integration with precision 9, 13, or 17, not { iprec } \" ) if ndims in [ 1 , 2 , 3 , 4 , 5 ]: if not GHdir . exists (): bs_error_abort ( \"I did not find the directory with the nodes/weights files.\" ) grid = np . loadtxt ( GHdir / f \"GHsparseGrid { ndims } prec { iprec } .txt\" ) print ( f \" { grid . shape =} \" ) if ndims == 1 : weights = grid [:, 0 ] nodes = grid [:, 1 ] else : weights = grid [:, 0 ] nodes = grid [:, 1 :] return nodes , weights else : bs_error_abort ( f \"We only do sparse integration in one to five dimensions, not { ndims } \" ) return np . zeros ( 1 ), np . zeros ( 1 ) # for mypy","title":"sparse integration for functions of Gaussians"},{"location":"bs_sparse_gaussian.html#bs_sparse_gaussian-module","text":"Sets up sparse integration over a Gaussian, given text files that contain rescaled Gauss-Hermite nodes and weights. These files must be named GHsparseGrid{ndims}prec{iprec}.txt , where ndims is the number of dimensions of integration and iprec is a precision level that must be 9, 13, or (most precise) 17. The file must have (ndims+1) columns, with the weights in the first column. The nodes and weights are rescaled so that f(nodes) @ weights approximates Ef(X) for X an N(0,I) variable.","title":"bs_sparse_gaussian module"},{"location":"bs_sparse_gaussian.html#bs_python_utils.bs_sparse_gaussian.setup_sparse_gaussian","text":"Get nodes and weights for sparse integration Ef(X) with X = N(0,1) in ndims dimensions. Examples: >>> nodes , weights = setup_sparse_gaussian ( mdims , iprec ) >>> integral_f = f ( nodes ) @ weights Parameters: Name Type Description Default ndims int number of dimensions (1 to 5) required iprec int precision (must be 9, 13, or 17) required GHsparsedir str | None the name of a directory that contains nodes and weights None Returns: Type Description TwoArrays a pair of arrays nodes and weights ; TwoArrays nodes has ndims-1 columns and weights is a vector with the same number of rows. Source code in bs_python_utils/bs_sparse_gaussian.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def setup_sparse_gaussian ( ndims : int , iprec : int , GHsparsedir : str | None = None ) -> TwoArrays : \"\"\" Get nodes and weights for sparse integration Ef(X) with X = N(0,1) in `ndims` dimensions. Examples: >>> nodes, weights = setup_sparse_gaussian(mdims, iprec) >>> integral_f = f(nodes) @ weights Args: ndims: number of dimensions (1 to 5) iprec: precision (must be 9, 13, or 17) GHsparsedir: the name of a directory that contains nodes and weights Returns: a pair of arrays `nodes` and `weights`; `nodes` has `ndims-1` columns and `weights` is a vector with the same number of rows. \"\"\" GHdir = ( Path . home () / \"Dropbox\" / \"GHsparseGrids\" if GHsparsedir is None else Path ( GHsparsedir ) ) if iprec not in [ 9 , 13 , 17 ]: bs_error_abort ( f \"We only do sparse integration with precision 9, 13, or 17, not { iprec } \" ) if ndims in [ 1 , 2 , 3 , 4 , 5 ]: if not GHdir . exists (): bs_error_abort ( \"I did not find the directory with the nodes/weights files.\" ) grid = np . loadtxt ( GHdir / f \"GHsparseGrid { ndims } prec { iprec } .txt\" ) print ( f \" { grid . shape =} \" ) if ndims == 1 : weights = grid [:, 0 ] nodes = grid [:, 1 ] else : weights = grid [:, 0 ] nodes = grid [:, 1 :] return nodes , weights else : bs_error_abort ( f \"We only do sparse integration in one to five dimensions, not { ndims } \" ) return np . zeros ( 1 ), np . zeros ( 1 ) # for mypy","title":"setup_sparse_gaussian()"},{"location":"bsmplutils.html","text":"bsmplutils module \u00b6 A Matplotlib utility program: ax_text : annotate an ax with text. ax_text ( ax , str_txt , x , y ) \u00b6 Annotate an ax with text in Matplotlib. Parameters: Name Type Description Default ax axes . Axes the axis we want to annotate required str_txt str a string of text required x float position in fraction of horizontal axis required y float position in fraction of vertical axis required Returns: Type Description axes . Axes the nnotated ax . Source code in bs_python_utils/bsmplutils.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def ax_text ( ax : axes . Axes , str_txt : str , x : float , y : float ) -> axes . Axes : \"\"\" Annotate an `ax` with text in Matplotlib. Args: ax: the axis we want to annotate str_txt: a string of text x: position in fraction of horizontal axis y: position in fraction of vertical axis Returns: the nnotated `ax`. \"\"\" if not ( isinstance ( x , float ) and 0 <= x <= 1 ): bs_error_abort ( \"x should be a number between 0.0 and 1.0\" ) if not ( isinstance ( y , float ) and 0 <= y <= 1 ): bs_error_abort ( \"y should be a number between 0.0 and 1.0\" ) ax . text ( x , y , str_txt , horizontalalignment = \"center\" , verticalalignment = \"center\" , transform = ax . transAxes , ) return ax","title":"Matplotlib"},{"location":"bsmplutils.html#bsmplutils-module","text":"A Matplotlib utility program: ax_text : annotate an ax with text.","title":"bsmplutils module"},{"location":"bsmplutils.html#bs_python_utils.bsmplutils.ax_text","text":"Annotate an ax with text in Matplotlib. Parameters: Name Type Description Default ax axes . Axes the axis we want to annotate required str_txt str a string of text required x float position in fraction of horizontal axis required y float position in fraction of vertical axis required Returns: Type Description axes . Axes the nnotated ax . Source code in bs_python_utils/bsmplutils.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def ax_text ( ax : axes . Axes , str_txt : str , x : float , y : float ) -> axes . Axes : \"\"\" Annotate an `ax` with text in Matplotlib. Args: ax: the axis we want to annotate str_txt: a string of text x: position in fraction of horizontal axis y: position in fraction of vertical axis Returns: the nnotated `ax`. \"\"\" if not ( isinstance ( x , float ) and 0 <= x <= 1 ): bs_error_abort ( \"x should be a number between 0.0 and 1.0\" ) if not ( isinstance ( y , float ) and 0 <= y <= 1 ): bs_error_abort ( \"y should be a number between 0.0 and 1.0\" ) ax . text ( x , y , str_txt , horizontalalignment = \"center\" , verticalalignment = \"center\" , transform = ax . transAxes , ) return ax","title":"ax_text()"},{"location":"bsnputils.html","text":"bsnputils module \u00b6 Contains various numpy utility programs. Note if the math looks strange in the documentation, just reload the page. BivariatePolynomial : a minimal class for bivariate polynomials outer_bivar : make a BivariatePolynomial from two Polynomial objects check_vector , check_matrix , check_vector_or_matrix , check_square , check_tensor : check an array and return its shape grid_function : apply a function on a lattice grid generate_RNG_streams : generate a number of random number streams (for parallelizations) ecdf, inv_ecdf : the empirical cdf of a sample and its inverse nprepeat_col, nprepeat_row : repeat a column or a row npmaxabs : maximum absolute value of the elements of an array rice_stderr : the Rice local standard errors of a random variable bs_sqrt_pdmatrix : square root of a posuitve definite matrix nplog , npexp, npxlogx : \\(C^2\\) extensions of np.log , np.exp , and \\(x\\log x\\) , with first two derivatives nppow : \\(a^b\\) for arrays, with first two derivatives nppad_beg_zeros , nppad_end_zeros , nppad2_end_zeros : pad the beginning or the end of an array with 0 bsgrid, make_lexico_grid : construct grid arrays gauleg, gauher : nodes and weights of Gauss-Legendre and Gauss-Hermite polynomials gaussian_expectation : uses Gauss-Hermite to compute \\(Ef(X)\\) for \\(X=N(0,1)\\) legendre_polynomials : evaluates the Legendre polynomials quantile_transform : returns the quantiles of values in an array print_quantiles : prints requested quantiles of an array set_elements_abovebelow_diagonal : sets all elements of the given matrix above or below the diagonal to a specified scalar value. BivariatePolynomial \u00b6 A class for bivariate polynomials as a list of Polynomial objects, with a minimal interface: construct from a matrix of coefficients add, subtract, multiply (with a constant and with a BivariatePolynomial ) evaluate \\(p(x, y)\\) when x, y are at most vectors (and have the same shape if both vectors) Source code in bs_python_utils/bsnputils.py 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 class BivariatePolynomial : \"\"\" A class for bivariate polynomials as a list of `Polynomial` objects, with a minimal interface: * construct from a matrix of coefficients * add, subtract, multiply (with a constant and with a `BivariatePolynomial`) * evaluate $p(x, y)$ when x, y are at most vectors (and have the same shape if both vectors) \"\"\" def __init__ ( self , coeffs : np . ndarray ): \"\"\" coeffs: a `(deg1+1, deg2+2)` matrix \"\"\" self . deg1 , self . deg2 = coeffs . shape [ 0 ] - 1 , coeffs . shape [ 1 ] - 1 self . coef = coeffs self . listpol2 = [] for k in range ( self . deg1 + 1 ): self . listpol2 . append ( Polynomial ( coeffs [ k , :])) def __add__ ( self , bivpol ): if isinstance ( bivpol , ( int , float )): coeffs = self . coef . copy () coeffs [ 0 , 0 ] += bivpol return BivariatePolynomial ( coeffs ) degbp1 , degbp2 = bivpol . deg1 , bivpol . deg2 max_deg1 = max ( degbp1 , self . deg1 ) max_deg2 = max ( degbp2 , self . deg2 ) coeffs_new = nppad2_end_zeros ( self . coef , max_deg1 + 1 , max_deg2 + 1 ) coeffsbp_new = nppad2_end_zeros ( bivpol . coef , max_deg1 + 1 , max_deg2 + 1 ) return BivariatePolynomial ( coeffs_new + coeffsbp_new ) def __repr__ ( self ): return f \"BivariatePolynomial( { self . deg1 !r} , { self . deg2 !r} )\" def __iadd__ ( self , bivpol ): return self . __add__ ( bivpol ) def __radd__ ( self , bivpol ): return self . __add__ ( bivpol ) def __sub__ ( self , bivpol ): if isinstance ( bivpol , ( int , float )): coeffs = self . coef . copy () coeffs [ 0 , 0 ] -= bivpol return BivariatePolynomial ( coeffs ) degbp1 , degbp2 = bivpol . deg1 , bivpol . deg2 max_deg1 = max ( degbp1 , self . deg1 ) max_deg2 = max ( degbp2 , self . deg2 ) coeffs_new = nppad2_end_zeros ( self . coef , max_deg1 + 1 , max_deg2 + 1 ) coeffsbp_new = nppad2_end_zeros ( bivpol . coef , max_deg1 + 1 , max_deg2 + 1 ) return BivariatePolynomial ( coeffs_new - coeffsbp_new ) def __mul__ ( self , bivpol ): if isinstance ( bivpol , ( int , float )): return BivariatePolynomial ( bivpol * self . coef ) deg1 , degbp1 = self . deg1 , bivpol . deg1 deg2 , degbp2 = self . deg2 , bivpol . deg2 degmul1 = deg1 + degbp1 degmul2 = deg2 + degbp2 lp2 , blp2 = self . listpol2 , bivpol . listpol2 coeffs_mul = np . zeros (( degmul1 + 1 , degmul2 + 1 )) for m in range ( degmul1 + 1 ): minm = max ( 0 , m - degbp1 ) maxm = min ( m , self . deg1 ) pm = Polynomial ( 0 ) for i in range ( minm , maxm + 1 ): pm += lp2 [ i ] * blp2 [ m - i ] coeffs_mul [ m , :] += pm . coef bp_mul = BivariatePolynomial ( coeffs_mul ) return bp_mul def __rmul__ ( self , bivpol ): return self . __mul__ ( bivpol ) def __call__ ( self , x1 , x2 ): x1fac = 1.0 val = 0.0 for p in self . listpol2 : val += p ( x2 ) * x1fac x1fac *= x1 return val __init__ ( coeffs ) \u00b6 coeffs: a (deg1+1, deg2+2) matrix Source code in bs_python_utils/bsnputils.py 717 718 719 720 721 722 723 724 725 def __init__ ( self , coeffs : np . ndarray ): \"\"\" coeffs: a `(deg1+1, deg2+2)` matrix \"\"\" self . deg1 , self . deg2 = coeffs . shape [ 0 ] - 1 , coeffs . shape [ 1 ] - 1 self . coef = coeffs self . listpol2 = [] for k in range ( self . deg1 + 1 ): self . listpol2 . append ( Polynomial ( coeffs [ k , :])) bs_sqrt_pdmatrix ( m ) \u00b6 square root of a positive definite matrix Parameters: Name Type Description Default m np . ndarray a positive definite matrix required Returns: Type Description np . ndarray the square root of the matrix. Source code in bs_python_utils/bsnputils.py 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 def bs_sqrt_pdmatrix ( m : np . ndarray ) -> np . ndarray : \"\"\" square root of a positive definite matrix Args: m: a positive definite matrix Returns: the square root of the matrix. \"\"\" _ = check_square ( m , \"bs_sqrt_pdmatrix\" ) eigval , eigvec = np . linalg . eigh ( m ) eigval = np . maximum ( eigval , 0.0 ) eigval_sqrt = np . sqrt ( eigval ) eigval_sqrt_diag = np . diag ( eigval_sqrt ) res = eigvec @ eigval_sqrt_diag @ eigvec . T return cast ( np . ndarray , res ) bsgrid ( v , w ) \u00b6 make a two-dimensional matrix of all pairs of elements of the vectors v and w Parameters: Name Type Description Default v np . ndarray basis vector, size m required w np . ndarray basis vector, size n required Returns: Type Description np . ndarray an array of shape (m n,2) . Examples: >>> v = np . array ([ 1 , 2 , 3 ]) >>> w = np . array ([ 4 , 5 ]) >>> bsgrid ( v , w ) array([[1, 4], [1, 5], [2, 4], [2, 5], [3, 4], [3, 5]]) Source code in bs_python_utils/bsnputils.py 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 def bsgrid ( v : np . ndarray , w : np . ndarray ) -> np . ndarray : \"\"\" make a two-dimensional matrix of all pairs of elements of the vectors `v` and `w` Args: v: basis vector, size m w: basis vector, size n Returns: an array of shape `(m n,2)`. Examples: >>> v = np.array([1,2,3]) >>> w = np.array([4,5]) >>> bsgrid(v, w) array([[1, 4], [1, 5], [2, 4], [2, 5], [3, 4], [3, 5]]) \"\"\" m = check_vector ( v ) n = check_vector ( w ) m , n = v . size , w . size v1 = np . repeat ( v , n ) v2 = np . tile ( w , m ) return np . column_stack (( v1 , v2 )) check_matrix ( x , fun_name = None ) \u00b6 test that x is a matrix; aborts otherwise Parameters: Name Type Description Default x Any a matrix, we hope required fun_name str name of the calling function None Returns: Type Description tuple [ int , int ] the shape if successful Source code in bs_python_utils/bsnputils.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def check_matrix ( x : Any , fun_name : str = None ) -> tuple [ int , int ]: \"\"\" test that `x` is a matrix; aborts otherwise Args: x: a matrix, we hope fun_name: name of the calling function Returns: the shape if successful \"\"\" fun_str = [ \"\" if fun_name is None else fun_name + \":\" ] if not isinstance ( x , np . ndarray ): bs_error_abort ( f \" { fun_str } Xx should be a Numpy array\" ) x = cast ( np . ndarray , x ) ndims_x = x . ndim if ndims_x != 2 : bs_error_abort ( f \" { fun_str } x should have two dimensions, not { ndims_x } \" ) return cast ( tuple [ int , int ], x . shape ) check_square ( A , fun_name = None ) \u00b6 test that an object used in fun_name is a square matrix Parameters: Name Type Description Default A Any square matrix, we hope required fun_name str the name of the calling function None Returns: Type Description int the number of rows and columns of A Source code in bs_python_utils/bsnputils.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def check_square ( A : Any , fun_name : str = None ) -> int : \"\"\" test that an object used in `fun_name` is a square matrix Args: A: square matrix, we hope fun_name: the name of the calling function Returns: the number of rows and columns of `A` \"\"\" fun_str = [ \"\" if fun_name is None else fun_name + \":\" ] if not isinstance ( A , np . ndarray ): bs_error_abort ( f \" { fun_str } A should be a Numpy array\" ) A = cast ( np . ndarray , A ) if A . ndim == 2 : n , nv = A . shape if nv != n : bs_error_abort ( f \" { fun_str } The matrix A should be square, not { A . shape } \" ) else : bs_error_abort ( f \" { fun_name } A should have two dimensions, not { A . ndim } \" ) return cast ( int , n ) check_tensor ( x , n_dims , fun_name = None ) \u00b6 test that x is an n_dims dimensional array; aborts otherwise Parameters: Name Type Description Default x Any an n_dims dimensional array, we hope required fun_name str name of the calling function None Returns: Type Description tuple [ int , ...] the shape if successful Source code in bs_python_utils/bsnputils.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def check_tensor ( x : Any , n_dims : int , fun_name : str = None ) -> tuple [ int , ... ]: \"\"\" test that `x` is an `n_dims` dimensional array; aborts otherwise Args: x: an `n_dims` dimensional array, we hope fun_name: name of the calling function Returns: the shape if successful \"\"\" fun_str = [ \"\" if fun_name is None else fun_name + \":\" ] if not isinstance ( x , np . ndarray ): bs_error_abort ( f \" { fun_str } x should be a Numpy array\" ) x = cast ( np . ndarray , x ) ndims_x = x . ndim if ndims_x != n_dims : bs_error_abort ( f \" { fun_str } x should have { n_dims } dimensions, not { ndims_x } \" ) return ( 0 ,) # for mypy return cast ( tuple [ int , ... ], x . shape ) check_vector ( v , fun_name = None ) \u00b6 test that v is a vector; aborts otherwise Parameters: Name Type Description Default v Any a vector, we hope required fun_name str name of the calling function None Returns: Type Description int the size if successful. Source code in bs_python_utils/bsnputils.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def check_vector ( v : Any , fun_name : str = None ) -> int : \"\"\" test that `v` is a vector; aborts otherwise Args: v: a vector, we hope fun_name: name of the calling function Returns: the size if successful. \"\"\" fun_str = [ \"\" if fun_name is None else fun_name + \":\" ] if not isinstance ( v , np . ndarray ): bs_error_abort ( f \" { fun_str } v should be a Numpy array\" ) v = cast ( np . ndarray , v ) ndims_v = v . ndim if ndims_v != 1 : bs_error_abort ( f \" { fun_str } v should have one dimension, not { ndims_v } \" ) return cast ( int , v . size ) check_vector_or_matrix ( x , fun_name = None ) \u00b6 test that x is a vector or a matrix; aborts otherwise Parameters: Name Type Description Default x Any a vector or matrix, we hope required fun_name str name of the calling function None Returns: Type Description int the number of dimensions of x (1 or 2) Source code in bs_python_utils/bsnputils.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def check_vector_or_matrix ( x : Any , fun_name : str = None ) -> int : \"\"\" test that `x` is a vector or a matrix; aborts otherwise Args: x: a vector or matrix, we hope fun_name: name of the calling function Returns: the number of dimensions of `x` (1 or 2) \"\"\" fun_str = [ \"\" if fun_name is None else fun_name + \":\" ] if not isinstance ( x , np . ndarray ): bs_error_abort ( f \" { fun_str } X should be a Numpy array\" ) x = cast ( np . ndarray , x ) ndims_x = x . ndim if ndims_x != 1 and ndims_x != 2 : bs_error_abort ( f \" { fun_str } x should have at most two dimensions, not { ndims_x } \" ) return cast ( int , ndims_x ) ecdf ( x ) \u00b6 Evaluate the empirical cdf at each point in sample Parameters: Name Type Description Default x np . ndarray 1-dim array (nobs) required Returns: Type Description np . ndarray A 1-dim array (nobs) with the values of the empirical cdf at x , from 1/ nobs to 1 Source code in bs_python_utils/bsnputils.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def ecdf ( x : np . ndarray ) -> np . ndarray : \"\"\"Evaluate the empirical cdf at each point in sample Args: x: 1-dim array `(nobs)` Returns: A 1-dim array `(nobs)` with the values of the empirical cdf at `x`, from 1/`nobs` to 1 \"\"\" if x . ndim != 1 : print_stars ( f \"ecdf: x should have 1 dimension, not { x . ndim } \" ) sys . exit () nx = x . size order_x = np . argsort ( x ) ecdf_val = np . zeros ( nx ) for i_order , n_order in enumerate ( order_x ): ecdf_val [ n_order ] = ( i_order + 1.0 ) / nx return ecdf_val gauher ( n ) \u00b6 nodes and weights for Gauss-Hermite integration Parameters: Name Type Description Default n int number of nodes required Returns: Type Description TwoArrays array of n nodes, array of n weights Source code in bs_python_utils/bsnputils.py 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 def gauher ( n : int ) -> TwoArrays : \"\"\" nodes and weights for Gauss-Hermite integration Args: n: number of nodes Returns: array of `n` nodes, array of `n` weights \"\"\" EPS = 1.0e-14 PIM4 = 0.7511255444649425 MAXIT = 10 x = np . zeros ( n ) w = np . zeros ( n ) m = ( n + 1 ) // 2 for i in range ( m ): if i == 0 : n2 = 2.0 * n + 1.0 z = sqrt ( n2 ) - 1.85575 * ( n2 **- 0.16667 ) elif i == 1 : z -= 1.14 * ( n ** 0.426 ) / z elif i == 2 : z = 1.86 * z - 0.86 * x [ 0 ] elif i == 3 : z = 1.91 * z - 0.91 * x [ 1 ] else : z = 2.0 * z - x [ i - 2 ] for _n_iter in range ( MAXIT ): p1 = PIM4 p2 = 0.0 for j in range ( n ): p3 = p2 p2 = p1 p1 = z * sqrt ( 2.0 / ( j + 1 )) * p2 - sqrt ( j / ( j + 1 )) * p3 pp = sqrt ( 2 * n ) * p2 z1 = z z = z1 - p1 / pp if abs ( z - z1 ) <= EPS : break if _n_iter >= MAXIT : bs_error_abort ( f \"too many iterations: { _n_iter } \" ) x [ i ] = z x [ n - 1 - i ] = - z w [ i ] = 2.0 / ( pp * pp ) w [ n - 1 - i ] = w [ i ] # need to reverse order for x (w is symmetric) return cast ( TwoArrays , ( x [:: - 1 ], w )) gauleg ( n ) \u00b6 nodes and weights for Gauss-Legendre integration \\int_{-1}^1 f(x)dx Parameters: Name Type Description Default n int number of nodes required Returns: Type Description TwoArrays array of n nodes, array of n weights Source code in bs_python_utils/bsnputils.py 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 def gauleg ( n : int ) -> TwoArrays : \"\"\" nodes and weights for Gauss-Legendre integration `\\\\int_{-1}^1 f(x)dx` Args: n: number of nodes Returns: array of `n` nodes, array of `n` weights \"\"\" x = np . zeros ( n ) w = np . zeros ( n ) EPS = 3e-11 m = ( n + 1 ) // 2 for i in range ( 1 , m + 1 ): z = cos ( pi * ( i - 0.25 ) / ( n + 0.5 )) z1 = np . inf while abs ( z - z1 ) > EPS : p1 = 1.0 p2 = 0.0 for j in range ( 1 , n + 1 ): p3 = p2 p2 = p1 p1 = (( 2.0 * j - 1.0 ) * z * p2 - ( j - 1.0 ) * p3 ) / j pp = n * ( z * p1 - p2 ) / ( z * z - 1.0 ) z1 = z z = z1 - p1 / pp x [ i - 1 ] = - z x [ n - i ] = z w [ i - 1 ] = 2.0 / (( 1.0 - z * z ) * pp * pp ) w [ n - i ] = w [ i - 1 ] return cast ( TwoArrays , ( x , w )) gaussian_expectation ( f , x , w , n = 16 , vectorized = False , pars = None ) \u00b6 computes the expectation of a function of an N(0,1) random variable using Gauss-Hermite with n nodes the nodes and weights can be provided, if available Parameters: Name Type Description Default f Callable a scalar or array function of a scalar or array variable and possibly other parameters required vectorized bool if True, the function accepts an array as argument False pars Iterable parameters for f , if any None n int number of nodes 16 x np . ndarray | None locations of the nodes required w np . ndarray | None their weights required Returns: Type Description np . ndarray | float the expectation of f(N(0,1)) Source code in bs_python_utils/bsnputils.py 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 def gaussian_expectation ( f : Callable , x : np . ndarray | None , w : np . ndarray | None , n : int = 16 , vectorized : bool = False , pars : Iterable = None , ) -> np . ndarray | float : \"\"\" computes the expectation of a function of an `N(0,1)` random variable using Gauss-Hermite with n nodes the nodes and weights can be provided, if available Args: f: a scalar or array function of a scalar or array variable and possibly other parameters vectorized: if True, the function accepts an array as argument pars: parameters for `f`, if any n: number of nodes x: locations of the nodes w: their weights Returns: the expectation of `f(N(0,1))` \"\"\" if x is None : nodes , weights = gauher ( n ) nodes *= sqrt ( 2.0 ) weights /= sqrt ( pi ) n_nodes = n elif w is None : bs_error_abort ( \"x is None but w is not\" ) elif w . size != x . size : bs_error_abort ( \"x has {x.size} elements and w has {w.size} \" ) else : nodes = x * sqrt ( 2.0 ) weights = w / sqrt ( pi ) n_nodes = nodes . size if pars is None : if vectorized : integral_vec = f ( nodes ) @ weights else : # to ensure integral_val has the same shape as f integral_val = weights [ 0 ] * f ( nodes [ 0 ]) for i in range ( 1 , n_nodes ): integral_val += weights [ i ] * f ( nodes [ i ]) else : if vectorized : integral_vec = f ( nodes , pars ) @ weights else : # to ensure integral_val has the same shape as f integral_val = weights [ 0 ] * f ( nodes [ 0 ], pars ) for i in range ( 1 , n_nodes ): integral_val += weights [ i ] * f ( nodes [ i ], pars ) return cast ( np . ndarray , integral_vec ) if vectorized else cast ( float , integral_val ) generate_RNG_streams ( nsim , initial_seed = 13091962 ) \u00b6 return nsim random number generators Parameters: Name Type Description Default nsim int number of RNGs we want required initial_seed int any large integer 13091962 Returns: Type Description list [ np . random . Generator ] nsim streams Examples: >>> streams = generate_RNG_streams ( 10 , 575856896 ) >>> x = streams [ i ] . normal ( scale = s , size = ( nmarkets , nproducts )) Source code in bs_python_utils/bsnputils.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def generate_RNG_streams ( nsim : int , initial_seed : int = 13091962 ) -> list [ np . random . Generator ]: \"\"\" return `nsim` random number generators Args: nsim: number of RNGs we want initial_seed: any large integer Returns: `nsim` streams Examples: >>> streams = generate_RNG_streams(10, 575856896) >>> x = streams[i].normal(scale=s, size=(nmarkets, nproducts)) \"\"\" ss = np . random . SeedSequence ( initial_seed ) # Spawn off child SeedSequences to pass to child processes. child_seeds = ss . spawn ( nsim ) streams = [ np . random . default_rng ( s ) for s in child_seeds ] return streams grid_function ( fun , x_points , y_points ) \u00b6 apply a function f(x, y) on a lattice grid Parameters: Name Type Description Default fun Callable [[ np . ndarray , np . ndarray ], np . ndarray ] should return a matrix (m, n) when called with two matrices (m, n) required x_points np . ndarray an m -vector required y_points np . ndarray an n -vector required Returns: Type Description np . ndarray the (m, n) matrix of values of fun on the grid Source code in bs_python_utils/bsnputils.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def grid_function ( fun : Callable [[ np . ndarray , np . ndarray ], np . ndarray ], x_points : np . ndarray , y_points : np . ndarray , ) -> np . ndarray : \"\"\"apply a function `f(x, y)` on a lattice grid Args: fun: should return a matrix `(m, n)` when called with two matrices `(m, n)` x_points: an `m`-vector y_points: an `n`-vector Returns: the `(m, n)` matrix of values of `fun` on the grid \"\"\" _ = check_vector ( x_points ) _ = check_vector ( y_points ) X1 , Y1 = np . meshgrid ( x_points , y_points ) z_grid = fun ( X1 , Y1 ) . T return z_grid inv_ecdf ( v , q ) \u00b6 Evaluate the empirical q -quantiles of the sample v in a way that is consistent with ecdf . Parameters: Name Type Description Default v np . ndarray 1-dim array (nobs) of the data points required q np . ndarray | float 1-dim array (nobs) of quantiles or float required Returns: Type Description np . ndarray | float A 1-dim array (nobs) with the values of the q -quantiles of v , or just the one quantile Source code in bs_python_utils/bsnputils.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 def inv_ecdf ( v : np . ndarray , q : np . ndarray | float ) -> np . ndarray | float : \"\"\"Evaluate the empirical `q`-quantiles of the sample `v` in a way that is consistent with `ecdf`. Args: v: 1-dim array `(nobs)` of the data points q: 1-dim array `(nobs)` of quantiles or float Returns: A 1-dim array `(nobs)` with the values of the `q`-quantiles of `v`, or just the one quantile \"\"\" if v . ndim != 1 : bs_error_abort ( f \"v should have 1 dimension, not { v . ndim } \" ) nv = v . size sorted_v = np . zeros ( nv + 2 ) sorted_v [ 1 : ( nv + 1 )] = np . sort ( v ) sorted_v [ 0 ] = 2.0 * sorted_v [ 1 ] - sorted_v [ 2 ] # added to extend for q < 1/nv sorted_v [ nv + 1 ] = sorted_v [ nv ] # added to extend for q = 1 if isinstance ( q , float ): q_floor = np . array ([ floor ( nv * q )]) val_q = sorted_v [ q_floor ] + ( nv * q - q_floor ) * ( sorted_v [ q_floor + 1 ] - sorted_v [ q_floor ] ) return cast ( float , val_q ) elif isinstance ( q , np . ndarray ): q_floor = np . floor ( nv * q ) . astype ( int ) vals_q = sorted_v [ q_floor ] + ( nv * q - q_floor ) * ( sorted_v [ q_floor + 1 ] - sorted_v [ q_floor ] ) return cast ( np . ndarray , vals_q ) legendre_polynomials ( x , max_deg , a =- 1.0 , b = 1.0 , no_constant = False ) \u00b6 evaluates the Legendre polynomials over x in the interval \\([a, b]\\) Parameters: Name Type Description Default x np . ndarray the points where the polynomials are to be evaluated required max_deg int the maximum degree required a float the start of the interval, classically -1 -1.0 b float the end of the interval, classically 1 1.0 no_constant bool if True, delete the constant polynomial False Returns: Type Description np . ndarray an array of (max_deg+1) arrays of the shape of x . Source code in bs_python_utils/bsnputils.py 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 def legendre_polynomials ( x : np . ndarray , max_deg : int , a : float = - 1.0 , b : float = 1.0 , no_constant : bool = False , ) -> np . ndarray : \"\"\"evaluates the Legendre polynomials over `x` in the interval $[a, b]$ Args: x: the points where the polynomials are to be evaluated max_deg: the maximum degree a: the start of the interval, classically -1 b: the end of the interval, classically 1 no_constant: if True, delete the constant polynomial Returns: an array of `(max_deg+1)` arrays of the shape of `x`. \"\"\" sx = check_vector ( x ) if a > np . min ( x ): sys . exit ( \"legendre_polynomials: points below start of interval\" ) if b < np . max ( x ): sys . exit ( \"legendre_polynomials: points above end of interval\" ) p = np . zeros (( sx , max_deg + 1 )) x_transf = 2.0 * ( x - a ) / ( b - a ) - 1.0 p [:, 0 ] = np . ones_like ( x ) p [:, 1 ] = x_transf for deg in range ( 2 , max_deg + 1 ): p2 = ( 2 * deg - 1 ) * ( p [:, deg - 1 ] * x_transf ) - ( deg - 1 ) * p [:, deg - 2 ] p [:, deg ] = p2 / deg polys_p = p [:, 1 :] if no_constant else p return polys_p make_lexico_grid ( arr ) \u00b6 1 make a lexicographic grid; it is a generalization of `bsgrid` for $n_c eq 2$. 1 2 3 4 5 Args: arr: an $n_r$-vector or an $(n_r,n_c)$ matrix; $n_c$` must be 1, 2 or 3 Returns: `arr` if it is a vector; otherwise a matrix $(n_r^{n_c}, n_c)$. Source code in bs_python_utils/bsnputils.py 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 def make_lexico_grid ( arr : np . ndarray ) -> np . ndarray : \"\"\" make a lexicographic grid; it is a generalization of `bsgrid` for $n_c\\neq 2$. Args: arr: an $n_r$-vector or an $(n_r,n_c)$ matrix; $n_c$` must be 1, 2 or 3 Returns: `arr` if it is a vector; otherwise a matrix $(n_r^{n_c}, n_c)$. \"\"\" ndims_arr = check_vector_or_matrix ( arr , \"make_lexico_grid`\" ) if ndims_arr == 1 : return arr else : nr , nc = arr . shape if nc == 2 : n0 = np . repeat ( arr [:, 0 ], nr ) n1 = np . tile ( arr [:, 1 ], nr ) return np . column_stack (( n0 , n1 )) elif nc == 3 : nr2 = nr * nr n0 = np . repeat ( arr [:, 0 ], nr2 ) n1 = np . repeat ( np . tile ( arr [:, 1 ], nr ), nr ) n2 = np . tile ( arr [:, 2 ], nr2 ) return np . column_stack (( n0 , n1 , n2 )) else : bs_error_abort ( f \"at this stage, the number of columns must be 3 or less, not { nc } ...\" ) return arr # for mypy npexp ( arr , bigx = 50.0 , lowx =- 50.0 , deriv = 0 , verbose = False ) \u00b6 \\(C^2\\) extension of \\(\\exp(a)\\) above bigx and below lowx , perhaps with derivatives Parameters: Name Type Description Default arr np . ndarray any Numpy array required bigx float upper bound 50.0 lowx float lower bound -50.0 deriv int if 1, compute derivative, if 2, second derivative 0 verbose bool prints debugging info False Returns: Type Description np . ndarray | TwoArrays | ThreeArrays \\(\\exp(a)\\) \\(C^2\\) -extended above bigx and below lowx , np . ndarray | TwoArrays | ThreeArrays perhaps with derivatives Source code in bs_python_utils/bsnputils.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def npexp ( arr : np . ndarray , bigx : float = 50.0 , lowx : float = - 50.0 , deriv : int = 0 , verbose : bool = False , ) -> np . ndarray | TwoArrays | ThreeArrays : \"\"\" $C^2$ extension of $\\\\exp(a)$ above `bigx` and below `lowx`, perhaps with derivatives Args: arr: any Numpy array bigx: upper bound lowx: lower bound deriv: if 1, compute derivative, if 2, second derivative verbose: prints debugging info Returns: $\\\\exp(a)$ $C^2$-extended above `bigx` and below `lowx`, perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv can only be 0, 1, or 2; not { deriv } \" ) min_arr , max_arr = np . min ( arr ), np . max ( arr ) if max_arr <= bigx and min_arr >= lowx : exparr = np . exp ( arr ) if deriv == 0 : return cast ( np . ndarray , exparr ) elif deriv == 1 : return cast ( TwoArrays , ( exparr , exparr )) # deriv == 2 return cast ( ThreeArrays , ( exparr , exparr , exparr )) else : # some large and/or small arguments exparr = np . exp ( np . maximum ( np . minimum ( arr , bigx ), lowx )) print ( f \" { exparr =} \" ) ebigx = exp ( bigx ) elowx = exp ( lowx ) darrb = arr - bigx darrl = lowx - arr exparr_larger = ebigx * ( 1.0 + darrb * ( 1.0 + 0.5 * darrb )) exparr_smaller = elowx * ( 1.0 - darrl * ( 1.0 - 0.5 * darrl )) if verbose : n_large_args = np . sum ( arr > bigx ) finals = \"s\" if n_large_args > 1 else \"\" print ( f \"npexp: { n_large_args } argument { finals } larger than { bigx } : \\n \" f \"maxi = { np . max ( arr ) } \" ) n_small_args = np . sum ( arr < lowx ) finals = \"s\" if n_small_args > 1 else \"\" print ( f \"npexp: { n_small_args } argument { finals } smaller than { lowx } : \\n \" f \"mini = { np . min ( arr ) } \" ) expval = exparr print ( expval ) expval = np . where ( arr > bigx , exparr_larger , expval ) expval = np . where ( arr < lowx , exparr_smaller , expval ) if deriv == 0 : return cast ( np . ndarray , expval ) dexpval = exparr dexparr_larger = ebigx * ( 1.0 + darrb ) dexparr_smaller = elowx * ( 1.0 - darrl ) dexpval = np . where ( arr > bigx , dexparr_larger , dexpval ) dexpval = np . where ( arr < lowx , dexparr_smaller , dexpval ) if deriv == 1 : return cast ( TwoArrays , ( expval , dexpval )) # deriv == 2 d2expval = exparr return cast ( ThreeArrays , ( expval , dexpval , d2expval )) nplog ( arr , eps = 1e-30 , deriv = 0 , verbose = False ) \u00b6 \\(C^2\\) extension of \\(\\ln(a)\\) below eps , perhaps with derivatives Parameters: Name Type Description Default arr np . ndarray any Numpy array required eps float lower bound 1e-30 deriv int if 1, compute derivative, if 2, second derivative 0 verbose bool prints debugging info False Returns: Type Description np . ndarray | TwoArrays | ThreeArrays \\(\\ln(a)\\) \\(C^2\\) -extended below eps , perhaps with derivatives Source code in bs_python_utils/bsnputils.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def nplog ( arr : np . ndarray , eps : float = 1e-30 , deriv : int = 0 , verbose : bool = False , ) -> np . ndarray | TwoArrays | ThreeArrays : \"\"\" $C^2$ extension of $\\\\ln(a)$ below `eps`, perhaps with derivatives Args: arr: any Numpy array eps: lower bound deriv: if 1, compute derivative, if 2, second derivative verbose: prints debugging info Returns: $\\\\ln(a)$ $C^2$-extended below `eps`, perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv can only be 0, 1, or 2; not { deriv } \" ) if np . min ( arr ) > eps : if deriv == 0 : return cast ( np . ndarray , np . log ( arr )) elif deriv == 1 : return cast ( TwoArrays , ( np . log ( arr ), 1.0 / arr )) # deriv == 2 return cast ( ThreeArrays , ( np . log ( arr ), 1.0 / arr , - 1.0 / ( arr * arr ))) else : logarreps = np . log ( np . maximum ( arr , eps )) darr = 1.0 - arr / eps logarr_smaller = log ( eps ) - darr * ( 1.0 + darr / 2.0 ) if verbose : n_small_args = np . sum ( arr < eps ) if n_small_args > 0 : finals = \"s\" if n_small_args > 1 else \"\" print ( f \"nplog: { n_small_args } argument { finals } smaller than { eps } : mini =\" f \" { np . min ( arr ) } \" ) logeps = np . where ( arr > eps , logarreps , logarr_smaller ) if deriv == 0 : return logeps arreps = np . maximum ( arr , eps ) der_logarreps = 1.0 / arreps der_logarr_smaller = ( 1.0 + darr ) / eps dlogeps = np . where ( arr > eps , der_logarreps , der_logarr_smaller ) if deriv == 1 : return cast ( TwoArrays , ( logeps , dlogeps )) # deriv == 2 der2_logarreps = - 1.0 / ( arreps * arreps ) der2_logarr_smaller = np . full ( arr . shape , - 1.0 / ( eps * eps )) d2logeps = np . where ( arr > eps , der2_logarreps , der2_logarr_smaller ) return cast ( ThreeArrays , ( logeps , dlogeps , d2logeps )) npmaxabs ( arr ) \u00b6 maximum absolute value in an array Parameters: Name Type Description Default arr np . ndarray any Numpy array required Returns: Type Description float the largest element in absolute value Source code in bs_python_utils/bsnputils.py 290 291 292 293 294 295 296 297 298 299 300 def npmaxabs ( arr : np . ndarray ) -> float : \"\"\" maximum absolute value in an array Args: arr: any Numpy array Returns: the largest element in absolute value \"\"\" return cast ( float , np . max ( np . abs ( arr ))) nppad2_end_zeros ( mat , m , n ) \u00b6 pad the ends of a 2-dim array with zeros to increase its size to (m,n) , if needed Parameters: Name Type Description Default mat np . ndarray 2-dim array required m int number of rows requested required n int number of columns requested required Returns: Type Description np . ndarray padded array, where needed Source code in bs_python_utils/bsnputils.py 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 def nppad2_end_zeros ( mat : np . ndarray , m : int , n : int ) -> np . ndarray : \"\"\" pad the ends of a 2-dim array with zeros to increase its size to `(m,n)`, if needed Args: mat: 2-dim array m: number of rows requested n: number of columns requested Returns: padded array, where needed \"\"\" nrows , ncols = check_matrix ( mat ) max_rows = max ( m , nrows ) max_cols = max ( n , ncols ) if nrows < max_rows and ncols < max_cols : # pad both dimensions pmat = np . zeros (( m , n )) pmat [: nrows , : ncols ] = mat return pmat elif nrows < max_rows : # pad rows pmat = np . zeros (( m , ncols )) pmat [: nrows , :] = mat return pmat elif ncols < max_cols : # pad columns pmat = np . zeros (( nrows , n )) pmat [:, : ncols ] = mat return pmat else : # no need for padding return mat nppad_beg_zeros ( v , n ) \u00b6 pad the beginning of a 1-dim array with zeros to increase its size to n , if needed Parameters: Name Type Description Default v np . ndarray 1-dim array of size (nv) required n int size requested required Returns: Type Description np . ndarray padded array if nv < n , otherwise v Source code in bs_python_utils/bsnputils.py 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 def nppad_beg_zeros ( v : np . ndarray , n : int ) -> np . ndarray : \"\"\" pad the beginning of a 1-dim array with zeros to increase its size to `n`, if needed Args: v: 1-dim array of size `(nv)` n: size requested Returns: padded array if `nv` < `n`, otherwise `v` \"\"\" nv = check_vector ( v ) if nv < n : return np . pad ( v , ( n - nv , 0 )) else : return v nppad_end_zeros ( v , n ) \u00b6 pad the end of a 1-dim array with zeros to increase its size to n , if needed Parameters: Name Type Description Default v np . ndarray 1-dim array of size (nv) required n int size requested required Returns: Type Description np . ndarray padded array if nv < n , else v Source code in bs_python_utils/bsnputils.py 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 def nppad_end_zeros ( v : np . ndarray , n : int ) -> np . ndarray : \"\"\" pad the end of a 1-dim array with zeros to increase its size to `n`, if needed Args: v: 1-dim array of size `(nv)` n: size requested Returns: padded array if `nv` < `n`, else `v` \"\"\" nv = check_vector ( v ) if nv < n : return np . pad ( v , ( 0 , n - nv )) else : return v nppow ( a , b , deriv = 0 ) \u00b6 evaluates a**b element-by-element, perhaps with derivatives Parameters: Name Type Description Default a np . ndarray an array required b int | float | np . ndarray if an array, should have the same shape as a required deriv int if 1, compute derivative, if 2, second derivative 0 Returns: Type Description np . ndarray | ThreeArrays | SixArrays an array of the same shape as a Source code in bs_python_utils/bsnputils.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def nppow ( a : np . ndarray , b : int | float | np . ndarray , deriv : int = 0 ) -> np . ndarray | ThreeArrays | SixArrays : \"\"\" evaluates a**b element-by-element, perhaps with derivatives Args: a: an array b: if an array, should have the same shape as `a` deriv: if 1, compute derivative, if 2, second derivative Returns: an array of the same shape as `a` \"\"\" if isinstance ( b , float ): mina = np . min ( a ) if mina < 0.0 : bs_error_abort ( \"All elements of a must be positive!\" ) if isinstance ( b , ( int , float )): a_pow_b = a ** b if deriv == 0 : return a_pow_b log_a = np . log ( a ) derivs1 = ( b * a_pow_b / a , a_pow_b * log_a ) if deriv == 1 : return cast ( ThreeArrays , ( a_pow_b , * derivs1 )) b1 = b - 1.0 a_pow_b1 = a_pow_b / a # deriv == 2 derivs2 = ( b * b1 * a_pow_b1 / a , a_pow_b1 * ( 1.0 + b * log_a ), a_pow_b * log_a * log_a , ) return cast ( SixArrays , ( a_pow_b , * derivs1 , * derivs2 )) else : if a . shape != b . shape : bs_error_abort ( \"b is not a number or an array of the same shape as a!\" ) return _nppow_arrays ( a , b , deriv ) nprepeat_col ( v , n ) \u00b6 create a matrix with n columns equal to v Parameters: Name Type Description Default v np . ndarray a 1-dim array of size m required n int the number of columns requested required Returns: Type Description np . ndarray a 2-dim array of shape (m, n) Source code in bs_python_utils/bsnputils.py 262 263 264 265 266 267 268 269 270 271 272 273 def nprepeat_col ( v : np . ndarray , n : int ) -> np . ndarray : \"\"\" create a matrix with `n` columns equal to `v` Args: v: a 1-dim array of size `m` n: the number of columns requested Returns: a 2-dim array of shape `(m, n)` \"\"\" return np . repeat ( v [:, np . newaxis ], n , axis = 1 ) nprepeat_row ( v , m ) \u00b6 create a matrix with m rows equal to v Parameters: Name Type Description Default v np . ndarray a 1-dim array of size n required m int the number of rows requested required Returns: Type Description np . ndarray a 2-dim array of shape (m, n) Source code in bs_python_utils/bsnputils.py 276 277 278 279 280 281 282 283 284 285 286 287 def nprepeat_row ( v : np . ndarray , m : int ) -> np . ndarray : \"\"\" create a matrix with `m` rows equal to `v` Args: v: a 1-dim array of size `n` m: the number of rows requested Returns: a 2-dim array of shape `(m, n)` \"\"\" return np . repeat ( v [ np . newaxis , :], m , axis = 0 ) npxlogx ( arr , eps = 1e-30 , deriv = 0 , verbose = False ) \u00b6 \\(C^2\\) extension of \\(a\\ln(a)\\) below eps , perhaps with derivatives Parameters: Name Type Description Default arr np . ndarray a Numpy array required eps float lower bound 1e-30 deriv int if 1, compute derivative, if 2, second derivative 0 verbose bool prints debugging info False Returns: Type Description np . ndarray | TwoArrays | ThreeArrays \\(a\\ln(a)\\) \\(C^2\\) -extended below eps , perhaps with derivatives Source code in bs_python_utils/bsnputils.py 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 def npxlogx ( arr : np . ndarray , eps : float = 1e-30 , deriv : int = 0 , verbose : bool = False , ) -> np . ndarray | TwoArrays | ThreeArrays : \"\"\" $C^2$ extension of $a\\\\ln(a)$ below `eps`, perhaps with derivatives Args: arr: a Numpy array eps: lower bound deriv: if 1, compute derivative, if 2, second derivative verbose: prints debugging info Returns: $a\\\\ln(a)$ $C^2$-extended below `eps`, perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv must be 0, 1, or 2; not { deriv } \" ) if np . min ( arr ) > eps : return cast ( np . ndarray , arr * np . log ( arr )) else : logeps = log ( eps ) logarreps = np . log ( np . maximum ( arr , eps )) xlogarreps = arr * logarreps xlogarr_smaller = arr * ( arr / eps + logeps - 1.0 ) if verbose : n_small_args = np . sum ( arr < eps ) if n_small_args > 0 : finals = \"s\" if n_small_args > 1 else \"\" print ( f \"npxlogx: { n_small_args } argument { finals } smaller than { eps } : mini\" f \" = { np . min ( arr ) } \" ) xlogval = np . where ( arr > eps , xlogarreps , xlogarr_smaller ) if deriv == 0 : return xlogval dxlogarreps = 1.0 + logarreps dxlogarr_smaller = logeps + arr / eps dxlogval = np . where ( arr > eps , dxlogarreps , dxlogarr_smaller ) if deriv == 1 : return cast ( TwoArrays , ( xlogval , dxlogval )) # deriv == 2 d2xlogval = 1.0 / np . maximum ( arr , eps ) return cast ( ThreeArrays , ( xlogval , dxlogval , d2xlogval )) outer_bivar ( pol1 , pol2 ) \u00b6 make a BivariatePolynomial from the product of two Polynomial objects Parameters: Name Type Description Default pol1 Polynomial Polynomial in the first variable required pol2 Polynomial Polynomial in the second variable required Returns: Type Description BivariatePolynomial a BivariatePolynomial = pol1 * pol2 Source code in bs_python_utils/bsnputils.py 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 def outer_bivar ( pol1 : Polynomial , pol2 : Polynomial ) -> BivariatePolynomial : \"\"\" make a `BivariatePolynomial` from the product of two `Polynomial` objects Args: pol1: Polynomial in the first variable pol2: Polynomial in the second variable Returns: a `BivariatePolynomial` = `pol1 * pol2` \"\"\" p1 = pol1 . coef p2 = pol2 . coef prod_coef = np . outer ( p1 , p2 ) return BivariatePolynomial ( prod_coef ) print_quantiles ( v , quantiles ) \u00b6 print these quantiles of the array(s) Parameters: Name Type Description Default v np . ndarray | Iterable [ np . ndarray ] a vector or an iterable of vectors required quantiles np . ndarray quantiles in [0,1] required Returns: Type Description np . ndarray the corresponding quantiles as a vector or a matrix Source code in bs_python_utils/bsnputils.py 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 def print_quantiles ( v : np . ndarray | Iterable [ np . ndarray ], quantiles : np . ndarray ) -> np . ndarray : \"\"\"print these quantiles of the array(s) Args: v: a vector or an iterable of vectors quantiles: quantiles in [0,1] Returns: the corresponding quantiles as a vector or a matrix \"\"\" nq = check_vector ( quantiles ) if isinstance ( v , np . ndarray ): qvals = np . quantile ( v , quantiles ) for q , qv in zip ( quantiles , qvals , strict = True ): print ( f \"Quantile { q : .3f } : { qv : >10.3f } \" ) elif isinstance ( v , Iterable ): v = list ( v ) for v_i in v : _ = check_vector ( v_i ) nv = len ( v ) qvals = np . zeros (( nq , nv )) for i in range ( nv ): qvals [:, i ] = np . quantile ( v [ i ], quantiles ) for iq , q in enumerate ( quantiles ): s = f \"Quantile { q : .3f } : \" qv = qvals [ iq , :] for i in range ( nv ): s += f \" { qv [ i ] : >10.3f } \" print ( f \" { s } \" ) else : bs_error_abort ( \"v must be a vector or a list of vectors\" ) return cast ( np . ndarray , qvals ) quantile_transform ( v ) \u00b6 transform a vector of counts into the corresponding quantiles Parameters: Name Type Description Default v np . ndarray a vector of counts required Returns: Type Description np . ndarray the corresponding quantiles Source code in bs_python_utils/bsnputils.py 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 def quantile_transform ( v : np . ndarray ) -> np . ndarray : \"\"\"transform a vector of counts into the corresponding quantiles Args: v: a vector of counts Returns: the corresponding quantiles \"\"\" n = check_vector ( v ) q = np . zeros ( n ) for i in range ( n ): q [ i ] = np . sum ( v <= v [ i ]) / ( n + 1 ) return q rice_stderr ( y , x , is_sorted = False ) \u00b6 computes the Rice local estimators of the standard error of y | x Parameters: Name Type Description Default y np . ndarray vector of y-values required x np . ndarray vector of x-values required is_sorted bool set it to True if x is in increasing order False Returns: Type Description np . ndarray | float an array of the same size with the stderr(y | x) Source code in bs_python_utils/bsnputils.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def rice_stderr ( y : np . ndarray , x : np . ndarray , is_sorted : bool = False ) -> np . ndarray | float : \"\"\" computes the Rice local estimators of the standard error of y | x Args: y: vector of y-values x: vector of x-values is_sorted: set it to `True` if `x` is in increasing order Returns: an array of the same size with the stderr(y | x) \"\"\" n = check_vector ( x ) ny = check_vector ( y ) if ny != n : bs_error_abort ( \"x and y should have the same size\" ) if not is_sorted : # need to sort by increasing value of x order_x = np . argsort ( x ) ys = y [ order_x ] else : ys = y variance_estimator = np . zeros ( n ) # we average over neighbors n_neighbors = int ( sqrt ( float ( n )) / 2.0 ) facd = 1.0 / ( 2.0 * n_neighbors ) n_neighbors2 = n_neighbors // 2 # for the first observations yleft = ys [: n_neighbors2 ] dy = yleft [ 1 :] - yleft [: - 1 ] variance_estimator [: n_neighbors2 ] = np . sum ( dy * dy ) * facd # for the middle of the sample minus_nn2 = n - n_neighbors2 for ix in range ( n_neighbors2 , minus_nn2 ): ix_neighbors = slice ( ix - n_neighbors2 , ix + n_neighbors2 ) yx = ys [ ix_neighbors ] dy = yx [ 1 :] - yx [: - 1 ] variance_estimator [ ix ] = np . sum ( dy * dy ) * facd # and for the last observations yright = ys [ minus_nn2 :] dy = yright [ 1 :] - yright [: - 1 ] variance_estimator [ minus_nn2 :] = np . sum ( dy * dy ) * facd stderr_estimator = np . sqrt ( variance_estimator ) return stderr_estimator set_elements_abovebelow_diagonal ( matrix , scalar , location ) \u00b6 Sets all elements of the given matrix above or below the diagonal to the specified scalar value. Parameters: Name Type Description Default matrix np . ndarray the input matrix; it must be square required scalar int | float The scalar value to set the elements above or below the diagonal. required location str 'above', 'below', 'on_above', 'on_below'. required Returns: Type Description np . ndarray The updated matrix with elements above or below the diagonal set to the scalar value, np . ndarray including the diagonal for the on_ options. Source code in bs_python_utils/bsnputils.py 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 def set_elements_abovebelow_diagonal ( matrix : np . ndarray , scalar : int | float , location : str ) -> np . ndarray : \"\"\" Sets all elements of the given matrix above or below the diagonal to the specified scalar value. Args: matrix: the input matrix; it must be square scalar: The scalar value to set the elements above or below the diagonal. location: 'above', 'below', 'on_above', 'on_below'. Returns: The updated matrix with elements above or below the diagonal set to the scalar value, including the diagonal for the `on_` options. \"\"\" _ = check_square ( matrix , \"set_elements_abovebelow_diagonal\" ) # copy the matrix new_matrix = matrix . copy () # Get the indices of elements above or below the diagonal if location == \"above\" : row_indices , col_indices = np . triu_indices_from ( new_matrix , k = 1 ) elif location == \"below\" : row_indices , col_indices = np . tril_indices_from ( new_matrix , k =- 1 ) elif location == \"on_above\" : row_indices , col_indices = np . triu_indices_from ( new_matrix , k = 0 ) elif location == \"on_below\" : row_indices , col_indices = np . tril_indices_from ( new_matrix , k = 0 ) else : bs_error_abort ( f \"\"\" location can only be 'above', 'below', 'on_above' or 'on_below', not { location } \"\"\" ) # Set the elements above or below the diagonal to the scalar value new_matrix [ row_indices , col_indices ] = scalar return new_matrix","title":"Numpy"},{"location":"bsnputils.html#bsnputils-module","text":"Contains various numpy utility programs. Note if the math looks strange in the documentation, just reload the page. BivariatePolynomial : a minimal class for bivariate polynomials outer_bivar : make a BivariatePolynomial from two Polynomial objects check_vector , check_matrix , check_vector_or_matrix , check_square , check_tensor : check an array and return its shape grid_function : apply a function on a lattice grid generate_RNG_streams : generate a number of random number streams (for parallelizations) ecdf, inv_ecdf : the empirical cdf of a sample and its inverse nprepeat_col, nprepeat_row : repeat a column or a row npmaxabs : maximum absolute value of the elements of an array rice_stderr : the Rice local standard errors of a random variable bs_sqrt_pdmatrix : square root of a posuitve definite matrix nplog , npexp, npxlogx : \\(C^2\\) extensions of np.log , np.exp , and \\(x\\log x\\) , with first two derivatives nppow : \\(a^b\\) for arrays, with first two derivatives nppad_beg_zeros , nppad_end_zeros , nppad2_end_zeros : pad the beginning or the end of an array with 0 bsgrid, make_lexico_grid : construct grid arrays gauleg, gauher : nodes and weights of Gauss-Legendre and Gauss-Hermite polynomials gaussian_expectation : uses Gauss-Hermite to compute \\(Ef(X)\\) for \\(X=N(0,1)\\) legendre_polynomials : evaluates the Legendre polynomials quantile_transform : returns the quantiles of values in an array print_quantiles : prints requested quantiles of an array set_elements_abovebelow_diagonal : sets all elements of the given matrix above or below the diagonal to a specified scalar value.","title":"bsnputils module"},{"location":"bsnputils.html#bs_python_utils.bsnputils.BivariatePolynomial","text":"A class for bivariate polynomials as a list of Polynomial objects, with a minimal interface: construct from a matrix of coefficients add, subtract, multiply (with a constant and with a BivariatePolynomial ) evaluate \\(p(x, y)\\) when x, y are at most vectors (and have the same shape if both vectors) Source code in bs_python_utils/bsnputils.py 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 class BivariatePolynomial : \"\"\" A class for bivariate polynomials as a list of `Polynomial` objects, with a minimal interface: * construct from a matrix of coefficients * add, subtract, multiply (with a constant and with a `BivariatePolynomial`) * evaluate $p(x, y)$ when x, y are at most vectors (and have the same shape if both vectors) \"\"\" def __init__ ( self , coeffs : np . ndarray ): \"\"\" coeffs: a `(deg1+1, deg2+2)` matrix \"\"\" self . deg1 , self . deg2 = coeffs . shape [ 0 ] - 1 , coeffs . shape [ 1 ] - 1 self . coef = coeffs self . listpol2 = [] for k in range ( self . deg1 + 1 ): self . listpol2 . append ( Polynomial ( coeffs [ k , :])) def __add__ ( self , bivpol ): if isinstance ( bivpol , ( int , float )): coeffs = self . coef . copy () coeffs [ 0 , 0 ] += bivpol return BivariatePolynomial ( coeffs ) degbp1 , degbp2 = bivpol . deg1 , bivpol . deg2 max_deg1 = max ( degbp1 , self . deg1 ) max_deg2 = max ( degbp2 , self . deg2 ) coeffs_new = nppad2_end_zeros ( self . coef , max_deg1 + 1 , max_deg2 + 1 ) coeffsbp_new = nppad2_end_zeros ( bivpol . coef , max_deg1 + 1 , max_deg2 + 1 ) return BivariatePolynomial ( coeffs_new + coeffsbp_new ) def __repr__ ( self ): return f \"BivariatePolynomial( { self . deg1 !r} , { self . deg2 !r} )\" def __iadd__ ( self , bivpol ): return self . __add__ ( bivpol ) def __radd__ ( self , bivpol ): return self . __add__ ( bivpol ) def __sub__ ( self , bivpol ): if isinstance ( bivpol , ( int , float )): coeffs = self . coef . copy () coeffs [ 0 , 0 ] -= bivpol return BivariatePolynomial ( coeffs ) degbp1 , degbp2 = bivpol . deg1 , bivpol . deg2 max_deg1 = max ( degbp1 , self . deg1 ) max_deg2 = max ( degbp2 , self . deg2 ) coeffs_new = nppad2_end_zeros ( self . coef , max_deg1 + 1 , max_deg2 + 1 ) coeffsbp_new = nppad2_end_zeros ( bivpol . coef , max_deg1 + 1 , max_deg2 + 1 ) return BivariatePolynomial ( coeffs_new - coeffsbp_new ) def __mul__ ( self , bivpol ): if isinstance ( bivpol , ( int , float )): return BivariatePolynomial ( bivpol * self . coef ) deg1 , degbp1 = self . deg1 , bivpol . deg1 deg2 , degbp2 = self . deg2 , bivpol . deg2 degmul1 = deg1 + degbp1 degmul2 = deg2 + degbp2 lp2 , blp2 = self . listpol2 , bivpol . listpol2 coeffs_mul = np . zeros (( degmul1 + 1 , degmul2 + 1 )) for m in range ( degmul1 + 1 ): minm = max ( 0 , m - degbp1 ) maxm = min ( m , self . deg1 ) pm = Polynomial ( 0 ) for i in range ( minm , maxm + 1 ): pm += lp2 [ i ] * blp2 [ m - i ] coeffs_mul [ m , :] += pm . coef bp_mul = BivariatePolynomial ( coeffs_mul ) return bp_mul def __rmul__ ( self , bivpol ): return self . __mul__ ( bivpol ) def __call__ ( self , x1 , x2 ): x1fac = 1.0 val = 0.0 for p in self . listpol2 : val += p ( x2 ) * x1fac x1fac *= x1 return val","title":"BivariatePolynomial"},{"location":"bsnputils.html#bs_python_utils.bsnputils.BivariatePolynomial.__init__","text":"coeffs: a (deg1+1, deg2+2) matrix Source code in bs_python_utils/bsnputils.py 717 718 719 720 721 722 723 724 725 def __init__ ( self , coeffs : np . ndarray ): \"\"\" coeffs: a `(deg1+1, deg2+2)` matrix \"\"\" self . deg1 , self . deg2 = coeffs . shape [ 0 ] - 1 , coeffs . shape [ 1 ] - 1 self . coef = coeffs self . listpol2 = [] for k in range ( self . deg1 + 1 ): self . listpol2 . append ( Polynomial ( coeffs [ k , :]))","title":"__init__()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.bs_sqrt_pdmatrix","text":"square root of a positive definite matrix Parameters: Name Type Description Default m np . ndarray a positive definite matrix required Returns: Type Description np . ndarray the square root of the matrix. Source code in bs_python_utils/bsnputils.py 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 def bs_sqrt_pdmatrix ( m : np . ndarray ) -> np . ndarray : \"\"\" square root of a positive definite matrix Args: m: a positive definite matrix Returns: the square root of the matrix. \"\"\" _ = check_square ( m , \"bs_sqrt_pdmatrix\" ) eigval , eigvec = np . linalg . eigh ( m ) eigval = np . maximum ( eigval , 0.0 ) eigval_sqrt = np . sqrt ( eigval ) eigval_sqrt_diag = np . diag ( eigval_sqrt ) res = eigvec @ eigval_sqrt_diag @ eigvec . T return cast ( np . ndarray , res )","title":"bs_sqrt_pdmatrix()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.bsgrid","text":"make a two-dimensional matrix of all pairs of elements of the vectors v and w Parameters: Name Type Description Default v np . ndarray basis vector, size m required w np . ndarray basis vector, size n required Returns: Type Description np . ndarray an array of shape (m n,2) . Examples: >>> v = np . array ([ 1 , 2 , 3 ]) >>> w = np . array ([ 4 , 5 ]) >>> bsgrid ( v , w ) array([[1, 4], [1, 5], [2, 4], [2, 5], [3, 4], [3, 5]]) Source code in bs_python_utils/bsnputils.py 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 def bsgrid ( v : np . ndarray , w : np . ndarray ) -> np . ndarray : \"\"\" make a two-dimensional matrix of all pairs of elements of the vectors `v` and `w` Args: v: basis vector, size m w: basis vector, size n Returns: an array of shape `(m n,2)`. Examples: >>> v = np.array([1,2,3]) >>> w = np.array([4,5]) >>> bsgrid(v, w) array([[1, 4], [1, 5], [2, 4], [2, 5], [3, 4], [3, 5]]) \"\"\" m = check_vector ( v ) n = check_vector ( w ) m , n = v . size , w . size v1 = np . repeat ( v , n ) v2 = np . tile ( w , m ) return np . column_stack (( v1 , v2 ))","title":"bsgrid()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.check_matrix","text":"test that x is a matrix; aborts otherwise Parameters: Name Type Description Default x Any a matrix, we hope required fun_name str name of the calling function None Returns: Type Description tuple [ int , int ] the shape if successful Source code in bs_python_utils/bsnputils.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def check_matrix ( x : Any , fun_name : str = None ) -> tuple [ int , int ]: \"\"\" test that `x` is a matrix; aborts otherwise Args: x: a matrix, we hope fun_name: name of the calling function Returns: the shape if successful \"\"\" fun_str = [ \"\" if fun_name is None else fun_name + \":\" ] if not isinstance ( x , np . ndarray ): bs_error_abort ( f \" { fun_str } Xx should be a Numpy array\" ) x = cast ( np . ndarray , x ) ndims_x = x . ndim if ndims_x != 2 : bs_error_abort ( f \" { fun_str } x should have two dimensions, not { ndims_x } \" ) return cast ( tuple [ int , int ], x . shape )","title":"check_matrix()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.check_square","text":"test that an object used in fun_name is a square matrix Parameters: Name Type Description Default A Any square matrix, we hope required fun_name str the name of the calling function None Returns: Type Description int the number of rows and columns of A Source code in bs_python_utils/bsnputils.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def check_square ( A : Any , fun_name : str = None ) -> int : \"\"\" test that an object used in `fun_name` is a square matrix Args: A: square matrix, we hope fun_name: the name of the calling function Returns: the number of rows and columns of `A` \"\"\" fun_str = [ \"\" if fun_name is None else fun_name + \":\" ] if not isinstance ( A , np . ndarray ): bs_error_abort ( f \" { fun_str } A should be a Numpy array\" ) A = cast ( np . ndarray , A ) if A . ndim == 2 : n , nv = A . shape if nv != n : bs_error_abort ( f \" { fun_str } The matrix A should be square, not { A . shape } \" ) else : bs_error_abort ( f \" { fun_name } A should have two dimensions, not { A . ndim } \" ) return cast ( int , n )","title":"check_square()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.check_tensor","text":"test that x is an n_dims dimensional array; aborts otherwise Parameters: Name Type Description Default x Any an n_dims dimensional array, we hope required fun_name str name of the calling function None Returns: Type Description tuple [ int , ...] the shape if successful Source code in bs_python_utils/bsnputils.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def check_tensor ( x : Any , n_dims : int , fun_name : str = None ) -> tuple [ int , ... ]: \"\"\" test that `x` is an `n_dims` dimensional array; aborts otherwise Args: x: an `n_dims` dimensional array, we hope fun_name: name of the calling function Returns: the shape if successful \"\"\" fun_str = [ \"\" if fun_name is None else fun_name + \":\" ] if not isinstance ( x , np . ndarray ): bs_error_abort ( f \" { fun_str } x should be a Numpy array\" ) x = cast ( np . ndarray , x ) ndims_x = x . ndim if ndims_x != n_dims : bs_error_abort ( f \" { fun_str } x should have { n_dims } dimensions, not { ndims_x } \" ) return ( 0 ,) # for mypy return cast ( tuple [ int , ... ], x . shape )","title":"check_tensor()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.check_vector","text":"test that v is a vector; aborts otherwise Parameters: Name Type Description Default v Any a vector, we hope required fun_name str name of the calling function None Returns: Type Description int the size if successful. Source code in bs_python_utils/bsnputils.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def check_vector ( v : Any , fun_name : str = None ) -> int : \"\"\" test that `v` is a vector; aborts otherwise Args: v: a vector, we hope fun_name: name of the calling function Returns: the size if successful. \"\"\" fun_str = [ \"\" if fun_name is None else fun_name + \":\" ] if not isinstance ( v , np . ndarray ): bs_error_abort ( f \" { fun_str } v should be a Numpy array\" ) v = cast ( np . ndarray , v ) ndims_v = v . ndim if ndims_v != 1 : bs_error_abort ( f \" { fun_str } v should have one dimension, not { ndims_v } \" ) return cast ( int , v . size )","title":"check_vector()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.check_vector_or_matrix","text":"test that x is a vector or a matrix; aborts otherwise Parameters: Name Type Description Default x Any a vector or matrix, we hope required fun_name str name of the calling function None Returns: Type Description int the number of dimensions of x (1 or 2) Source code in bs_python_utils/bsnputils.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def check_vector_or_matrix ( x : Any , fun_name : str = None ) -> int : \"\"\" test that `x` is a vector or a matrix; aborts otherwise Args: x: a vector or matrix, we hope fun_name: name of the calling function Returns: the number of dimensions of `x` (1 or 2) \"\"\" fun_str = [ \"\" if fun_name is None else fun_name + \":\" ] if not isinstance ( x , np . ndarray ): bs_error_abort ( f \" { fun_str } X should be a Numpy array\" ) x = cast ( np . ndarray , x ) ndims_x = x . ndim if ndims_x != 1 and ndims_x != 2 : bs_error_abort ( f \" { fun_str } x should have at most two dimensions, not { ndims_x } \" ) return cast ( int , ndims_x )","title":"check_vector_or_matrix()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.ecdf","text":"Evaluate the empirical cdf at each point in sample Parameters: Name Type Description Default x np . ndarray 1-dim array (nobs) required Returns: Type Description np . ndarray A 1-dim array (nobs) with the values of the empirical cdf at x , from 1/ nobs to 1 Source code in bs_python_utils/bsnputils.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def ecdf ( x : np . ndarray ) -> np . ndarray : \"\"\"Evaluate the empirical cdf at each point in sample Args: x: 1-dim array `(nobs)` Returns: A 1-dim array `(nobs)` with the values of the empirical cdf at `x`, from 1/`nobs` to 1 \"\"\" if x . ndim != 1 : print_stars ( f \"ecdf: x should have 1 dimension, not { x . ndim } \" ) sys . exit () nx = x . size order_x = np . argsort ( x ) ecdf_val = np . zeros ( nx ) for i_order , n_order in enumerate ( order_x ): ecdf_val [ n_order ] = ( i_order + 1.0 ) / nx return ecdf_val","title":"ecdf()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.gauher","text":"nodes and weights for Gauss-Hermite integration Parameters: Name Type Description Default n int number of nodes required Returns: Type Description TwoArrays array of n nodes, array of n weights Source code in bs_python_utils/bsnputils.py 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 def gauher ( n : int ) -> TwoArrays : \"\"\" nodes and weights for Gauss-Hermite integration Args: n: number of nodes Returns: array of `n` nodes, array of `n` weights \"\"\" EPS = 1.0e-14 PIM4 = 0.7511255444649425 MAXIT = 10 x = np . zeros ( n ) w = np . zeros ( n ) m = ( n + 1 ) // 2 for i in range ( m ): if i == 0 : n2 = 2.0 * n + 1.0 z = sqrt ( n2 ) - 1.85575 * ( n2 **- 0.16667 ) elif i == 1 : z -= 1.14 * ( n ** 0.426 ) / z elif i == 2 : z = 1.86 * z - 0.86 * x [ 0 ] elif i == 3 : z = 1.91 * z - 0.91 * x [ 1 ] else : z = 2.0 * z - x [ i - 2 ] for _n_iter in range ( MAXIT ): p1 = PIM4 p2 = 0.0 for j in range ( n ): p3 = p2 p2 = p1 p1 = z * sqrt ( 2.0 / ( j + 1 )) * p2 - sqrt ( j / ( j + 1 )) * p3 pp = sqrt ( 2 * n ) * p2 z1 = z z = z1 - p1 / pp if abs ( z - z1 ) <= EPS : break if _n_iter >= MAXIT : bs_error_abort ( f \"too many iterations: { _n_iter } \" ) x [ i ] = z x [ n - 1 - i ] = - z w [ i ] = 2.0 / ( pp * pp ) w [ n - 1 - i ] = w [ i ] # need to reverse order for x (w is symmetric) return cast ( TwoArrays , ( x [:: - 1 ], w ))","title":"gauher()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.gauleg","text":"nodes and weights for Gauss-Legendre integration \\int_{-1}^1 f(x)dx Parameters: Name Type Description Default n int number of nodes required Returns: Type Description TwoArrays array of n nodes, array of n weights Source code in bs_python_utils/bsnputils.py 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 def gauleg ( n : int ) -> TwoArrays : \"\"\" nodes and weights for Gauss-Legendre integration `\\\\int_{-1}^1 f(x)dx` Args: n: number of nodes Returns: array of `n` nodes, array of `n` weights \"\"\" x = np . zeros ( n ) w = np . zeros ( n ) EPS = 3e-11 m = ( n + 1 ) // 2 for i in range ( 1 , m + 1 ): z = cos ( pi * ( i - 0.25 ) / ( n + 0.5 )) z1 = np . inf while abs ( z - z1 ) > EPS : p1 = 1.0 p2 = 0.0 for j in range ( 1 , n + 1 ): p3 = p2 p2 = p1 p1 = (( 2.0 * j - 1.0 ) * z * p2 - ( j - 1.0 ) * p3 ) / j pp = n * ( z * p1 - p2 ) / ( z * z - 1.0 ) z1 = z z = z1 - p1 / pp x [ i - 1 ] = - z x [ n - i ] = z w [ i - 1 ] = 2.0 / (( 1.0 - z * z ) * pp * pp ) w [ n - i ] = w [ i - 1 ] return cast ( TwoArrays , ( x , w ))","title":"gauleg()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.gaussian_expectation","text":"computes the expectation of a function of an N(0,1) random variable using Gauss-Hermite with n nodes the nodes and weights can be provided, if available Parameters: Name Type Description Default f Callable a scalar or array function of a scalar or array variable and possibly other parameters required vectorized bool if True, the function accepts an array as argument False pars Iterable parameters for f , if any None n int number of nodes 16 x np . ndarray | None locations of the nodes required w np . ndarray | None their weights required Returns: Type Description np . ndarray | float the expectation of f(N(0,1)) Source code in bs_python_utils/bsnputils.py 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 def gaussian_expectation ( f : Callable , x : np . ndarray | None , w : np . ndarray | None , n : int = 16 , vectorized : bool = False , pars : Iterable = None , ) -> np . ndarray | float : \"\"\" computes the expectation of a function of an `N(0,1)` random variable using Gauss-Hermite with n nodes the nodes and weights can be provided, if available Args: f: a scalar or array function of a scalar or array variable and possibly other parameters vectorized: if True, the function accepts an array as argument pars: parameters for `f`, if any n: number of nodes x: locations of the nodes w: their weights Returns: the expectation of `f(N(0,1))` \"\"\" if x is None : nodes , weights = gauher ( n ) nodes *= sqrt ( 2.0 ) weights /= sqrt ( pi ) n_nodes = n elif w is None : bs_error_abort ( \"x is None but w is not\" ) elif w . size != x . size : bs_error_abort ( \"x has {x.size} elements and w has {w.size} \" ) else : nodes = x * sqrt ( 2.0 ) weights = w / sqrt ( pi ) n_nodes = nodes . size if pars is None : if vectorized : integral_vec = f ( nodes ) @ weights else : # to ensure integral_val has the same shape as f integral_val = weights [ 0 ] * f ( nodes [ 0 ]) for i in range ( 1 , n_nodes ): integral_val += weights [ i ] * f ( nodes [ i ]) else : if vectorized : integral_vec = f ( nodes , pars ) @ weights else : # to ensure integral_val has the same shape as f integral_val = weights [ 0 ] * f ( nodes [ 0 ], pars ) for i in range ( 1 , n_nodes ): integral_val += weights [ i ] * f ( nodes [ i ], pars ) return cast ( np . ndarray , integral_vec ) if vectorized else cast ( float , integral_val )","title":"gaussian_expectation()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.generate_RNG_streams","text":"return nsim random number generators Parameters: Name Type Description Default nsim int number of RNGs we want required initial_seed int any large integer 13091962 Returns: Type Description list [ np . random . Generator ] nsim streams Examples: >>> streams = generate_RNG_streams ( 10 , 575856896 ) >>> x = streams [ i ] . normal ( scale = s , size = ( nmarkets , nproducts )) Source code in bs_python_utils/bsnputils.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def generate_RNG_streams ( nsim : int , initial_seed : int = 13091962 ) -> list [ np . random . Generator ]: \"\"\" return `nsim` random number generators Args: nsim: number of RNGs we want initial_seed: any large integer Returns: `nsim` streams Examples: >>> streams = generate_RNG_streams(10, 575856896) >>> x = streams[i].normal(scale=s, size=(nmarkets, nproducts)) \"\"\" ss = np . random . SeedSequence ( initial_seed ) # Spawn off child SeedSequences to pass to child processes. child_seeds = ss . spawn ( nsim ) streams = [ np . random . default_rng ( s ) for s in child_seeds ] return streams","title":"generate_RNG_streams()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.grid_function","text":"apply a function f(x, y) on a lattice grid Parameters: Name Type Description Default fun Callable [[ np . ndarray , np . ndarray ], np . ndarray ] should return a matrix (m, n) when called with two matrices (m, n) required x_points np . ndarray an m -vector required y_points np . ndarray an n -vector required Returns: Type Description np . ndarray the (m, n) matrix of values of fun on the grid Source code in bs_python_utils/bsnputils.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def grid_function ( fun : Callable [[ np . ndarray , np . ndarray ], np . ndarray ], x_points : np . ndarray , y_points : np . ndarray , ) -> np . ndarray : \"\"\"apply a function `f(x, y)` on a lattice grid Args: fun: should return a matrix `(m, n)` when called with two matrices `(m, n)` x_points: an `m`-vector y_points: an `n`-vector Returns: the `(m, n)` matrix of values of `fun` on the grid \"\"\" _ = check_vector ( x_points ) _ = check_vector ( y_points ) X1 , Y1 = np . meshgrid ( x_points , y_points ) z_grid = fun ( X1 , Y1 ) . T return z_grid","title":"grid_function()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.inv_ecdf","text":"Evaluate the empirical q -quantiles of the sample v in a way that is consistent with ecdf . Parameters: Name Type Description Default v np . ndarray 1-dim array (nobs) of the data points required q np . ndarray | float 1-dim array (nobs) of quantiles or float required Returns: Type Description np . ndarray | float A 1-dim array (nobs) with the values of the q -quantiles of v , or just the one quantile Source code in bs_python_utils/bsnputils.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 def inv_ecdf ( v : np . ndarray , q : np . ndarray | float ) -> np . ndarray | float : \"\"\"Evaluate the empirical `q`-quantiles of the sample `v` in a way that is consistent with `ecdf`. Args: v: 1-dim array `(nobs)` of the data points q: 1-dim array `(nobs)` of quantiles or float Returns: A 1-dim array `(nobs)` with the values of the `q`-quantiles of `v`, or just the one quantile \"\"\" if v . ndim != 1 : bs_error_abort ( f \"v should have 1 dimension, not { v . ndim } \" ) nv = v . size sorted_v = np . zeros ( nv + 2 ) sorted_v [ 1 : ( nv + 1 )] = np . sort ( v ) sorted_v [ 0 ] = 2.0 * sorted_v [ 1 ] - sorted_v [ 2 ] # added to extend for q < 1/nv sorted_v [ nv + 1 ] = sorted_v [ nv ] # added to extend for q = 1 if isinstance ( q , float ): q_floor = np . array ([ floor ( nv * q )]) val_q = sorted_v [ q_floor ] + ( nv * q - q_floor ) * ( sorted_v [ q_floor + 1 ] - sorted_v [ q_floor ] ) return cast ( float , val_q ) elif isinstance ( q , np . ndarray ): q_floor = np . floor ( nv * q ) . astype ( int ) vals_q = sorted_v [ q_floor ] + ( nv * q - q_floor ) * ( sorted_v [ q_floor + 1 ] - sorted_v [ q_floor ] ) return cast ( np . ndarray , vals_q )","title":"inv_ecdf()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.legendre_polynomials","text":"evaluates the Legendre polynomials over x in the interval \\([a, b]\\) Parameters: Name Type Description Default x np . ndarray the points where the polynomials are to be evaluated required max_deg int the maximum degree required a float the start of the interval, classically -1 -1.0 b float the end of the interval, classically 1 1.0 no_constant bool if True, delete the constant polynomial False Returns: Type Description np . ndarray an array of (max_deg+1) arrays of the shape of x . Source code in bs_python_utils/bsnputils.py 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 def legendre_polynomials ( x : np . ndarray , max_deg : int , a : float = - 1.0 , b : float = 1.0 , no_constant : bool = False , ) -> np . ndarray : \"\"\"evaluates the Legendre polynomials over `x` in the interval $[a, b]$ Args: x: the points where the polynomials are to be evaluated max_deg: the maximum degree a: the start of the interval, classically -1 b: the end of the interval, classically 1 no_constant: if True, delete the constant polynomial Returns: an array of `(max_deg+1)` arrays of the shape of `x`. \"\"\" sx = check_vector ( x ) if a > np . min ( x ): sys . exit ( \"legendre_polynomials: points below start of interval\" ) if b < np . max ( x ): sys . exit ( \"legendre_polynomials: points above end of interval\" ) p = np . zeros (( sx , max_deg + 1 )) x_transf = 2.0 * ( x - a ) / ( b - a ) - 1.0 p [:, 0 ] = np . ones_like ( x ) p [:, 1 ] = x_transf for deg in range ( 2 , max_deg + 1 ): p2 = ( 2 * deg - 1 ) * ( p [:, deg - 1 ] * x_transf ) - ( deg - 1 ) * p [:, deg - 2 ] p [:, deg ] = p2 / deg polys_p = p [:, 1 :] if no_constant else p return polys_p","title":"legendre_polynomials()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.make_lexico_grid","text":"1 make a lexicographic grid; it is a generalization of `bsgrid` for $n_c eq 2$. 1 2 3 4 5 Args: arr: an $n_r$-vector or an $(n_r,n_c)$ matrix; $n_c$` must be 1, 2 or 3 Returns: `arr` if it is a vector; otherwise a matrix $(n_r^{n_c}, n_c)$. Source code in bs_python_utils/bsnputils.py 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 def make_lexico_grid ( arr : np . ndarray ) -> np . ndarray : \"\"\" make a lexicographic grid; it is a generalization of `bsgrid` for $n_c\\neq 2$. Args: arr: an $n_r$-vector or an $(n_r,n_c)$ matrix; $n_c$` must be 1, 2 or 3 Returns: `arr` if it is a vector; otherwise a matrix $(n_r^{n_c}, n_c)$. \"\"\" ndims_arr = check_vector_or_matrix ( arr , \"make_lexico_grid`\" ) if ndims_arr == 1 : return arr else : nr , nc = arr . shape if nc == 2 : n0 = np . repeat ( arr [:, 0 ], nr ) n1 = np . tile ( arr [:, 1 ], nr ) return np . column_stack (( n0 , n1 )) elif nc == 3 : nr2 = nr * nr n0 = np . repeat ( arr [:, 0 ], nr2 ) n1 = np . repeat ( np . tile ( arr [:, 1 ], nr ), nr ) n2 = np . tile ( arr [:, 2 ], nr2 ) return np . column_stack (( n0 , n1 , n2 )) else : bs_error_abort ( f \"at this stage, the number of columns must be 3 or less, not { nc } ...\" ) return arr # for mypy","title":"make_lexico_grid()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.npexp","text":"\\(C^2\\) extension of \\(\\exp(a)\\) above bigx and below lowx , perhaps with derivatives Parameters: Name Type Description Default arr np . ndarray any Numpy array required bigx float upper bound 50.0 lowx float lower bound -50.0 deriv int if 1, compute derivative, if 2, second derivative 0 verbose bool prints debugging info False Returns: Type Description np . ndarray | TwoArrays | ThreeArrays \\(\\exp(a)\\) \\(C^2\\) -extended above bigx and below lowx , np . ndarray | TwoArrays | ThreeArrays perhaps with derivatives Source code in bs_python_utils/bsnputils.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def npexp ( arr : np . ndarray , bigx : float = 50.0 , lowx : float = - 50.0 , deriv : int = 0 , verbose : bool = False , ) -> np . ndarray | TwoArrays | ThreeArrays : \"\"\" $C^2$ extension of $\\\\exp(a)$ above `bigx` and below `lowx`, perhaps with derivatives Args: arr: any Numpy array bigx: upper bound lowx: lower bound deriv: if 1, compute derivative, if 2, second derivative verbose: prints debugging info Returns: $\\\\exp(a)$ $C^2$-extended above `bigx` and below `lowx`, perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv can only be 0, 1, or 2; not { deriv } \" ) min_arr , max_arr = np . min ( arr ), np . max ( arr ) if max_arr <= bigx and min_arr >= lowx : exparr = np . exp ( arr ) if deriv == 0 : return cast ( np . ndarray , exparr ) elif deriv == 1 : return cast ( TwoArrays , ( exparr , exparr )) # deriv == 2 return cast ( ThreeArrays , ( exparr , exparr , exparr )) else : # some large and/or small arguments exparr = np . exp ( np . maximum ( np . minimum ( arr , bigx ), lowx )) print ( f \" { exparr =} \" ) ebigx = exp ( bigx ) elowx = exp ( lowx ) darrb = arr - bigx darrl = lowx - arr exparr_larger = ebigx * ( 1.0 + darrb * ( 1.0 + 0.5 * darrb )) exparr_smaller = elowx * ( 1.0 - darrl * ( 1.0 - 0.5 * darrl )) if verbose : n_large_args = np . sum ( arr > bigx ) finals = \"s\" if n_large_args > 1 else \"\" print ( f \"npexp: { n_large_args } argument { finals } larger than { bigx } : \\n \" f \"maxi = { np . max ( arr ) } \" ) n_small_args = np . sum ( arr < lowx ) finals = \"s\" if n_small_args > 1 else \"\" print ( f \"npexp: { n_small_args } argument { finals } smaller than { lowx } : \\n \" f \"mini = { np . min ( arr ) } \" ) expval = exparr print ( expval ) expval = np . where ( arr > bigx , exparr_larger , expval ) expval = np . where ( arr < lowx , exparr_smaller , expval ) if deriv == 0 : return cast ( np . ndarray , expval ) dexpval = exparr dexparr_larger = ebigx * ( 1.0 + darrb ) dexparr_smaller = elowx * ( 1.0 - darrl ) dexpval = np . where ( arr > bigx , dexparr_larger , dexpval ) dexpval = np . where ( arr < lowx , dexparr_smaller , dexpval ) if deriv == 1 : return cast ( TwoArrays , ( expval , dexpval )) # deriv == 2 d2expval = exparr return cast ( ThreeArrays , ( expval , dexpval , d2expval ))","title":"npexp()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.nplog","text":"\\(C^2\\) extension of \\(\\ln(a)\\) below eps , perhaps with derivatives Parameters: Name Type Description Default arr np . ndarray any Numpy array required eps float lower bound 1e-30 deriv int if 1, compute derivative, if 2, second derivative 0 verbose bool prints debugging info False Returns: Type Description np . ndarray | TwoArrays | ThreeArrays \\(\\ln(a)\\) \\(C^2\\) -extended below eps , perhaps with derivatives Source code in bs_python_utils/bsnputils.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def nplog ( arr : np . ndarray , eps : float = 1e-30 , deriv : int = 0 , verbose : bool = False , ) -> np . ndarray | TwoArrays | ThreeArrays : \"\"\" $C^2$ extension of $\\\\ln(a)$ below `eps`, perhaps with derivatives Args: arr: any Numpy array eps: lower bound deriv: if 1, compute derivative, if 2, second derivative verbose: prints debugging info Returns: $\\\\ln(a)$ $C^2$-extended below `eps`, perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv can only be 0, 1, or 2; not { deriv } \" ) if np . min ( arr ) > eps : if deriv == 0 : return cast ( np . ndarray , np . log ( arr )) elif deriv == 1 : return cast ( TwoArrays , ( np . log ( arr ), 1.0 / arr )) # deriv == 2 return cast ( ThreeArrays , ( np . log ( arr ), 1.0 / arr , - 1.0 / ( arr * arr ))) else : logarreps = np . log ( np . maximum ( arr , eps )) darr = 1.0 - arr / eps logarr_smaller = log ( eps ) - darr * ( 1.0 + darr / 2.0 ) if verbose : n_small_args = np . sum ( arr < eps ) if n_small_args > 0 : finals = \"s\" if n_small_args > 1 else \"\" print ( f \"nplog: { n_small_args } argument { finals } smaller than { eps } : mini =\" f \" { np . min ( arr ) } \" ) logeps = np . where ( arr > eps , logarreps , logarr_smaller ) if deriv == 0 : return logeps arreps = np . maximum ( arr , eps ) der_logarreps = 1.0 / arreps der_logarr_smaller = ( 1.0 + darr ) / eps dlogeps = np . where ( arr > eps , der_logarreps , der_logarr_smaller ) if deriv == 1 : return cast ( TwoArrays , ( logeps , dlogeps )) # deriv == 2 der2_logarreps = - 1.0 / ( arreps * arreps ) der2_logarr_smaller = np . full ( arr . shape , - 1.0 / ( eps * eps )) d2logeps = np . where ( arr > eps , der2_logarreps , der2_logarr_smaller ) return cast ( ThreeArrays , ( logeps , dlogeps , d2logeps ))","title":"nplog()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.npmaxabs","text":"maximum absolute value in an array Parameters: Name Type Description Default arr np . ndarray any Numpy array required Returns: Type Description float the largest element in absolute value Source code in bs_python_utils/bsnputils.py 290 291 292 293 294 295 296 297 298 299 300 def npmaxabs ( arr : np . ndarray ) -> float : \"\"\" maximum absolute value in an array Args: arr: any Numpy array Returns: the largest element in absolute value \"\"\" return cast ( float , np . max ( np . abs ( arr )))","title":"npmaxabs()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.nppad2_end_zeros","text":"pad the ends of a 2-dim array with zeros to increase its size to (m,n) , if needed Parameters: Name Type Description Default mat np . ndarray 2-dim array required m int number of rows requested required n int number of columns requested required Returns: Type Description np . ndarray padded array, where needed Source code in bs_python_utils/bsnputils.py 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 def nppad2_end_zeros ( mat : np . ndarray , m : int , n : int ) -> np . ndarray : \"\"\" pad the ends of a 2-dim array with zeros to increase its size to `(m,n)`, if needed Args: mat: 2-dim array m: number of rows requested n: number of columns requested Returns: padded array, where needed \"\"\" nrows , ncols = check_matrix ( mat ) max_rows = max ( m , nrows ) max_cols = max ( n , ncols ) if nrows < max_rows and ncols < max_cols : # pad both dimensions pmat = np . zeros (( m , n )) pmat [: nrows , : ncols ] = mat return pmat elif nrows < max_rows : # pad rows pmat = np . zeros (( m , ncols )) pmat [: nrows , :] = mat return pmat elif ncols < max_cols : # pad columns pmat = np . zeros (( nrows , n )) pmat [:, : ncols ] = mat return pmat else : # no need for padding return mat","title":"nppad2_end_zeros()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.nppad_beg_zeros","text":"pad the beginning of a 1-dim array with zeros to increase its size to n , if needed Parameters: Name Type Description Default v np . ndarray 1-dim array of size (nv) required n int size requested required Returns: Type Description np . ndarray padded array if nv < n , otherwise v Source code in bs_python_utils/bsnputils.py 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 def nppad_beg_zeros ( v : np . ndarray , n : int ) -> np . ndarray : \"\"\" pad the beginning of a 1-dim array with zeros to increase its size to `n`, if needed Args: v: 1-dim array of size `(nv)` n: size requested Returns: padded array if `nv` < `n`, otherwise `v` \"\"\" nv = check_vector ( v ) if nv < n : return np . pad ( v , ( n - nv , 0 )) else : return v","title":"nppad_beg_zeros()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.nppad_end_zeros","text":"pad the end of a 1-dim array with zeros to increase its size to n , if needed Parameters: Name Type Description Default v np . ndarray 1-dim array of size (nv) required n int size requested required Returns: Type Description np . ndarray padded array if nv < n , else v Source code in bs_python_utils/bsnputils.py 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 def nppad_end_zeros ( v : np . ndarray , n : int ) -> np . ndarray : \"\"\" pad the end of a 1-dim array with zeros to increase its size to `n`, if needed Args: v: 1-dim array of size `(nv)` n: size requested Returns: padded array if `nv` < `n`, else `v` \"\"\" nv = check_vector ( v ) if nv < n : return np . pad ( v , ( 0 , n - nv )) else : return v","title":"nppad_end_zeros()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.nppow","text":"evaluates a**b element-by-element, perhaps with derivatives Parameters: Name Type Description Default a np . ndarray an array required b int | float | np . ndarray if an array, should have the same shape as a required deriv int if 1, compute derivative, if 2, second derivative 0 Returns: Type Description np . ndarray | ThreeArrays | SixArrays an array of the same shape as a Source code in bs_python_utils/bsnputils.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def nppow ( a : np . ndarray , b : int | float | np . ndarray , deriv : int = 0 ) -> np . ndarray | ThreeArrays | SixArrays : \"\"\" evaluates a**b element-by-element, perhaps with derivatives Args: a: an array b: if an array, should have the same shape as `a` deriv: if 1, compute derivative, if 2, second derivative Returns: an array of the same shape as `a` \"\"\" if isinstance ( b , float ): mina = np . min ( a ) if mina < 0.0 : bs_error_abort ( \"All elements of a must be positive!\" ) if isinstance ( b , ( int , float )): a_pow_b = a ** b if deriv == 0 : return a_pow_b log_a = np . log ( a ) derivs1 = ( b * a_pow_b / a , a_pow_b * log_a ) if deriv == 1 : return cast ( ThreeArrays , ( a_pow_b , * derivs1 )) b1 = b - 1.0 a_pow_b1 = a_pow_b / a # deriv == 2 derivs2 = ( b * b1 * a_pow_b1 / a , a_pow_b1 * ( 1.0 + b * log_a ), a_pow_b * log_a * log_a , ) return cast ( SixArrays , ( a_pow_b , * derivs1 , * derivs2 )) else : if a . shape != b . shape : bs_error_abort ( \"b is not a number or an array of the same shape as a!\" ) return _nppow_arrays ( a , b , deriv )","title":"nppow()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.nprepeat_col","text":"create a matrix with n columns equal to v Parameters: Name Type Description Default v np . ndarray a 1-dim array of size m required n int the number of columns requested required Returns: Type Description np . ndarray a 2-dim array of shape (m, n) Source code in bs_python_utils/bsnputils.py 262 263 264 265 266 267 268 269 270 271 272 273 def nprepeat_col ( v : np . ndarray , n : int ) -> np . ndarray : \"\"\" create a matrix with `n` columns equal to `v` Args: v: a 1-dim array of size `m` n: the number of columns requested Returns: a 2-dim array of shape `(m, n)` \"\"\" return np . repeat ( v [:, np . newaxis ], n , axis = 1 )","title":"nprepeat_col()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.nprepeat_row","text":"create a matrix with m rows equal to v Parameters: Name Type Description Default v np . ndarray a 1-dim array of size n required m int the number of rows requested required Returns: Type Description np . ndarray a 2-dim array of shape (m, n) Source code in bs_python_utils/bsnputils.py 276 277 278 279 280 281 282 283 284 285 286 287 def nprepeat_row ( v : np . ndarray , m : int ) -> np . ndarray : \"\"\" create a matrix with `m` rows equal to `v` Args: v: a 1-dim array of size `n` m: the number of rows requested Returns: a 2-dim array of shape `(m, n)` \"\"\" return np . repeat ( v [ np . newaxis , :], m , axis = 0 )","title":"nprepeat_row()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.npxlogx","text":"\\(C^2\\) extension of \\(a\\ln(a)\\) below eps , perhaps with derivatives Parameters: Name Type Description Default arr np . ndarray a Numpy array required eps float lower bound 1e-30 deriv int if 1, compute derivative, if 2, second derivative 0 verbose bool prints debugging info False Returns: Type Description np . ndarray | TwoArrays | ThreeArrays \\(a\\ln(a)\\) \\(C^2\\) -extended below eps , perhaps with derivatives Source code in bs_python_utils/bsnputils.py 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 def npxlogx ( arr : np . ndarray , eps : float = 1e-30 , deriv : int = 0 , verbose : bool = False , ) -> np . ndarray | TwoArrays | ThreeArrays : \"\"\" $C^2$ extension of $a\\\\ln(a)$ below `eps`, perhaps with derivatives Args: arr: a Numpy array eps: lower bound deriv: if 1, compute derivative, if 2, second derivative verbose: prints debugging info Returns: $a\\\\ln(a)$ $C^2$-extended below `eps`, perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv must be 0, 1, or 2; not { deriv } \" ) if np . min ( arr ) > eps : return cast ( np . ndarray , arr * np . log ( arr )) else : logeps = log ( eps ) logarreps = np . log ( np . maximum ( arr , eps )) xlogarreps = arr * logarreps xlogarr_smaller = arr * ( arr / eps + logeps - 1.0 ) if verbose : n_small_args = np . sum ( arr < eps ) if n_small_args > 0 : finals = \"s\" if n_small_args > 1 else \"\" print ( f \"npxlogx: { n_small_args } argument { finals } smaller than { eps } : mini\" f \" = { np . min ( arr ) } \" ) xlogval = np . where ( arr > eps , xlogarreps , xlogarr_smaller ) if deriv == 0 : return xlogval dxlogarreps = 1.0 + logarreps dxlogarr_smaller = logeps + arr / eps dxlogval = np . where ( arr > eps , dxlogarreps , dxlogarr_smaller ) if deriv == 1 : return cast ( TwoArrays , ( xlogval , dxlogval )) # deriv == 2 d2xlogval = 1.0 / np . maximum ( arr , eps ) return cast ( ThreeArrays , ( xlogval , dxlogval , d2xlogval ))","title":"npxlogx()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.outer_bivar","text":"make a BivariatePolynomial from the product of two Polynomial objects Parameters: Name Type Description Default pol1 Polynomial Polynomial in the first variable required pol2 Polynomial Polynomial in the second variable required Returns: Type Description BivariatePolynomial a BivariatePolynomial = pol1 * pol2 Source code in bs_python_utils/bsnputils.py 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 def outer_bivar ( pol1 : Polynomial , pol2 : Polynomial ) -> BivariatePolynomial : \"\"\" make a `BivariatePolynomial` from the product of two `Polynomial` objects Args: pol1: Polynomial in the first variable pol2: Polynomial in the second variable Returns: a `BivariatePolynomial` = `pol1 * pol2` \"\"\" p1 = pol1 . coef p2 = pol2 . coef prod_coef = np . outer ( p1 , p2 ) return BivariatePolynomial ( prod_coef )","title":"outer_bivar()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.print_quantiles","text":"print these quantiles of the array(s) Parameters: Name Type Description Default v np . ndarray | Iterable [ np . ndarray ] a vector or an iterable of vectors required quantiles np . ndarray quantiles in [0,1] required Returns: Type Description np . ndarray the corresponding quantiles as a vector or a matrix Source code in bs_python_utils/bsnputils.py 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 def print_quantiles ( v : np . ndarray | Iterable [ np . ndarray ], quantiles : np . ndarray ) -> np . ndarray : \"\"\"print these quantiles of the array(s) Args: v: a vector or an iterable of vectors quantiles: quantiles in [0,1] Returns: the corresponding quantiles as a vector or a matrix \"\"\" nq = check_vector ( quantiles ) if isinstance ( v , np . ndarray ): qvals = np . quantile ( v , quantiles ) for q , qv in zip ( quantiles , qvals , strict = True ): print ( f \"Quantile { q : .3f } : { qv : >10.3f } \" ) elif isinstance ( v , Iterable ): v = list ( v ) for v_i in v : _ = check_vector ( v_i ) nv = len ( v ) qvals = np . zeros (( nq , nv )) for i in range ( nv ): qvals [:, i ] = np . quantile ( v [ i ], quantiles ) for iq , q in enumerate ( quantiles ): s = f \"Quantile { q : .3f } : \" qv = qvals [ iq , :] for i in range ( nv ): s += f \" { qv [ i ] : >10.3f } \" print ( f \" { s } \" ) else : bs_error_abort ( \"v must be a vector or a list of vectors\" ) return cast ( np . ndarray , qvals )","title":"print_quantiles()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.quantile_transform","text":"transform a vector of counts into the corresponding quantiles Parameters: Name Type Description Default v np . ndarray a vector of counts required Returns: Type Description np . ndarray the corresponding quantiles Source code in bs_python_utils/bsnputils.py 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 def quantile_transform ( v : np . ndarray ) -> np . ndarray : \"\"\"transform a vector of counts into the corresponding quantiles Args: v: a vector of counts Returns: the corresponding quantiles \"\"\" n = check_vector ( v ) q = np . zeros ( n ) for i in range ( n ): q [ i ] = np . sum ( v <= v [ i ]) / ( n + 1 ) return q","title":"quantile_transform()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.rice_stderr","text":"computes the Rice local estimators of the standard error of y | x Parameters: Name Type Description Default y np . ndarray vector of y-values required x np . ndarray vector of x-values required is_sorted bool set it to True if x is in increasing order False Returns: Type Description np . ndarray | float an array of the same size with the stderr(y | x) Source code in bs_python_utils/bsnputils.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def rice_stderr ( y : np . ndarray , x : np . ndarray , is_sorted : bool = False ) -> np . ndarray | float : \"\"\" computes the Rice local estimators of the standard error of y | x Args: y: vector of y-values x: vector of x-values is_sorted: set it to `True` if `x` is in increasing order Returns: an array of the same size with the stderr(y | x) \"\"\" n = check_vector ( x ) ny = check_vector ( y ) if ny != n : bs_error_abort ( \"x and y should have the same size\" ) if not is_sorted : # need to sort by increasing value of x order_x = np . argsort ( x ) ys = y [ order_x ] else : ys = y variance_estimator = np . zeros ( n ) # we average over neighbors n_neighbors = int ( sqrt ( float ( n )) / 2.0 ) facd = 1.0 / ( 2.0 * n_neighbors ) n_neighbors2 = n_neighbors // 2 # for the first observations yleft = ys [: n_neighbors2 ] dy = yleft [ 1 :] - yleft [: - 1 ] variance_estimator [: n_neighbors2 ] = np . sum ( dy * dy ) * facd # for the middle of the sample minus_nn2 = n - n_neighbors2 for ix in range ( n_neighbors2 , minus_nn2 ): ix_neighbors = slice ( ix - n_neighbors2 , ix + n_neighbors2 ) yx = ys [ ix_neighbors ] dy = yx [ 1 :] - yx [: - 1 ] variance_estimator [ ix ] = np . sum ( dy * dy ) * facd # and for the last observations yright = ys [ minus_nn2 :] dy = yright [ 1 :] - yright [: - 1 ] variance_estimator [ minus_nn2 :] = np . sum ( dy * dy ) * facd stderr_estimator = np . sqrt ( variance_estimator ) return stderr_estimator","title":"rice_stderr()"},{"location":"bsnputils.html#bs_python_utils.bsnputils.set_elements_abovebelow_diagonal","text":"Sets all elements of the given matrix above or below the diagonal to the specified scalar value. Parameters: Name Type Description Default matrix np . ndarray the input matrix; it must be square required scalar int | float The scalar value to set the elements above or below the diagonal. required location str 'above', 'below', 'on_above', 'on_below'. required Returns: Type Description np . ndarray The updated matrix with elements above or below the diagonal set to the scalar value, np . ndarray including the diagonal for the on_ options. Source code in bs_python_utils/bsnputils.py 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 def set_elements_abovebelow_diagonal ( matrix : np . ndarray , scalar : int | float , location : str ) -> np . ndarray : \"\"\" Sets all elements of the given matrix above or below the diagonal to the specified scalar value. Args: matrix: the input matrix; it must be square scalar: The scalar value to set the elements above or below the diagonal. location: 'above', 'below', 'on_above', 'on_below'. Returns: The updated matrix with elements above or below the diagonal set to the scalar value, including the diagonal for the `on_` options. \"\"\" _ = check_square ( matrix , \"set_elements_abovebelow_diagonal\" ) # copy the matrix new_matrix = matrix . copy () # Get the indices of elements above or below the diagonal if location == \"above\" : row_indices , col_indices = np . triu_indices_from ( new_matrix , k = 1 ) elif location == \"below\" : row_indices , col_indices = np . tril_indices_from ( new_matrix , k =- 1 ) elif location == \"on_above\" : row_indices , col_indices = np . triu_indices_from ( new_matrix , k = 0 ) elif location == \"on_below\" : row_indices , col_indices = np . tril_indices_from ( new_matrix , k = 0 ) else : bs_error_abort ( f \"\"\" location can only be 'above', 'below', 'on_above' or 'on_below', not { location } \"\"\" ) # Set the elements above or below the diagonal to the scalar value new_matrix [ row_indices , col_indices ] = scalar return new_matrix","title":"set_elements_abovebelow_diagonal()"},{"location":"bssputils.html","text":"bssputils module \u00b6 Contains scipy utility programs: describe_array : report descriptive statistics on a vectorized array spline_reg : spline interpolation in one dimension. describe_array ( v , name = 'v' ) \u00b6 descriptive statistics on an array interpreted as a vector Parameters: Name Type Description Default v np . ndarray the array required name str | None its name 'v' Returns: Type Description Any the scipy.stats.describe object Source code in bs_python_utils/bssputils.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def describe_array ( v : np . ndarray , name : str | None = \"v\" ) -> Any : \"\"\" descriptive statistics on an array interpreted as a vector Args: v: the array name: its name Returns: the `scipy.stats.describe` object \"\"\" print_stars ( f \" { name } has:\" ) d = sts . describe ( v , None ) print ( f \"Number of elements: { d . nobs } \" ) print ( f \"Minimum: { d . minmax [ 0 ] } \" ) print ( f \"Maximum: { d . minmax [ 1 ] } \" ) print ( f \"Mean: { d . mean } \" ) print ( f \"Stderr: { sqrt ( d . variance ) } \" ) return d spline_reg ( y , x , x_new = None , is_sorted = False , smooth = True ) \u00b6 one-dimensional spline interpolation of vector y on vector x Parameters: Name Type Description Default y np . ndarray vector of y-values required x np . ndarray vector of x-values required x_new np . ndarray | None where we evaluate (at the points in x by default) None is_sorted bool | None True if x is sorted in increasing order False smooth bool | None True if we want a smoother; otherwise we go through all points provided True Returns: Type Description np . ndarray values interpolated at x_new . Source code in bs_python_utils/bssputils.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def spline_reg ( y : np . ndarray , x : np . ndarray , x_new : np . ndarray | None = None , is_sorted : bool | None = False , smooth : bool | None = True , ) -> np . ndarray : \"\"\" one-dimensional spline interpolation of vector `y` on vector `x` Args: y: vector of y-values x: vector of x-values x_new: where we evaluate (at the points in `x` by default) is_sorted: True if `x` is sorted in increasing order smooth: True if we want a smoother; otherwise we go through all points provided Returns: values interpolated at `x_new`. \"\"\" n = check_vector ( x ) ny = check_vector ( y ) if ny != n : bs_error_abort ( \"x and y should have the same size\" ) if not is_sorted : # need to sort by increasing value of x order_rhs = np . argsort ( x ) rhs = x [ order_rhs ] lhs = y [ order_rhs ] else : rhs , lhs = x , y if smooth : # we compute a local estimator of the stderr of (y | x) and we use it to enter weights sigyx = rice_stderr ( lhs , rhs ) w = 1 / sigyx spl = UnivariateSpline ( rhs , lhs , w = w ) else : spl = UnivariateSpline ( rhs , lhs ) xeval = x if x_new is None else x_new y_pred = spl ( xeval ) return cast ( np . ndarray , y_pred )","title":"Scipy"},{"location":"bssputils.html#bssputils-module","text":"Contains scipy utility programs: describe_array : report descriptive statistics on a vectorized array spline_reg : spline interpolation in one dimension.","title":"bssputils module"},{"location":"bssputils.html#bs_python_utils.bssputils.describe_array","text":"descriptive statistics on an array interpreted as a vector Parameters: Name Type Description Default v np . ndarray the array required name str | None its name 'v' Returns: Type Description Any the scipy.stats.describe object Source code in bs_python_utils/bssputils.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def describe_array ( v : np . ndarray , name : str | None = \"v\" ) -> Any : \"\"\" descriptive statistics on an array interpreted as a vector Args: v: the array name: its name Returns: the `scipy.stats.describe` object \"\"\" print_stars ( f \" { name } has:\" ) d = sts . describe ( v , None ) print ( f \"Number of elements: { d . nobs } \" ) print ( f \"Minimum: { d . minmax [ 0 ] } \" ) print ( f \"Maximum: { d . minmax [ 1 ] } \" ) print ( f \"Mean: { d . mean } \" ) print ( f \"Stderr: { sqrt ( d . variance ) } \" ) return d","title":"describe_array()"},{"location":"bssputils.html#bs_python_utils.bssputils.spline_reg","text":"one-dimensional spline interpolation of vector y on vector x Parameters: Name Type Description Default y np . ndarray vector of y-values required x np . ndarray vector of x-values required x_new np . ndarray | None where we evaluate (at the points in x by default) None is_sorted bool | None True if x is sorted in increasing order False smooth bool | None True if we want a smoother; otherwise we go through all points provided True Returns: Type Description np . ndarray values interpolated at x_new . Source code in bs_python_utils/bssputils.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def spline_reg ( y : np . ndarray , x : np . ndarray , x_new : np . ndarray | None = None , is_sorted : bool | None = False , smooth : bool | None = True , ) -> np . ndarray : \"\"\" one-dimensional spline interpolation of vector `y` on vector `x` Args: y: vector of y-values x: vector of x-values x_new: where we evaluate (at the points in `x` by default) is_sorted: True if `x` is sorted in increasing order smooth: True if we want a smoother; otherwise we go through all points provided Returns: values interpolated at `x_new`. \"\"\" n = check_vector ( x ) ny = check_vector ( y ) if ny != n : bs_error_abort ( \"x and y should have the same size\" ) if not is_sorted : # need to sort by increasing value of x order_rhs = np . argsort ( x ) rhs = x [ order_rhs ] lhs = y [ order_rhs ] else : rhs , lhs = x , y if smooth : # we compute a local estimator of the stderr of (y | x) and we use it to enter weights sigyx = rice_stderr ( lhs , rhs ) w = 1 / sigyx spl = UnivariateSpline ( rhs , lhs , w = w ) else : spl = UnivariateSpline ( rhs , lhs ) xeval = x if x_new is None else x_new y_pred = spl ( xeval ) return cast ( np . ndarray , y_pred )","title":"spline_reg()"},{"location":"bsstats.html","text":"bsstats module \u00b6 Contains some statistical routines. TslsResults dataclass \u00b6 contains the full results of a TSLS regression Source code in bs_python_utils/bsstats.py 23 24 25 26 27 28 29 30 31 32 33 34 @dataclass class TslsResults : \"\"\"contains the full results of a TSLS regression\"\"\" iv_estimates : float | np . ndarray | None r2_first_iv : float | np . ndarray | None r2_y : float | None r2_second : float | np . ndarray | None y_proj : float | np . ndarray | None y_coeffs : float | np . ndarray | None X_IV_proj : float | np . ndarray | None b_proj_IV : float | np . ndarray | None bs_multivariate_normal_pdf ( values_x , means_x , cov_mat ) \u00b6 Multivariate (or univariate) normal probability density function at values_x Parameters: Name Type Description Default values_x np . ndarray values at which to evaluate the pdf, an n -vector or an (n, nvars) matrix required means_x float | np . ndarray means of the multivariate normal, a float or an (nvars) vector required cov_mat float | np . ndarray covariance matrix of the multivariate normal, a float or an (nvars, nvars) matrix required Returns: Type Description np . ndarray the values of the density at values_x Source code in bs_python_utils/bsstats.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def bs_multivariate_normal_pdf ( values_x : np . ndarray , means_x : float | np . ndarray , cov_mat : float | np . ndarray ) -> np . ndarray : \"\"\"Multivariate (or univariate) normal probability density function at values_x Args: values_x: values at which to evaluate the pdf, an `n`-vector or an `(n, nvars)` matrix means_x: means of the multivariate normal, a float or an `(nvars)` vector cov_mat: covariance matrix of the multivariate normal, a float or an `(nvars, nvars)` matrix Returns: the values of the density at `values_x` \"\"\" ndims_values = check_vector_or_matrix ( values_x , \"bs_multivariate_normal_pdf\" ) if ndims_values == 1 : # we are evaluating a univariate normal # if not type(means_x) == float: # bs_error_abort(f\"means_x should be a float as values_x is a vector\") # if not type(cov_mat) == float: # bs_error_abort(f\"cov_mat should be a float as values_x is a vector\") sigma2 = cov_mat resid = values_x - means_x dval = np . exp ( - 0.5 * resid * resid / sigma2 ) / np . sqrt ( 2 * np . pi * sigma2 ) return cast ( np . ndarray , dval ) else : # we are evaluating a multivariate normal n , nvars = values_x . shape n_means = check_vector ( means_x , \"bs_multivariate_normal_pdf\" ) if n_means != nvars : bs_error_abort ( f \"means_x should be a vector of size { nvars } not { n_means } \" ) nrows , ncols = check_matrix ( cov_mat , \"bs_multivariate_normal_pdf\" ) if nrows != ncols or nrows != nvars : bs_error_abort ( f \"cov_mat should be a matrix ( { nvars } , { nvars } ) not ( { nrows } , { ncols } )\" ) resid = values_x - means_x argresid = spla . solve ( cov_mat , resid . T ) argexp = np . zeros ( n ) for i in range ( n ): argexp [ i ] = np . dot ( resid [ i , :], argresid [:, i ]) dval = np . exp ( - 0.5 * argexp ) / np . sqrt ( (( 2 * np . pi ) ** nvars ) * spla . det ( cov_mat ) ) return cast ( np . ndarray , dval ) emcee_draw ( n_samples , log_pdf , p0 , params = None , n_burn_in = 100 , seed = 8754 ) \u00b6 uses MCMC to draw n_samples samples from the log pdf with given parameters Parameters: Name Type Description Default n_samples int the number of samples to draw required log_pdf Callable [[ np . ndarray , list ], float ] the log of the pdf, log_pdf(x, *params) ; retuns a float from the array for one sample required p0 np . ndarray the initial values for the walkers required params list | None a list of parameters None n_burn_in int | None the number of burn-in iterations for MCMC 100 seed int | None to randomly draw the samples from the walkers 8754 Returns: Type Description np . ndarray an (n_samples, n_dims) array of samples. Warning The log_pdf function should return minus infinity outside of the support of the pdf, and p0 should be contained in that support. Source code in bs_python_utils/bsstats.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 def emcee_draw ( n_samples : int , log_pdf : Callable [[ np . ndarray , list ], float ], p0 : np . ndarray , params : list | None = None , n_burn_in : int | None = 100 , seed : int | None = 8754 , ) -> np . ndarray : \"\"\"uses MCMC to draw `n_samples` samples from the log pdf with given parameters Args: n_samples: the number of samples to draw log_pdf: the log of the pdf, `log_pdf(x, *params)`; retuns a float from the array for one sample p0: the initial values for the walkers params: a list of parameters n_burn_in: the number of burn-in iterations for MCMC seed: to randomly draw the samples from the walkers Returns: an `(n_samples, n_dims)` array of samples. Warning: The `log_pdf` function should return minus infinity outside of the support of the pdf, and `p0` should be contained in that support. \"\"\" n_walkers , n_dims = p0 . shape # burn in sampler = EnsembleSampler ( n_walkers , n_dims , log_pdf , args = params ) state = sampler . run_mcmc ( p0 , n_burn_in ) sampler . reset () # generate the samples sampler . run_mcmc ( state , n_samples ) samples = sampler . get_chain ( flat = True ) rng = np . random . default_rng ( seed ) samples = rng . choice ( samples , size = n_samples , replace = False ) return cast ( np . ndarray , samples ) estimate_pdf ( x_obs , x_points , MIN_SIZE_NONPAR = 200 , weights = None ) \u00b6 return an estimate of the conditional densities of x at points values_x (Silverman rule) Parameters: Name Type Description Default x_obs np . ndarray an n -vector or an (n, nvars) matrix of the observed values of x required x_points np . ndarray an m -vector or an (m, nvars) matrix of x values required MIN_SIZE_NONPAR int minimum size above which we use kernel density estimators 200 weights np . ndarray | None an n -vector of weights for the observations, if present None Returns: Type Description np . ndarray the density estimates at values_x Source code in bs_python_utils/bsstats.py 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 def estimate_pdf ( x_obs : np . ndarray , x_points : np . ndarray , MIN_SIZE_NONPAR : int = 200 , weights : np . ndarray | None = None , ) -> np . ndarray : \"\"\"return an estimate of the conditional densities of `x` at points `values_x` (Silverman rule) Args: x_obs: an `n`-vector or an `(n, nvars)` matrix of the observed values of `x` x_points: an `m`-vector or an `(m, nvars)` matrix of x values MIN_SIZE_NONPAR: minimum size above which we use kernel density estimators weights: an `n`-vector of weights for the observations, if present Returns: the density estimates at `values_x` \"\"\" ndims_x = check_vector_or_matrix ( x_obs , \"estimate_pdf\" ) ndims_valx = check_vector_or_matrix ( x_points , \"estimate_pdf\" ) if ndims_x == 1 : n_obs = x_obs . size if ndims_valx != 1 : bs_error_abort ( f \"x_points should have one dimension, not { ndims_valx } \" ) xt_obs = x_obs . reshape (( - 1 , 1 )) nvars = 1 xt_points = x_points . reshape (( - 1 , 1 )) else : # ndims_x == 2 n_obs , nvars = x_obs . shape if ndims_valx == 1 : # only one x point with nv elements nv = x_points . size else : # several x points with nv elements nv = x_points . shape [ 1 ] if nv != nvars : bs_error_abort ( f \"x_points should have { nvars } variables, not { nv } \" ) xt_obs = x_obs xt_points = x_points if weights is not None : n_w = check_vector ( weights , \"estimate_pdf\" ) if n_w != n_obs : bs_error_abort ( f \"if weights is given, it should be a vector of size { n_obs } not { n_w } \" ) min_size_np = MIN_SIZE_NONPAR ** (( 4.0 + nvars ) / 5.0 ) if n_obs > min_size_np : # cell large enough to use nonparametrics # fit joint density of x kde = sts . gaussian_kde ( xt_obs . T , bw_method = \"silverman\" , weights = weights ) # density of x at values_x f_x = kde . evaluate ( xt_points . T ) else : # sample too small, we fit a normal if ndims_x == 1 : # univariate mean_x = np . mean ( x_obs ) var_x = np . var ( x_obs ) f_x = bs_multivariate_normal_pdf ( x_points , mean_x , var_x ) else : # multivariate means_x = np . mean ( x_obs , 0 ) cov_mat = np . cov ( x_obs . T ) f_x = bs_multivariate_normal_pdf ( x_points , means_x , cov_mat ) if weights is not None : f_x *= weights / np . mean ( weights ) return cast ( np . ndarray , f_x ) flexible_reg ( Y , X , mode = 'NP' , var_types = None , n_sub = None , n_res = 1 , verbose = False ) \u00b6 flexible regression of Y on X Parameters: Name Type Description Default Y np . ndarray independent variable (nobs) or (nobs, ny) required X np . ndarray covariates (nobs) or (nobs, nx) ; should not include a constant term required mode str what flexible means * 'NP': non parametric * 'SPL': spline regression, only on one covariate * '1': linear * '2': quadratic 'NP' var_types str | None [for 'NP' only] specify types of all X variables if not all of them are continuous; one character per variable * 'c' for continuous * 'u' discrete unordered * 'o' discrete ordered None n_sub int | None [for 'NP' only] size of subsample for cross-validation; by default it is 200^{(m+4)/5} None n_res int [for 'NP' only] how many subsamples we draw; 1 if None 1 verbose bool prints stuff if True False Returns: Type Description np . ndarray E(y|X) at the sample points Source code in bs_python_utils/bsstats.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 def flexible_reg ( Y : np . ndarray , X : np . ndarray , mode : str = \"NP\" , var_types : str | None = None , n_sub : int | None = None , n_res : int = 1 , verbose : bool = False , ) -> np . ndarray : \"\"\"flexible regression of `Y` on `X` Args: Y: independent variable `(nobs)` or `(nobs, ny)` X: covariates `(nobs)` or `(nobs, nx)`; should **not** include a constant term mode: what flexible means * 'NP': non parametric * 'SPL': spline regression, only on one covariate * '1': linear * '2': quadratic var_types: [for 'NP' only] specify types of all `X` variables if not all of them are continuous; one character per variable * 'c' for continuous * 'u' discrete unordered * 'o' discrete ordered n_sub: [for 'NP' only] size of subsample for cross-validation; \\ by default it is `200^{(m+4)/5}` n_res: [for 'NP' only] how many subsamples we draw; 1 if `None` verbose: prints stuff if True Returns: `E(y|X)` at the sample points \"\"\" if mode == \"NP\" : if Y . ndim == 2 : ny = Y . shape [ 1 ] Y_fit = np . zeros_like ( Y ) for iy in range ( ny ): Y_fit [:, iy ] = reg_nonpar_fit ( Y [:, iy ], X , var_types = var_types , n_sub = n_sub , n_res = n_res , verbose = verbose , ) return Y_fit else : return reg_nonpar_fit ( Y , X , var_types = var_types , n_sub = n_sub , n_res = n_res , verbose = verbose ) elif mode == \"SPL\" : if X . ndim > 1 : bs_error_abort ( \"with a spline, only works in one dimension\" ) return spline_reg ( Y , X ) else : try : imode = int ( mode ) except TypeError : bs_error_abort ( f \"does not accept mode= { mode } \" ) preg , _ , _ = proj_Z ( Y , X , p = imode , verbose = verbose ) return preg kde_resample ( data , n_samples , n_bw = 10 ) \u00b6 Fit a nonparametric density to data by KDE, with cross-validation with starting point at rule of thumb, and generate samples from the estimated density. Parameters: Name Type Description Default data np . ndarray an (n_obs, n_dims) matrix of data required n_samples int how many iid draws we want required n_bw int how mamy bandwidths we try from 1/10th to 10 times the rule-of-thumb 10 Returns: Type Description tuple [ np . ndarray , float ] an (n_samples, n_dims) matrix of draws, and the bandwidth used. Source code in bs_python_utils/bsstats.py 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 def kde_resample ( data : np . ndarray , n_samples : int , n_bw : int = 10 ) -> tuple [ np . ndarray , float ]: \"\"\"Fit a nonparametric density to data by KDE, with cross-validation with starting point at rule of thumb, and generate samples from the estimated density. Args: data: an `(n_obs, n_dims)` matrix of data n_samples: how many iid draws we want n_bw: how mamy bandwidths we try from 1/10th to 10 times the rule-of-thumb Returns: an `(n_samples, n_dims)` matrix of draws, and the bandwidth used. \"\"\" n_obs , n_dims = check_matrix ( data ) h_rot = np . std ( data ) * ( n_obs ** ( - 1 / ( n_dims + 4 ))) params = { \"bandwidth\" : h_rot * np . logspace ( - 1 , 1 , n_bw )} grid = GridSearchCV ( KernelDensity (), params ) grid . fit ( data ) kde = grid . best_estimator_ best_bandwidth = grid . best_params_ [ \"bandwidth\" ] resampled_data = kde . sample ( n_samples ) return resampled_data , best_bandwidth proj_Z ( W , Z , p = 1 , verbose = False ) \u00b6 project W on all interactions of degree p or less of Z Parameters: Name Type Description Default W np . ndarray variable(s) (nobs) or (nobs, nw) required Z np . ndarray instruments (nobs) or (nobs, nz)`; they should not include a constant term required p int maximum total degree for interactions of the columns of Z 1 verbose bool prints stuff if True False Returns: Type Description tuple [ np . ndarray , np . ndarray , float ] the projections of the columns of W on Z etc, the coefficients, and the R^2 of each column Source code in bs_python_utils/bsstats.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def proj_Z ( W : np . ndarray , Z : np . ndarray , p : int = 1 , verbose : bool = False ) -> tuple [ np . ndarray , np . ndarray , float ]: \"\"\"project `W` on all interactions of degree `p` or less of `Z` Args: W: variable(s) `(nobs)` or `(nobs, nw)` Z: instruments `(nobs) or `(nobs, nz)`; they should **not** include a constant term p: maximum total degree for interactions of the columns of `Z` verbose: prints stuff if True Returns: the projections of the columns of `W` on `Z` etc, the coefficients, and the `R^2` of each column \"\"\" nobs = Z . shape [ 0 ] if W . shape [ 0 ] != nobs : bs_error_abort ( \"W and Z should have the same number of rows\" ) if W . ndim > 2 : bs_error_abort ( \"W should have 1 or 2 dimensions\" ) if Z . ndim > 2 : bs_error_abort ( \"Z should have 1 or 2 dimensions\" ) if Z . ndim == 1 : Zp = np . zeros (( nobs , 1 + p )) Zp [:, 0 ] = np . ones ( nobs ) for q in range ( 1 , p + 1 ): Zp [:, q ] = Z ** q else : # Z is a matrix Zp , k = _make_Zp ( Z , p ) if verbose : print ( f \"_proj_Z with degree { p } , using { k } regressors\" ) return _final_proj ( Zp , W ) reg_nonpar ( y , X , var_types = None , n_sub = None , n_res = 1 ) \u00b6 nonparametric regression of y on the columns of X; the bandwidth is chosen on a subsample of size nsub if nsub < nobs , and rescaled. Parameters: Name Type Description Default y np . ndarray a vector of size nobs required X np . ndarray a (nobs) vector or a matrix of shape (nobs, m) required var_types str | None specify types of all X variables if not all of them are continuous; one character per variable 'c' for continuous 'u' discrete unordered 'o' discrete ordered. None n_sub int | None size of subsample for cross-validation; by default it is 200^{(m+4)/5} None n_res int | None how many subsamples we draw; 1 by default 1 Returns: Type Description KernelReg fitted on sample (nobs, with derivatives) np . ndarray and bandwidths (m) Source code in bs_python_utils/bsstats.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def reg_nonpar ( y : np . ndarray , X : np . ndarray , var_types : str | None = None , n_sub : int | None = None , n_res : int | None = 1 , ) -> tuple [ KernelReg , np . ndarray ]: \"\"\"nonparametric regression of y on the columns of X; the bandwidth is chosen on a subsample of size `nsub` if `nsub` < `nobs`, and rescaled. Args: y: a vector of size nobs X: a (nobs) vector or a matrix of shape (nobs, m) var_types: specify types of all `X` variables if not all of them are continuous; one character per variable * 'c' for continuous * 'u' discrete unordered * 'o' discrete ordered. n_sub: size of subsample for cross-validation; by default it is `200^{(m+4)/5}` n_res: how many subsamples we draw; 1 by default Returns: fitted on sample (nobs, with derivatives) and bandwidths (m) \"\"\" _ = check_vector_or_matrix ( X ) n_obs = check_vector ( y ) if X . shape [ 0 ] != n_obs : bs_error_abort ( \"X and y should have the same number of observations\" ) m = 1 if X . ndim == 1 else X . shape [ 1 ] if var_types is None : types = \"c\" * m else : if len ( var_types ) != m : bs_error_abort ( \"var_types should have one entry for each column of X\" ) types = var_types if n_sub is None : n_sub = round ( 200 ** (( m + 4.0 ) / 5.0 )) k = KernelReg ( y , X , var_type = types , defaults = EstimatorSettings ( efficient = True , n_sub = n_sub , randomize = True , n_res = n_res ), ) return k . fit (), k . bw reg_nonpar_fit ( y , X , var_types = None , n_sub = None , n_res = 1 , verbose = False ) \u00b6 nonparametric regression of y on the columns of X; the bandwidth is chosen on a subsample of size nsub if nsub < nobs , and rescaled. Parameters: Name Type Description Default y np . ndarray a vector of size nobs required X np . ndarray a (nobs) vector or a matrix of shape (nobs, m) required var_types str | None specify types of all X variables if not all of them are continuous; one character per variable 'c' for continuous 'u' discrete unordered 'o' discrete ordered. None n_sub int | None size of subsample for cross-validation; by default it is 200^{(m+4)/5} None n_res int how many subsamples we draw; 1 by default 1 verbose bool prints stuff if True False Returns: Type Description np . ndarray fitted values on sample (nobs) Source code in bs_python_utils/bsstats.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def reg_nonpar_fit ( y : np . ndarray , X : np . ndarray , var_types : str | None = None , n_sub : int | None = None , n_res : int = 1 , verbose : bool = False , ) -> np . ndarray : \"\"\"nonparametric regression of y on the columns of X; the bandwidth is chosen on a subsample of size `nsub` if `nsub` < `nobs`, and rescaled. Args: y: a vector of size nobs X: a (nobs) vector or a matrix of shape (nobs, m) var_types: specify types of all `X` variables if not all of them are continuous; one character per variable * 'c' for continuous * 'u' discrete unordered * 'o' discrete ordered. n_sub: size of subsample for cross-validation; by default it is `200^{(m+4)/5}` n_res: how many subsamples we draw; 1 by default verbose: prints stuff if True Returns: fitted values on sample (nobs) \"\"\" kfbw = reg_nonpar ( y , X , var_types , n_sub , n_res ) fitted_vals = cast ( np . ndarray , kfbw [ 0 ][ 0 ]) return fitted_vals tsls ( y , X , Z ) \u00b6 TSLS of y on X with instruments Z Parameters: Name Type Description Default y np . ndarray independent variable (nobs) required X np . ndarray covariates (nobs, nx) required Z np . ndarray instruments (nobs, nz) required Returns: Type Description TslsResults a tsls_results object Source code in bs_python_utils/bsstats.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def tsls ( y : np . ndarray , X : np . ndarray , Z : np . ndarray ) -> TslsResults : \"\"\"TSLS of `y` on `X` with instruments `Z` Args: y: independent variable `(nobs)` X: covariates `(nobs, nx)` Z: instruments `(nobs, nz)` Returns: a `tsls_results` object \"\"\" # first stage X_IV_proj , b_proj_IV , r2_first_iv = proj_Z ( X , Z ) # second stage y_proj , y_coeffs , r2_y = proj_Z ( y , Z ) _ , iv_estimates , r2_second = proj_Z ( y_proj , X_IV_proj ) return TslsResults ( iv_estimates , r2_first_iv , r2_y , r2_second , y_proj , y_coeffs , X_IV_proj , b_proj_IV , )","title":"statistics"},{"location":"bsstats.html#bsstats-module","text":"Contains some statistical routines.","title":"bsstats module"},{"location":"bsstats.html#bs_python_utils.bsstats.TslsResults","text":"contains the full results of a TSLS regression Source code in bs_python_utils/bsstats.py 23 24 25 26 27 28 29 30 31 32 33 34 @dataclass class TslsResults : \"\"\"contains the full results of a TSLS regression\"\"\" iv_estimates : float | np . ndarray | None r2_first_iv : float | np . ndarray | None r2_y : float | None r2_second : float | np . ndarray | None y_proj : float | np . ndarray | None y_coeffs : float | np . ndarray | None X_IV_proj : float | np . ndarray | None b_proj_IV : float | np . ndarray | None","title":"TslsResults"},{"location":"bsstats.html#bs_python_utils.bsstats.bs_multivariate_normal_pdf","text":"Multivariate (or univariate) normal probability density function at values_x Parameters: Name Type Description Default values_x np . ndarray values at which to evaluate the pdf, an n -vector or an (n, nvars) matrix required means_x float | np . ndarray means of the multivariate normal, a float or an (nvars) vector required cov_mat float | np . ndarray covariance matrix of the multivariate normal, a float or an (nvars, nvars) matrix required Returns: Type Description np . ndarray the values of the density at values_x Source code in bs_python_utils/bsstats.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def bs_multivariate_normal_pdf ( values_x : np . ndarray , means_x : float | np . ndarray , cov_mat : float | np . ndarray ) -> np . ndarray : \"\"\"Multivariate (or univariate) normal probability density function at values_x Args: values_x: values at which to evaluate the pdf, an `n`-vector or an `(n, nvars)` matrix means_x: means of the multivariate normal, a float or an `(nvars)` vector cov_mat: covariance matrix of the multivariate normal, a float or an `(nvars, nvars)` matrix Returns: the values of the density at `values_x` \"\"\" ndims_values = check_vector_or_matrix ( values_x , \"bs_multivariate_normal_pdf\" ) if ndims_values == 1 : # we are evaluating a univariate normal # if not type(means_x) == float: # bs_error_abort(f\"means_x should be a float as values_x is a vector\") # if not type(cov_mat) == float: # bs_error_abort(f\"cov_mat should be a float as values_x is a vector\") sigma2 = cov_mat resid = values_x - means_x dval = np . exp ( - 0.5 * resid * resid / sigma2 ) / np . sqrt ( 2 * np . pi * sigma2 ) return cast ( np . ndarray , dval ) else : # we are evaluating a multivariate normal n , nvars = values_x . shape n_means = check_vector ( means_x , \"bs_multivariate_normal_pdf\" ) if n_means != nvars : bs_error_abort ( f \"means_x should be a vector of size { nvars } not { n_means } \" ) nrows , ncols = check_matrix ( cov_mat , \"bs_multivariate_normal_pdf\" ) if nrows != ncols or nrows != nvars : bs_error_abort ( f \"cov_mat should be a matrix ( { nvars } , { nvars } ) not ( { nrows } , { ncols } )\" ) resid = values_x - means_x argresid = spla . solve ( cov_mat , resid . T ) argexp = np . zeros ( n ) for i in range ( n ): argexp [ i ] = np . dot ( resid [ i , :], argresid [:, i ]) dval = np . exp ( - 0.5 * argexp ) / np . sqrt ( (( 2 * np . pi ) ** nvars ) * spla . det ( cov_mat ) ) return cast ( np . ndarray , dval )","title":"bs_multivariate_normal_pdf()"},{"location":"bsstats.html#bs_python_utils.bsstats.emcee_draw","text":"uses MCMC to draw n_samples samples from the log pdf with given parameters Parameters: Name Type Description Default n_samples int the number of samples to draw required log_pdf Callable [[ np . ndarray , list ], float ] the log of the pdf, log_pdf(x, *params) ; retuns a float from the array for one sample required p0 np . ndarray the initial values for the walkers required params list | None a list of parameters None n_burn_in int | None the number of burn-in iterations for MCMC 100 seed int | None to randomly draw the samples from the walkers 8754 Returns: Type Description np . ndarray an (n_samples, n_dims) array of samples. Warning The log_pdf function should return minus infinity outside of the support of the pdf, and p0 should be contained in that support. Source code in bs_python_utils/bsstats.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 def emcee_draw ( n_samples : int , log_pdf : Callable [[ np . ndarray , list ], float ], p0 : np . ndarray , params : list | None = None , n_burn_in : int | None = 100 , seed : int | None = 8754 , ) -> np . ndarray : \"\"\"uses MCMC to draw `n_samples` samples from the log pdf with given parameters Args: n_samples: the number of samples to draw log_pdf: the log of the pdf, `log_pdf(x, *params)`; retuns a float from the array for one sample p0: the initial values for the walkers params: a list of parameters n_burn_in: the number of burn-in iterations for MCMC seed: to randomly draw the samples from the walkers Returns: an `(n_samples, n_dims)` array of samples. Warning: The `log_pdf` function should return minus infinity outside of the support of the pdf, and `p0` should be contained in that support. \"\"\" n_walkers , n_dims = p0 . shape # burn in sampler = EnsembleSampler ( n_walkers , n_dims , log_pdf , args = params ) state = sampler . run_mcmc ( p0 , n_burn_in ) sampler . reset () # generate the samples sampler . run_mcmc ( state , n_samples ) samples = sampler . get_chain ( flat = True ) rng = np . random . default_rng ( seed ) samples = rng . choice ( samples , size = n_samples , replace = False ) return cast ( np . ndarray , samples )","title":"emcee_draw()"},{"location":"bsstats.html#bs_python_utils.bsstats.estimate_pdf","text":"return an estimate of the conditional densities of x at points values_x (Silverman rule) Parameters: Name Type Description Default x_obs np . ndarray an n -vector or an (n, nvars) matrix of the observed values of x required x_points np . ndarray an m -vector or an (m, nvars) matrix of x values required MIN_SIZE_NONPAR int minimum size above which we use kernel density estimators 200 weights np . ndarray | None an n -vector of weights for the observations, if present None Returns: Type Description np . ndarray the density estimates at values_x Source code in bs_python_utils/bsstats.py 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 def estimate_pdf ( x_obs : np . ndarray , x_points : np . ndarray , MIN_SIZE_NONPAR : int = 200 , weights : np . ndarray | None = None , ) -> np . ndarray : \"\"\"return an estimate of the conditional densities of `x` at points `values_x` (Silverman rule) Args: x_obs: an `n`-vector or an `(n, nvars)` matrix of the observed values of `x` x_points: an `m`-vector or an `(m, nvars)` matrix of x values MIN_SIZE_NONPAR: minimum size above which we use kernel density estimators weights: an `n`-vector of weights for the observations, if present Returns: the density estimates at `values_x` \"\"\" ndims_x = check_vector_or_matrix ( x_obs , \"estimate_pdf\" ) ndims_valx = check_vector_or_matrix ( x_points , \"estimate_pdf\" ) if ndims_x == 1 : n_obs = x_obs . size if ndims_valx != 1 : bs_error_abort ( f \"x_points should have one dimension, not { ndims_valx } \" ) xt_obs = x_obs . reshape (( - 1 , 1 )) nvars = 1 xt_points = x_points . reshape (( - 1 , 1 )) else : # ndims_x == 2 n_obs , nvars = x_obs . shape if ndims_valx == 1 : # only one x point with nv elements nv = x_points . size else : # several x points with nv elements nv = x_points . shape [ 1 ] if nv != nvars : bs_error_abort ( f \"x_points should have { nvars } variables, not { nv } \" ) xt_obs = x_obs xt_points = x_points if weights is not None : n_w = check_vector ( weights , \"estimate_pdf\" ) if n_w != n_obs : bs_error_abort ( f \"if weights is given, it should be a vector of size { n_obs } not { n_w } \" ) min_size_np = MIN_SIZE_NONPAR ** (( 4.0 + nvars ) / 5.0 ) if n_obs > min_size_np : # cell large enough to use nonparametrics # fit joint density of x kde = sts . gaussian_kde ( xt_obs . T , bw_method = \"silverman\" , weights = weights ) # density of x at values_x f_x = kde . evaluate ( xt_points . T ) else : # sample too small, we fit a normal if ndims_x == 1 : # univariate mean_x = np . mean ( x_obs ) var_x = np . var ( x_obs ) f_x = bs_multivariate_normal_pdf ( x_points , mean_x , var_x ) else : # multivariate means_x = np . mean ( x_obs , 0 ) cov_mat = np . cov ( x_obs . T ) f_x = bs_multivariate_normal_pdf ( x_points , means_x , cov_mat ) if weights is not None : f_x *= weights / np . mean ( weights ) return cast ( np . ndarray , f_x )","title":"estimate_pdf()"},{"location":"bsstats.html#bs_python_utils.bsstats.flexible_reg","text":"flexible regression of Y on X Parameters: Name Type Description Default Y np . ndarray independent variable (nobs) or (nobs, ny) required X np . ndarray covariates (nobs) or (nobs, nx) ; should not include a constant term required mode str what flexible means * 'NP': non parametric * 'SPL': spline regression, only on one covariate * '1': linear * '2': quadratic 'NP' var_types str | None [for 'NP' only] specify types of all X variables if not all of them are continuous; one character per variable * 'c' for continuous * 'u' discrete unordered * 'o' discrete ordered None n_sub int | None [for 'NP' only] size of subsample for cross-validation; by default it is 200^{(m+4)/5} None n_res int [for 'NP' only] how many subsamples we draw; 1 if None 1 verbose bool prints stuff if True False Returns: Type Description np . ndarray E(y|X) at the sample points Source code in bs_python_utils/bsstats.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 def flexible_reg ( Y : np . ndarray , X : np . ndarray , mode : str = \"NP\" , var_types : str | None = None , n_sub : int | None = None , n_res : int = 1 , verbose : bool = False , ) -> np . ndarray : \"\"\"flexible regression of `Y` on `X` Args: Y: independent variable `(nobs)` or `(nobs, ny)` X: covariates `(nobs)` or `(nobs, nx)`; should **not** include a constant term mode: what flexible means * 'NP': non parametric * 'SPL': spline regression, only on one covariate * '1': linear * '2': quadratic var_types: [for 'NP' only] specify types of all `X` variables if not all of them are continuous; one character per variable * 'c' for continuous * 'u' discrete unordered * 'o' discrete ordered n_sub: [for 'NP' only] size of subsample for cross-validation; \\ by default it is `200^{(m+4)/5}` n_res: [for 'NP' only] how many subsamples we draw; 1 if `None` verbose: prints stuff if True Returns: `E(y|X)` at the sample points \"\"\" if mode == \"NP\" : if Y . ndim == 2 : ny = Y . shape [ 1 ] Y_fit = np . zeros_like ( Y ) for iy in range ( ny ): Y_fit [:, iy ] = reg_nonpar_fit ( Y [:, iy ], X , var_types = var_types , n_sub = n_sub , n_res = n_res , verbose = verbose , ) return Y_fit else : return reg_nonpar_fit ( Y , X , var_types = var_types , n_sub = n_sub , n_res = n_res , verbose = verbose ) elif mode == \"SPL\" : if X . ndim > 1 : bs_error_abort ( \"with a spline, only works in one dimension\" ) return spline_reg ( Y , X ) else : try : imode = int ( mode ) except TypeError : bs_error_abort ( f \"does not accept mode= { mode } \" ) preg , _ , _ = proj_Z ( Y , X , p = imode , verbose = verbose ) return preg","title":"flexible_reg()"},{"location":"bsstats.html#bs_python_utils.bsstats.kde_resample","text":"Fit a nonparametric density to data by KDE, with cross-validation with starting point at rule of thumb, and generate samples from the estimated density. Parameters: Name Type Description Default data np . ndarray an (n_obs, n_dims) matrix of data required n_samples int how many iid draws we want required n_bw int how mamy bandwidths we try from 1/10th to 10 times the rule-of-thumb 10 Returns: Type Description tuple [ np . ndarray , float ] an (n_samples, n_dims) matrix of draws, and the bandwidth used. Source code in bs_python_utils/bsstats.py 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 def kde_resample ( data : np . ndarray , n_samples : int , n_bw : int = 10 ) -> tuple [ np . ndarray , float ]: \"\"\"Fit a nonparametric density to data by KDE, with cross-validation with starting point at rule of thumb, and generate samples from the estimated density. Args: data: an `(n_obs, n_dims)` matrix of data n_samples: how many iid draws we want n_bw: how mamy bandwidths we try from 1/10th to 10 times the rule-of-thumb Returns: an `(n_samples, n_dims)` matrix of draws, and the bandwidth used. \"\"\" n_obs , n_dims = check_matrix ( data ) h_rot = np . std ( data ) * ( n_obs ** ( - 1 / ( n_dims + 4 ))) params = { \"bandwidth\" : h_rot * np . logspace ( - 1 , 1 , n_bw )} grid = GridSearchCV ( KernelDensity (), params ) grid . fit ( data ) kde = grid . best_estimator_ best_bandwidth = grid . best_params_ [ \"bandwidth\" ] resampled_data = kde . sample ( n_samples ) return resampled_data , best_bandwidth","title":"kde_resample()"},{"location":"bsstats.html#bs_python_utils.bsstats.proj_Z","text":"project W on all interactions of degree p or less of Z Parameters: Name Type Description Default W np . ndarray variable(s) (nobs) or (nobs, nw) required Z np . ndarray instruments (nobs) or (nobs, nz)`; they should not include a constant term required p int maximum total degree for interactions of the columns of Z 1 verbose bool prints stuff if True False Returns: Type Description tuple [ np . ndarray , np . ndarray , float ] the projections of the columns of W on Z etc, the coefficients, and the R^2 of each column Source code in bs_python_utils/bsstats.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def proj_Z ( W : np . ndarray , Z : np . ndarray , p : int = 1 , verbose : bool = False ) -> tuple [ np . ndarray , np . ndarray , float ]: \"\"\"project `W` on all interactions of degree `p` or less of `Z` Args: W: variable(s) `(nobs)` or `(nobs, nw)` Z: instruments `(nobs) or `(nobs, nz)`; they should **not** include a constant term p: maximum total degree for interactions of the columns of `Z` verbose: prints stuff if True Returns: the projections of the columns of `W` on `Z` etc, the coefficients, and the `R^2` of each column \"\"\" nobs = Z . shape [ 0 ] if W . shape [ 0 ] != nobs : bs_error_abort ( \"W and Z should have the same number of rows\" ) if W . ndim > 2 : bs_error_abort ( \"W should have 1 or 2 dimensions\" ) if Z . ndim > 2 : bs_error_abort ( \"Z should have 1 or 2 dimensions\" ) if Z . ndim == 1 : Zp = np . zeros (( nobs , 1 + p )) Zp [:, 0 ] = np . ones ( nobs ) for q in range ( 1 , p + 1 ): Zp [:, q ] = Z ** q else : # Z is a matrix Zp , k = _make_Zp ( Z , p ) if verbose : print ( f \"_proj_Z with degree { p } , using { k } regressors\" ) return _final_proj ( Zp , W )","title":"proj_Z()"},{"location":"bsstats.html#bs_python_utils.bsstats.reg_nonpar","text":"nonparametric regression of y on the columns of X; the bandwidth is chosen on a subsample of size nsub if nsub < nobs , and rescaled. Parameters: Name Type Description Default y np . ndarray a vector of size nobs required X np . ndarray a (nobs) vector or a matrix of shape (nobs, m) required var_types str | None specify types of all X variables if not all of them are continuous; one character per variable 'c' for continuous 'u' discrete unordered 'o' discrete ordered. None n_sub int | None size of subsample for cross-validation; by default it is 200^{(m+4)/5} None n_res int | None how many subsamples we draw; 1 by default 1 Returns: Type Description KernelReg fitted on sample (nobs, with derivatives) np . ndarray and bandwidths (m) Source code in bs_python_utils/bsstats.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def reg_nonpar ( y : np . ndarray , X : np . ndarray , var_types : str | None = None , n_sub : int | None = None , n_res : int | None = 1 , ) -> tuple [ KernelReg , np . ndarray ]: \"\"\"nonparametric regression of y on the columns of X; the bandwidth is chosen on a subsample of size `nsub` if `nsub` < `nobs`, and rescaled. Args: y: a vector of size nobs X: a (nobs) vector or a matrix of shape (nobs, m) var_types: specify types of all `X` variables if not all of them are continuous; one character per variable * 'c' for continuous * 'u' discrete unordered * 'o' discrete ordered. n_sub: size of subsample for cross-validation; by default it is `200^{(m+4)/5}` n_res: how many subsamples we draw; 1 by default Returns: fitted on sample (nobs, with derivatives) and bandwidths (m) \"\"\" _ = check_vector_or_matrix ( X ) n_obs = check_vector ( y ) if X . shape [ 0 ] != n_obs : bs_error_abort ( \"X and y should have the same number of observations\" ) m = 1 if X . ndim == 1 else X . shape [ 1 ] if var_types is None : types = \"c\" * m else : if len ( var_types ) != m : bs_error_abort ( \"var_types should have one entry for each column of X\" ) types = var_types if n_sub is None : n_sub = round ( 200 ** (( m + 4.0 ) / 5.0 )) k = KernelReg ( y , X , var_type = types , defaults = EstimatorSettings ( efficient = True , n_sub = n_sub , randomize = True , n_res = n_res ), ) return k . fit (), k . bw","title":"reg_nonpar()"},{"location":"bsstats.html#bs_python_utils.bsstats.reg_nonpar_fit","text":"nonparametric regression of y on the columns of X; the bandwidth is chosen on a subsample of size nsub if nsub < nobs , and rescaled. Parameters: Name Type Description Default y np . ndarray a vector of size nobs required X np . ndarray a (nobs) vector or a matrix of shape (nobs, m) required var_types str | None specify types of all X variables if not all of them are continuous; one character per variable 'c' for continuous 'u' discrete unordered 'o' discrete ordered. None n_sub int | None size of subsample for cross-validation; by default it is 200^{(m+4)/5} None n_res int how many subsamples we draw; 1 by default 1 verbose bool prints stuff if True False Returns: Type Description np . ndarray fitted values on sample (nobs) Source code in bs_python_utils/bsstats.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def reg_nonpar_fit ( y : np . ndarray , X : np . ndarray , var_types : str | None = None , n_sub : int | None = None , n_res : int = 1 , verbose : bool = False , ) -> np . ndarray : \"\"\"nonparametric regression of y on the columns of X; the bandwidth is chosen on a subsample of size `nsub` if `nsub` < `nobs`, and rescaled. Args: y: a vector of size nobs X: a (nobs) vector or a matrix of shape (nobs, m) var_types: specify types of all `X` variables if not all of them are continuous; one character per variable * 'c' for continuous * 'u' discrete unordered * 'o' discrete ordered. n_sub: size of subsample for cross-validation; by default it is `200^{(m+4)/5}` n_res: how many subsamples we draw; 1 by default verbose: prints stuff if True Returns: fitted values on sample (nobs) \"\"\" kfbw = reg_nonpar ( y , X , var_types , n_sub , n_res ) fitted_vals = cast ( np . ndarray , kfbw [ 0 ][ 0 ]) return fitted_vals","title":"reg_nonpar_fit()"},{"location":"bsstats.html#bs_python_utils.bsstats.tsls","text":"TSLS of y on X with instruments Z Parameters: Name Type Description Default y np . ndarray independent variable (nobs) required X np . ndarray covariates (nobs, nx) required Z np . ndarray instruments (nobs, nz) required Returns: Type Description TslsResults a tsls_results object Source code in bs_python_utils/bsstats.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def tsls ( y : np . ndarray , X : np . ndarray , Z : np . ndarray ) -> TslsResults : \"\"\"TSLS of `y` on `X` with instruments `Z` Args: y: independent variable `(nobs)` X: covariates `(nobs, nx)` Z: instruments `(nobs, nz)` Returns: a `tsls_results` object \"\"\" # first stage X_IV_proj , b_proj_IV , r2_first_iv = proj_Z ( X , Z ) # second stage y_proj , y_coeffs , r2_y = proj_Z ( y , Z ) _ , iv_estimates , r2_second = proj_Z ( y_proj , X_IV_proj ) return TslsResults ( iv_estimates , r2_first_iv , r2_y , r2_second , y_proj , y_coeffs , X_IV_proj , b_proj_IV , )","title":"tsls()"},{"location":"bsutils.html","text":"bsutils module \u00b6 Contains various utilities programs: printargs : a decorator that reports the arguments of the function it decorates bs_name_func : returns the name of the current function or its callers bs_error_abort : reports on error and aborts execution final_s : whether a word should have a final 's' bs_switch : an improved switch statement find_first : returns the index of and the first item in an iterable that satisfies a condition print_stars : prints a title within lines of stars file_print_stars : does the same, to a file fstring_integer_with_significant_digits : rounds an integer and returns an f-string mkdir_if_needed : creates a directory if it does not exist bscomb : a combination \\({n \\choose k}\\) operator bslog, bsxlogx, bsexp : \\(C^2\\) extensions of \\(\\log(x), x\\log x, \\exp(x)\\) , and their first two derivatives bs_projection_point : projects a point on a line in the plane. Note if the math looks strange in the documentation, just reload the page. bs_error_abort ( msg = 'error, aborting' ) \u00b6 report error and abort Parameters: Name Type Description Default msg str a message 'error, aborting' Returns: Type Description None prints the message and exits with code 1 Source code in bs_python_utils/bsutils.py 66 67 68 69 70 71 72 73 74 75 76 77 def bs_error_abort ( msg : str = \"error, aborting\" ) -> None : \"\"\" report error and abort Args: msg: a message Returns: prints the message and exits with code 1 \"\"\" print_stars ( f \" { bs_name_func ( 3 ) } : { msg } \" ) sys . exit ( 1 ) bs_name_func ( back = 2 ) \u00b6 get the name of the current function, or further back in stack Parameters: Name Type Description Default back int 2 is current function, 3 the function that called it etc 2 Returns: Type Description str the name of the function Source code in bs_python_utils/bsutils.py 51 52 53 54 55 56 57 58 59 60 61 62 63 def bs_name_func ( back : int = 2 ) -> str : \"\"\" get the name of the current function, or further back in stack Args: back: 2 is current function, 3 the function that called it etc Returns: the name of the function \"\"\" stack = traceback . extract_stack () * _ , func_name , _ = stack [ - back ] return cast ( str , func_name ) bs_projection_point ( x , y , a , b , c ) \u00b6 projection of point (x,y) on line ax+by+c=0 Parameters: Name Type Description Default x float y: coordinates required a float b: c: line parameters (as in ax+by+c=0) required Returns: Name Type Description x_proj float y_proj: coordinates of projection point dist float distance of point from line Source code in bs_python_utils/bsutils.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 def bs_projection_point ( x : float , y : float , a : float , b : float , c : float ) -> tuple [ float , float , float ]: \"\"\" projection of point (x,y) on line ax+by+c=0 Args: x: y: coordinates a: b: c: line parameters (as in ax+by+c=0) Returns: x_proj: y_proj: coordinates of projection point dist: distance of point from line \"\"\" a2b2 = a * a + b * b denom = sqrt ( a2b2 ) value = a * x + b * y + c x_proj = x - a * value / a2b2 y_proj = y - b * value / a2b2 dist = abs ( value ) / denom return x_proj , y_proj , dist bs_switch ( match , dico , strict = True , default = 'no match' ) \u00b6 a switch statement that allows for partial matches if strict is False Parameters: Name Type Description Default match str what we look for in the keys required dico dict a dictionary with string keys required strict bool if False , we accept a partial match True default Any what we return if no match is found 'no match' Returns: Type Description Any the value for the match, or default Examples: >>> calc_dict = { \"plus\": lambda x, y: x + y, \"minus\": lambda x, y: x - y } >>> plus = bs_switch ( 'plus' , calc_dict , default = \"unintended function\" ) >>> minus = bs_switch ( 'min' , calc_dict , strict = False , default = \"unintended function\" ) >>> plus ( 6 , 4 ) 10 >>> minus ( 6 , 4 ) 2 >>> bs_switch ( 'plu' , calc_dict ) \"no match\" Source code in bs_python_utils/bsutils.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def bs_switch ( match : str , dico : dict , strict : bool = True , default : Any = \"no match\" ) -> Any : \"\"\" a switch statement that allows for partial matches if strict is False Args: match: what we look for in the keys dico: a dictionary with string keys strict: if `False`, we accept a partial match default: what we return if no match is found Returns: the value for the match, or `default` Examples: >>> calc_dict = { \"plus\": lambda x, y: x + y, \"minus\": lambda x, y: x - y } >>> plus = bs_switch('plus', calc_dict, default=\"unintended function\") >>> minus = bs_switch('min', calc_dict, strict=False, default=\"unintended function\") >>> plus(6, 4) 10 >>> minus(6, 4) 2 >>> bs_switch('plu', calc_dict) \"no match\" \"\"\" if strict : for key in dico : if match == key : return dico . get ( key ) else : for key in dico : if match in key : return dico . get ( key ) return default bscomb ( n , k ) \u00b6 number of combinations of k among n {n \\choose k} Parameters: Name Type Description Default n int required k int should be smaller than n required Returns: Type Description int {n \\choose k} . Source code in bs_python_utils/bsutils.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def bscomb ( n : int , k : int ) -> int : \"\"\" number of combinations of k among n `{n \\\\choose k}` Args: n: k: should be smaller than n Returns: `{n \\\\choose k}`. \"\"\" if not isinstance ( n , int ): bs_error_abort ( f \"n should be an integer, not { n } \" ) if not isinstance ( k , int ): bs_error_abort ( f \"k should be an integer, not { k } \" ) if n < k : bs_error_abort ( f \"k= { k } should not be larger than n= { n } \" ) return factorial ( n ) // ( factorial ( k ) * factorial ( n - k )) bsexp ( x , bigx = 50.0 , lowx =- 50.0 , deriv = 0 ) \u00b6 C^2 -extends the exponential above bigx and below lowx perhaps with derivatives Parameters: Name Type Description Default x float argument required bigx float upper bound 50.0 lowx float lower bound -50.0 deriv int if 1, also return first derivative; if 2, the first two derivatives 0 Returns: Type Description float | TwoFloats | ThreeFloats the exponential C^2 -extended above bigx and below lowx float | TwoFloats | ThreeFloats perhaps with derivatives Source code in bs_python_utils/bsutils.py 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def bsexp ( x : float , bigx : float = 50.0 , lowx : float = - 50.0 , deriv : int = 0 , ) -> float | TwoFloats | ThreeFloats : \"\"\" `C^2`-extends the exponential above `bigx` and below `lowx` perhaps with derivatives Args: x: argument bigx: upper bound lowx: lower bound deriv: if 1, also return first derivative; if 2, the first two derivatives Returns: the exponential `C^2`-extended above `bigx` and below `lowx` perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv can only be 0, 1, or 2; not { deriv } \" ) if lowx < x < bigx : expx = exp ( x ) if deriv == 0 : return expx if deriv == 1 : return expx , expx return expx , expx , expx elif x < lowx : return _bsexp_extend ( x , deriv , lowx ) else : return _bsexp_extend ( x , deriv , bigx ) bslog ( x , eps = 1e-30 , deriv = 0 ) \u00b6 extends the logarithm below eps by taking a second-order approximation perhaps with derivatives Parameters: Name Type Description Default x float argument required eps float lower bound 1e-30 deriv int if 1, also return first derivative; if 2, the first two derivatives 0 Returns: Type Description float | TwoFloats | ThreeFloats \\ln(x) C^2 -extended below eps , perhaps with derivatives Source code in bs_python_utils/bsutils.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def bslog ( x : float , eps : float = 1e-30 , deriv : int = 0 ) -> float | TwoFloats | ThreeFloats : \"\"\" extends the logarithm below `eps` by taking a second-order approximation perhaps with derivatives Args: x: argument eps: lower bound deriv: if 1, also return first derivative; if 2, the first two derivatives Returns: `\\\\ln(x)` `C^2`-extended below `eps`, perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv can only be 0, 1, or 2; not { deriv } \" ) if x > eps : logx = log ( x ) if deriv == 0 : return logx dlogx = 1.0 / x if deriv == 1 : return logx , dlogx d2logx = - dlogx * dlogx return logx , dlogx , d2logx else : dx = 1.0 - x / eps log_smaller = log ( eps ) - dx - dx * dx / 2.0 if deriv == 0 : return log_smaller dlog_smaller = ( 1.0 + dx ) / eps if deriv == 1 : return log_smaller , dlog_smaller d2log_smaller = - 1.0 / eps / eps return log_smaller , dlog_smaller , d2log_smaller bsxlogx ( x , eps = 1e-30 , deriv = 0 ) \u00b6 extends x \\ln(x) below eps by making it go to zero in a C^2 extension perhaps with derivatives Parameters: Name Type Description Default x float argument required eps float lower bound 1e-30 deriv int if 1, also return first derivative; if 2, the first two derivatives 0 Returns: Type Description float | TwoFloats | ThreeFloats x \\ln(x) C^2 -extended below eps , perhaps with derivatives Source code in bs_python_utils/bsutils.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 def bsxlogx ( x : float , eps : float = 1e-30 , deriv : int = 0 ) -> float | TwoFloats | ThreeFloats : \"\"\" extends `x \\\\ln(x)` below `eps` by making it go to zero in a `C^2` extension perhaps with derivatives Args: x: argument eps: lower bound deriv: if 1, also return first derivative; if 2, the first two derivatives Returns: `x \\\\ln(x)` `C^2`-extended below `eps`, perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv can only be 0, 1, or 2; not { deriv } \" ) if x > eps : logx = log ( x ) if deriv == 0 : return x * logx if deriv == 1 : return x * logx , 1.0 + logx return x * logx , 1.0 + logx , 1.0 / x else : logeps = log ( eps ) dx = x / eps log_smaller = x * logeps - eps / 2.0 + x * dx / 2.0 if deriv == 0 : return log_smaller if deriv == 1 : return log_smaller , logeps + dx return log_smaller , logeps + dx , 1.0 / eps file_print_stars ( file_handle , title = None , n = 70 ) \u00b6 prints to a file a title within stars Parameters: Name Type Description Default file_handle TextIOBase file handle required title str title None n int length of line 70 Returns: Type Description None prints a starred line to the file, or two around the title Source code in bs_python_utils/bsutils.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def file_print_stars ( file_handle : TextIOBase , title : str = None , n : int = 70 ) -> None : \"\"\" prints to a file a title within stars Args: file_handle: file handle title: title n: length of line Returns: prints a starred line to the file, or two around the title \"\"\" line_stars = \"*\" * n file_handle . write ( \" \\n \" ) file_handle . write ( line_stars ) file_handle . write ( \" \\n \" ) if title : file_handle . write ( title . center ( n )) file_handle . write ( \" \\n \" ) file_handle . write ( line_stars ) file_handle . write ( \" \\n \" ) file_handle . write ( \" \\n \" ) final_s ( n , word ) \u00b6 pluralizes word if n > 1 Parameters: Name Type Description Default n int how many times required word str to be pluralized, maybe required Returns: Type Description str 1 word or n words Source code in bs_python_utils/bsutils.py 80 81 82 83 84 85 86 87 88 89 90 91 92 def final_s ( n : int , word : str ) -> str : \"\"\" pluralizes word if n > 1 Args: n: how many times word: to be pluralized, maybe Returns: `1 word` or `n words` \"\"\" suffix = \"s\" if n > 1 else \"\" return f \" { n } { word }{ suffix } \" find_first ( iterable , condition = lambda x : True ) \u00b6 Returns the index of and the first item in the iterable that satisfies the condition . Parameters: Name Type Description Default iterable Iterable where to look required condition Callable must return a boolean lambda x: True Returns: Type Description Any If the condition is not given, returns 0 and the first item of Any the iterable. Any Raises StopIteration if no item satisfyng the condition is found. Examples: >>> find_first ( ( 1 , 2 , 3 ), condition = lambda x : x % 2 == 0 ) (1, 2) >>> find_first ( range ( 3 , 100 )) (0, 3) >>> find_first ( () ) Traceback (most recent call last): ... StopIteration Source code in bs_python_utils/bsutils.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def find_first ( iterable : Iterable , condition : Callable = lambda x : True ) -> Any : \"\"\" Returns the index of and the first item in the `iterable` that satisfies the `condition`. Args: iterable: where to look condition: must return a boolean Returns: If the condition is not given, returns 0 and the first item of the iterable. Raises `StopIteration` if no item satisfyng the condition is found. Examples: >>> find_first( (1,2,3), condition=lambda x: x % 2 == 0) (1, 2) >>> find_first(range(3, 100)) (0, 3) >>> find_first( () ) Traceback (most recent call last): ... StopIteration \"\"\" return next (( i , x ) for i , x in enumerate ( iterable ) if condition ( x )) fstring_integer_with_significant_digits ( number , m ) \u00b6 returns an f-string with number rounded to m significant digits Parameters: Name Type Description Default number int the integer we want to round required m int how many digits we keep; the rest is filled with zeroes required Return a string with the rounded integer Examples: fstring_integer_with_significant_digits(12345, 0) ' 0' fstring_integer_with_significant_digits(12345, 3) '12,300' fstring_integer_with_significant_digits(12345, 7) '12345' Source code in bs_python_utils/bsutils.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def fstring_integer_with_significant_digits ( number : int , m : int ) -> str : \"\"\"returns an f-string with `number` rounded to `m` significant digits Args: number: the integer we want to round m: how many digits we keep; the rest is filled with zeroes Return: a string with the rounded integer Examples: >>> fstring_integer_with_significant_digits(12345, 0) ' 0' >>> fstring_integer_with_significant_digits(12345, 3) '12,300' >>> fstring_integer_with_significant_digits(12345, 7) '12345' \"\"\" if ( not isinstance ( number , int )) or ( not isinstance ( m , int )): bs_error_abort ( \"Both arguments should be integers.\" ) if number == 0 : return f \" { number : d } \" else : digits = len ( str ( abs ( int ( number )))) if digits <= m : return f \" { int ( number ) : d } \" else : power = digits - m rounded = round ( number / 10 ** power ) * 10 ** power return f \" { int ( rounded ) : ,d } \" mkdir_if_needed ( p ) \u00b6 create the directory if it does not exist Parameters: Name Type Description Default p Path | str a path required Returns: Type Description Path the directory Path Source code in bs_python_utils/bsutils.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def mkdir_if_needed ( p : Path | str ) -> Path : \"\"\" create the directory if it does not exist Args: p: a path Returns: the directory Path \"\"\" try : q = Path ( p ) except OSError : bs_error_abort ( f \" { p } is not a path\" ) if not q . exists (): q . mkdir ( parents = True ) return q print_stars ( title = None , n = 70 ) \u00b6 prints a title within stars Parameters: Name Type Description Default title str title None n int number of stars on line 70 Returns: Type Description None prints a starred line, or two around the title Source code in bs_python_utils/bsutils.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def print_stars ( title : str = None , n : int = 70 ) -> None : \"\"\" prints a title within stars Args: title: title n: number of stars on line Returns: prints a starred line, or two around the title \"\"\" line_stars = \"*\" * n print () print ( line_stars ) if title : print ( title . center ( n )) print ( line_stars ) print () printargs ( func ) \u00b6 Decorator that reports the arguments of the function Parameters: Name Type Description Default func Callable the decorated function required Source code in bs_python_utils/bsutils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def printargs ( func : Callable ) -> Callable : \"\"\" Decorator that reports the arguments of the function Args: func: the decorated function \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): print ( f \"Function { func . __name__ } called with args = { args } and kwargs = { kwargs } \" ) return func ( * args , ** kwargs ) return wrapper","title":"error reporting and other utilities"},{"location":"bsutils.html#bsutils-module","text":"Contains various utilities programs: printargs : a decorator that reports the arguments of the function it decorates bs_name_func : returns the name of the current function or its callers bs_error_abort : reports on error and aborts execution final_s : whether a word should have a final 's' bs_switch : an improved switch statement find_first : returns the index of and the first item in an iterable that satisfies a condition print_stars : prints a title within lines of stars file_print_stars : does the same, to a file fstring_integer_with_significant_digits : rounds an integer and returns an f-string mkdir_if_needed : creates a directory if it does not exist bscomb : a combination \\({n \\choose k}\\) operator bslog, bsxlogx, bsexp : \\(C^2\\) extensions of \\(\\log(x), x\\log x, \\exp(x)\\) , and their first two derivatives bs_projection_point : projects a point on a line in the plane. Note if the math looks strange in the documentation, just reload the page.","title":"bsutils module"},{"location":"bsutils.html#bs_python_utils.bsutils.bs_error_abort","text":"report error and abort Parameters: Name Type Description Default msg str a message 'error, aborting' Returns: Type Description None prints the message and exits with code 1 Source code in bs_python_utils/bsutils.py 66 67 68 69 70 71 72 73 74 75 76 77 def bs_error_abort ( msg : str = \"error, aborting\" ) -> None : \"\"\" report error and abort Args: msg: a message Returns: prints the message and exits with code 1 \"\"\" print_stars ( f \" { bs_name_func ( 3 ) } : { msg } \" ) sys . exit ( 1 )","title":"bs_error_abort()"},{"location":"bsutils.html#bs_python_utils.bsutils.bs_name_func","text":"get the name of the current function, or further back in stack Parameters: Name Type Description Default back int 2 is current function, 3 the function that called it etc 2 Returns: Type Description str the name of the function Source code in bs_python_utils/bsutils.py 51 52 53 54 55 56 57 58 59 60 61 62 63 def bs_name_func ( back : int = 2 ) -> str : \"\"\" get the name of the current function, or further back in stack Args: back: 2 is current function, 3 the function that called it etc Returns: the name of the function \"\"\" stack = traceback . extract_stack () * _ , func_name , _ = stack [ - back ] return cast ( str , func_name )","title":"bs_name_func()"},{"location":"bsutils.html#bs_python_utils.bsutils.bs_projection_point","text":"projection of point (x,y) on line ax+by+c=0 Parameters: Name Type Description Default x float y: coordinates required a float b: c: line parameters (as in ax+by+c=0) required Returns: Name Type Description x_proj float y_proj: coordinates of projection point dist float distance of point from line Source code in bs_python_utils/bsutils.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 def bs_projection_point ( x : float , y : float , a : float , b : float , c : float ) -> tuple [ float , float , float ]: \"\"\" projection of point (x,y) on line ax+by+c=0 Args: x: y: coordinates a: b: c: line parameters (as in ax+by+c=0) Returns: x_proj: y_proj: coordinates of projection point dist: distance of point from line \"\"\" a2b2 = a * a + b * b denom = sqrt ( a2b2 ) value = a * x + b * y + c x_proj = x - a * value / a2b2 y_proj = y - b * value / a2b2 dist = abs ( value ) / denom return x_proj , y_proj , dist","title":"bs_projection_point()"},{"location":"bsutils.html#bs_python_utils.bsutils.bs_switch","text":"a switch statement that allows for partial matches if strict is False Parameters: Name Type Description Default match str what we look for in the keys required dico dict a dictionary with string keys required strict bool if False , we accept a partial match True default Any what we return if no match is found 'no match' Returns: Type Description Any the value for the match, or default Examples: >>> calc_dict = { \"plus\": lambda x, y: x + y, \"minus\": lambda x, y: x - y } >>> plus = bs_switch ( 'plus' , calc_dict , default = \"unintended function\" ) >>> minus = bs_switch ( 'min' , calc_dict , strict = False , default = \"unintended function\" ) >>> plus ( 6 , 4 ) 10 >>> minus ( 6 , 4 ) 2 >>> bs_switch ( 'plu' , calc_dict ) \"no match\" Source code in bs_python_utils/bsutils.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def bs_switch ( match : str , dico : dict , strict : bool = True , default : Any = \"no match\" ) -> Any : \"\"\" a switch statement that allows for partial matches if strict is False Args: match: what we look for in the keys dico: a dictionary with string keys strict: if `False`, we accept a partial match default: what we return if no match is found Returns: the value for the match, or `default` Examples: >>> calc_dict = { \"plus\": lambda x, y: x + y, \"minus\": lambda x, y: x - y } >>> plus = bs_switch('plus', calc_dict, default=\"unintended function\") >>> minus = bs_switch('min', calc_dict, strict=False, default=\"unintended function\") >>> plus(6, 4) 10 >>> minus(6, 4) 2 >>> bs_switch('plu', calc_dict) \"no match\" \"\"\" if strict : for key in dico : if match == key : return dico . get ( key ) else : for key in dico : if match in key : return dico . get ( key ) return default","title":"bs_switch()"},{"location":"bsutils.html#bs_python_utils.bsutils.bscomb","text":"number of combinations of k among n {n \\choose k} Parameters: Name Type Description Default n int required k int should be smaller than n required Returns: Type Description int {n \\choose k} . Source code in bs_python_utils/bsutils.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def bscomb ( n : int , k : int ) -> int : \"\"\" number of combinations of k among n `{n \\\\choose k}` Args: n: k: should be smaller than n Returns: `{n \\\\choose k}`. \"\"\" if not isinstance ( n , int ): bs_error_abort ( f \"n should be an integer, not { n } \" ) if not isinstance ( k , int ): bs_error_abort ( f \"k should be an integer, not { k } \" ) if n < k : bs_error_abort ( f \"k= { k } should not be larger than n= { n } \" ) return factorial ( n ) // ( factorial ( k ) * factorial ( n - k ))","title":"bscomb()"},{"location":"bsutils.html#bs_python_utils.bsutils.bsexp","text":"C^2 -extends the exponential above bigx and below lowx perhaps with derivatives Parameters: Name Type Description Default x float argument required bigx float upper bound 50.0 lowx float lower bound -50.0 deriv int if 1, also return first derivative; if 2, the first two derivatives 0 Returns: Type Description float | TwoFloats | ThreeFloats the exponential C^2 -extended above bigx and below lowx float | TwoFloats | ThreeFloats perhaps with derivatives Source code in bs_python_utils/bsutils.py 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def bsexp ( x : float , bigx : float = 50.0 , lowx : float = - 50.0 , deriv : int = 0 , ) -> float | TwoFloats | ThreeFloats : \"\"\" `C^2`-extends the exponential above `bigx` and below `lowx` perhaps with derivatives Args: x: argument bigx: upper bound lowx: lower bound deriv: if 1, also return first derivative; if 2, the first two derivatives Returns: the exponential `C^2`-extended above `bigx` and below `lowx` perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv can only be 0, 1, or 2; not { deriv } \" ) if lowx < x < bigx : expx = exp ( x ) if deriv == 0 : return expx if deriv == 1 : return expx , expx return expx , expx , expx elif x < lowx : return _bsexp_extend ( x , deriv , lowx ) else : return _bsexp_extend ( x , deriv , bigx )","title":"bsexp()"},{"location":"bsutils.html#bs_python_utils.bsutils.bslog","text":"extends the logarithm below eps by taking a second-order approximation perhaps with derivatives Parameters: Name Type Description Default x float argument required eps float lower bound 1e-30 deriv int if 1, also return first derivative; if 2, the first two derivatives 0 Returns: Type Description float | TwoFloats | ThreeFloats \\ln(x) C^2 -extended below eps , perhaps with derivatives Source code in bs_python_utils/bsutils.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def bslog ( x : float , eps : float = 1e-30 , deriv : int = 0 ) -> float | TwoFloats | ThreeFloats : \"\"\" extends the logarithm below `eps` by taking a second-order approximation perhaps with derivatives Args: x: argument eps: lower bound deriv: if 1, also return first derivative; if 2, the first two derivatives Returns: `\\\\ln(x)` `C^2`-extended below `eps`, perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv can only be 0, 1, or 2; not { deriv } \" ) if x > eps : logx = log ( x ) if deriv == 0 : return logx dlogx = 1.0 / x if deriv == 1 : return logx , dlogx d2logx = - dlogx * dlogx return logx , dlogx , d2logx else : dx = 1.0 - x / eps log_smaller = log ( eps ) - dx - dx * dx / 2.0 if deriv == 0 : return log_smaller dlog_smaller = ( 1.0 + dx ) / eps if deriv == 1 : return log_smaller , dlog_smaller d2log_smaller = - 1.0 / eps / eps return log_smaller , dlog_smaller , d2log_smaller","title":"bslog()"},{"location":"bsutils.html#bs_python_utils.bsutils.bsxlogx","text":"extends x \\ln(x) below eps by making it go to zero in a C^2 extension perhaps with derivatives Parameters: Name Type Description Default x float argument required eps float lower bound 1e-30 deriv int if 1, also return first derivative; if 2, the first two derivatives 0 Returns: Type Description float | TwoFloats | ThreeFloats x \\ln(x) C^2 -extended below eps , perhaps with derivatives Source code in bs_python_utils/bsutils.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 def bsxlogx ( x : float , eps : float = 1e-30 , deriv : int = 0 ) -> float | TwoFloats | ThreeFloats : \"\"\" extends `x \\\\ln(x)` below `eps` by making it go to zero in a `C^2` extension perhaps with derivatives Args: x: argument eps: lower bound deriv: if 1, also return first derivative; if 2, the first two derivatives Returns: `x \\\\ln(x)` `C^2`-extended below `eps`, perhaps with derivatives \"\"\" if deriv not in [ 0 , 1 , 2 ]: bs_error_abort ( f \"deriv can only be 0, 1, or 2; not { deriv } \" ) if x > eps : logx = log ( x ) if deriv == 0 : return x * logx if deriv == 1 : return x * logx , 1.0 + logx return x * logx , 1.0 + logx , 1.0 / x else : logeps = log ( eps ) dx = x / eps log_smaller = x * logeps - eps / 2.0 + x * dx / 2.0 if deriv == 0 : return log_smaller if deriv == 1 : return log_smaller , logeps + dx return log_smaller , logeps + dx , 1.0 / eps","title":"bsxlogx()"},{"location":"bsutils.html#bs_python_utils.bsutils.file_print_stars","text":"prints to a file a title within stars Parameters: Name Type Description Default file_handle TextIOBase file handle required title str title None n int length of line 70 Returns: Type Description None prints a starred line to the file, or two around the title Source code in bs_python_utils/bsutils.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def file_print_stars ( file_handle : TextIOBase , title : str = None , n : int = 70 ) -> None : \"\"\" prints to a file a title within stars Args: file_handle: file handle title: title n: length of line Returns: prints a starred line to the file, or two around the title \"\"\" line_stars = \"*\" * n file_handle . write ( \" \\n \" ) file_handle . write ( line_stars ) file_handle . write ( \" \\n \" ) if title : file_handle . write ( title . center ( n )) file_handle . write ( \" \\n \" ) file_handle . write ( line_stars ) file_handle . write ( \" \\n \" ) file_handle . write ( \" \\n \" )","title":"file_print_stars()"},{"location":"bsutils.html#bs_python_utils.bsutils.final_s","text":"pluralizes word if n > 1 Parameters: Name Type Description Default n int how many times required word str to be pluralized, maybe required Returns: Type Description str 1 word or n words Source code in bs_python_utils/bsutils.py 80 81 82 83 84 85 86 87 88 89 90 91 92 def final_s ( n : int , word : str ) -> str : \"\"\" pluralizes word if n > 1 Args: n: how many times word: to be pluralized, maybe Returns: `1 word` or `n words` \"\"\" suffix = \"s\" if n > 1 else \"\" return f \" { n } { word }{ suffix } \"","title":"final_s()"},{"location":"bsutils.html#bs_python_utils.bsutils.find_first","text":"Returns the index of and the first item in the iterable that satisfies the condition . Parameters: Name Type Description Default iterable Iterable where to look required condition Callable must return a boolean lambda x: True Returns: Type Description Any If the condition is not given, returns 0 and the first item of Any the iterable. Any Raises StopIteration if no item satisfyng the condition is found. Examples: >>> find_first ( ( 1 , 2 , 3 ), condition = lambda x : x % 2 == 0 ) (1, 2) >>> find_first ( range ( 3 , 100 )) (0, 3) >>> find_first ( () ) Traceback (most recent call last): ... StopIteration Source code in bs_python_utils/bsutils.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def find_first ( iterable : Iterable , condition : Callable = lambda x : True ) -> Any : \"\"\" Returns the index of and the first item in the `iterable` that satisfies the `condition`. Args: iterable: where to look condition: must return a boolean Returns: If the condition is not given, returns 0 and the first item of the iterable. Raises `StopIteration` if no item satisfyng the condition is found. Examples: >>> find_first( (1,2,3), condition=lambda x: x % 2 == 0) (1, 2) >>> find_first(range(3, 100)) (0, 3) >>> find_first( () ) Traceback (most recent call last): ... StopIteration \"\"\" return next (( i , x ) for i , x in enumerate ( iterable ) if condition ( x ))","title":"find_first()"},{"location":"bsutils.html#bs_python_utils.bsutils.fstring_integer_with_significant_digits","text":"returns an f-string with number rounded to m significant digits Parameters: Name Type Description Default number int the integer we want to round required m int how many digits we keep; the rest is filled with zeroes required Return a string with the rounded integer Examples: fstring_integer_with_significant_digits(12345, 0) ' 0' fstring_integer_with_significant_digits(12345, 3) '12,300' fstring_integer_with_significant_digits(12345, 7) '12345' Source code in bs_python_utils/bsutils.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def fstring_integer_with_significant_digits ( number : int , m : int ) -> str : \"\"\"returns an f-string with `number` rounded to `m` significant digits Args: number: the integer we want to round m: how many digits we keep; the rest is filled with zeroes Return: a string with the rounded integer Examples: >>> fstring_integer_with_significant_digits(12345, 0) ' 0' >>> fstring_integer_with_significant_digits(12345, 3) '12,300' >>> fstring_integer_with_significant_digits(12345, 7) '12345' \"\"\" if ( not isinstance ( number , int )) or ( not isinstance ( m , int )): bs_error_abort ( \"Both arguments should be integers.\" ) if number == 0 : return f \" { number : d } \" else : digits = len ( str ( abs ( int ( number )))) if digits <= m : return f \" { int ( number ) : d } \" else : power = digits - m rounded = round ( number / 10 ** power ) * 10 ** power return f \" { int ( rounded ) : ,d } \"","title":"fstring_integer_with_significant_digits()"},{"location":"bsutils.html#bs_python_utils.bsutils.mkdir_if_needed","text":"create the directory if it does not exist Parameters: Name Type Description Default p Path | str a path required Returns: Type Description Path the directory Path Source code in bs_python_utils/bsutils.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def mkdir_if_needed ( p : Path | str ) -> Path : \"\"\" create the directory if it does not exist Args: p: a path Returns: the directory Path \"\"\" try : q = Path ( p ) except OSError : bs_error_abort ( f \" { p } is not a path\" ) if not q . exists (): q . mkdir ( parents = True ) return q","title":"mkdir_if_needed()"},{"location":"bsutils.html#bs_python_utils.bsutils.print_stars","text":"prints a title within stars Parameters: Name Type Description Default title str title None n int number of stars on line 70 Returns: Type Description None prints a starred line, or two around the title Source code in bs_python_utils/bsutils.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def print_stars ( title : str = None , n : int = 70 ) -> None : \"\"\" prints a title within stars Args: title: title n: number of stars on line Returns: prints a starred line, or two around the title \"\"\" line_stars = \"*\" * n print () print ( line_stars ) if title : print ( title . center ( n )) print ( line_stars ) print ()","title":"print_stars()"},{"location":"bsutils.html#bs_python_utils.bsutils.printargs","text":"Decorator that reports the arguments of the function Parameters: Name Type Description Default func Callable the decorated function required Source code in bs_python_utils/bsutils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def printargs ( func : Callable ) -> Callable : \"\"\" Decorator that reports the arguments of the function Args: func: the decorated function \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): print ( f \"Function { func . __name__ } called with args = { args } and kwargs = { kwargs } \" ) return func ( * args , ** kwargs ) return wrapper","title":"printargs()"},{"location":"chebyshev.html","text":"chebyshev module \u00b6 Chebyshev interpolation and integration in 1, 2, and 4 dimensions Note if the math looks strange in the documentation, just reload the page. Interval , Rectangle : basic classes to define the integration domain move_from1m1, move_to1m1 : rescale to and from the \\([-1,1]\\) interval cheb_get_nodes_1d : get Chebyshev nodes and weights on an interval cheb_eval_fun_at_nodes_1d : evaluates a function at is nodes on an interval cheb_get_coefficients_1d : get the Chebyshev coefficients for a function cheb_interp_1d : interpolate a function on an interval given its definition or its coefficients cheb_interp_1d_from_nodes : interpolate a function on an interval given its values at the nodes cheb_find_root : finds the roots of a function in an interval cheb_integrate_from_coeffs_1d : integrates a function given its coefficients cheb_integrate_from_nodes_1d : integrates a function given its values at the nodes (less precise) cheb_get_nodes_2d : get Chebyshev nodes and weights on a rectangle cheb_eval_fun_at_nodes_2d : evaluates a function at is nodes on a rectangle cheb_get_coefficients_2d : get the Chebyshev coefficients for a function of 2 arguments cheb_interp_2d : interpolate a function on a rectangle given its definition or its coefficients cheb_interp_2d_from_nodes : interpolate a function on a rectangle given its values at the nodes cheb_integrate_from_nodes_4d : integrate over a product of rectangles given values at the tensor products of the 2d nodes. Interval dataclass \u00b6 a real interval \\([x_0,x_1]\\) Source code in bs_python_utils/chebyshev.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @dataclass class Interval : \"\"\"a real interval $[x_0,x_1]$\"\"\" x0 : float x1 : float def __post_init__ ( self ): x0 , x1 = self . x0 , self . x1 if x0 > x1 : bs_error_abort ( f \"x0 = { x0 } is larger than x1 = { x1 } \" ) def bounds ( self ): return self . x0 , self . x1 Rectangle dataclass \u00b6 a product interval \\([x_0,x_1] imes [y_0, y_1]\\) Source code in bs_python_utils/chebyshev.py 57 58 59 60 61 62 @dataclass class Rectangle : \"\"\"a product interval $[x_0,x_1] \\times [y_0, y_1]$\"\"\" x_interval : Interval y_interval : Interval cheb_eval_fun_at_nodes_1d ( fun , nodes = None , interval = None , degree = None ) \u00b6 evaluate a function at the Chebyshev nodes on an interval Parameters: Name Type Description Default fun ArrayFunctionOfArray the function to evaluate on an interval required nodes np . ndarray | None the Chebyshev nodes on that interval, if precomputed None interval Interval | None the Interval None degree int | None the degree of the Chebyshev expansion None Notes interval , degree are required if nodes is not provided Returns: Type Description np . ndarray the values of the function at the Chebyshev nodes Source code in bs_python_utils/chebyshev.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def cheb_eval_fun_at_nodes_1d ( fun : ArrayFunctionOfArray , nodes : np . ndarray | None = None , interval : Interval | None = None , degree : int | None = None , ) -> np . ndarray : \"\"\"evaluate a function at the Chebyshev nodes on an interval Args: fun: the function to evaluate on an interval nodes: the Chebyshev nodes on that interval, if precomputed interval: the Interval degree: the degree of the Chebyshev expansion Notes: `interval`, `degree` are required if `nodes` is not provided Returns: the values of the function at the Chebyshev nodes \"\"\" if nodes is None : if degree is None or interval is None : bs_error_abort ( \"if nodes is not provided, then degree and interval must be\" ) degree = cast ( int , degree ) interval = cast ( Interval , interval ) nodes , _ = cheb_get_nodes_1d ( interval , degree ) vals_at_nodes = fun ( nodes ) return vals_at_nodes cheb_eval_fun_at_nodes_2d ( fun , nodes = None , rectangle = None , degree = None ) \u00b6 evaluate a function at the Chebyshev nodes on a rectangle $ Parameters: Name Type Description Default fun ArrayFunctionOfArray the function to evaluate on the rectangle required nodes np . ndarray | None the Chebyshev nodes on that rectangle, if precomputed None rectangle Rectangle | None the Rectangle None degree int | None the degree of the Chebyshev expansion in each dimension None Notes rectangle and degree are required if nodes is not provided Returns: Type Description np . ndarray the values of the function at the Chebyshev nodes Source code in bs_python_utils/chebyshev.py 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def cheb_eval_fun_at_nodes_2d ( fun : ArrayFunctionOfArray , nodes : np . ndarray | None = None , rectangle : Rectangle | None = None , degree : int | None = None , ) -> np . ndarray : \"\"\"evaluate a function at the Chebyshev nodes on a rectangle $ Args: fun: the function to evaluate on the rectangle nodes: the Chebyshev nodes on that rectangle, if precomputed rectangle: the Rectangle degree: the degree of the Chebyshev expansion in each dimension Notes: `rectangle` and `degree` are required if `nodes` is not provided Returns: the values of the function at the Chebyshev nodes \"\"\" if nodes is None : if degree is None or rectangle is None : bs_error_abort ( \"if nodes is not provided, then degree and rectangle must be\" ) degree = cast ( int , degree ) rectangle = cast ( Rectangle , rectangle ) nodes , _ = cheb_get_nodes_2d ( rectangle , degree ) vals_at_nodes = fun ( nodes ) return vals_at_nodes cheb_find_root ( f , degree , interval = None ) \u00b6 find the roots of \\(f(x)=0\\) in \\([0,1]\\) ; also return the one(s) within the interval, if given Parameters: Name Type Description Default f ArrayFunctionOfArray the function required degree int the degree of the Chebyshev expansion required interval Interval | None the interval where we want the root None Returns: Type Description np . ndarray | tuple [ np . ndarray , np . ndarray | float | None] the roots in \\([0,1]\\) ; and the one(s) in interval , if specified Source code in bs_python_utils/chebyshev.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def cheb_find_root ( f : ArrayFunctionOfArray , degree : int , interval : Interval | None = None ) -> np . ndarray | tuple [ np . ndarray , np . ndarray | float | None ]: \"\"\"find the roots of $f(x)=0$ in $[0,1]$; also return the one(s) within the interval, if given Args: f: the function degree: the degree of the Chebyshev expansion interval: the interval where we want the root Returns: the roots in $[0,1]$; and the one(s) in `interval`, if specified \"\"\" interval01 = Interval ( 0.0 , 1.0 ) coeffs_f = cheb_get_coefficients_1d ( f , interval01 , degree ) roots = cast ( np . ndarray , move_from1m1 ( ncheb . chebroots ( coeffs_f ), interval01 )) if interval : roots_in_interval = roots [( roots >= interval . x0 ) & ( roots <= interval . x1 )] if len ( roots_in_interval ) == 0 : return roots , None elif len ( roots_in_interval ) == 1 : return roots , roots_in_interval [ 0 ] else : return roots , roots_in_interval else : return roots cheb_get_coefficients_1d ( fun , interval , degree ) \u00b6 get the Chebyshev coefficients for fun on an interval Parameters: Name Type Description Default fun ArrayFunctionOfArray the function required interval Interval the Interval required degree int the degree of the Chebyshev expansion required Returns: Type Description np . ndarray a degree -vector of coefficients Source code in bs_python_utils/chebyshev.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def cheb_get_coefficients_1d ( fun : ArrayFunctionOfArray , interval : Interval , degree : int ) -> np . ndarray : \"\"\"get the Chebyshev coefficients for `fun` on an interval Args: fun: the function interval: the Interval degree: the degree of the Chebyshev expansion Returns: a `degree`-vector of coefficients \"\"\" def fun_t ( t : np . ndarray ) -> np . ndarray : x = cast ( np . ndarray , move_from1m1 ( t , interval )) return fun ( x ) c = ncheb . chebinterpolate ( fun_t , degree ) return cast ( np . ndarray , c ) cheb_get_coefficients_2d ( rectangle , degree , vals_at_nodes = None , fun = None ) \u00b6 get the Chebyshev coefficients for fun on a rectangle, using an OLS fit on the values on the grid of nodes Parameters: Name Type Description Default rectangle Rectangle the Rectangle required degree int the degree of the Chebyshev expansion in each dimension required vals_at_nodes np . ndarray | None the values on the grid, if precomputed None fun ArrayFunctionOfArray | None the function None Notes if vals_at-nodes is not provided then fun must be. Returns: Type Description np . ndarray the Chebyshev coefficients of the OLS Chebyshev fit, an (M,M) matrix np . ndarray the approximation is \\(f(x_1,x_2) = \\sum_{k,l} c_{kl} T_k(x_1)T_l(x_2)\\) Source code in bs_python_utils/chebyshev.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 def cheb_get_coefficients_2d ( rectangle : Rectangle , degree : int , vals_at_nodes : np . ndarray | None = None , fun : ArrayFunctionOfArray | None = None , ) -> np . ndarray : \"\"\"get the Chebyshev coefficients for `fun` on a rectangle, using an OLS fit on the values on the grid of nodes Args: rectangle: the Rectangle degree: the degree of the Chebyshev expansion in each dimension vals_at_nodes: the values on the grid, if precomputed fun: the function Notes: if `vals_at-nodes` is not provided then `fun` must be. Returns: the Chebyshev coefficients of the OLS Chebyshev fit, an `(M,M)` matrix the approximation is $f(x_1,x_2) = \\\\sum_{k,l} c_{kl} T_k(x_1)T_l(x_2)$ \"\"\" if vals_at_nodes is None : if fun is None : bs_error_abort ( \"vals_at_nodes was not provided, so fun must be.\" ) fun = cast ( ArrayFunctionOfArray , fun ) vals_at_nodes = cheb_eval_fun_at_nodes_2d ( fun , rectangle = rectangle , degree = degree ) # we need the nodes on $[-1, 1]$ interval_1m1 = Interval ( x0 =- 1.0 , x1 = 1.0 ) nodes1m1 , _ = cheb_get_nodes_1d ( interval_1m1 , degree ) # first we fit fixing the node on the first dimension c = np . zeros (( degree , degree )) c_bar = np . zeros (( degree , degree )) i_beg = 0 for i in range ( degree ): i_end = i_beg + degree vals_for_i = vals_at_nodes [ i_beg : i_end ] c_bar [ i , :] = ncheb . chebfit ( nodes1m1 , vals_for_i , degree - 1 ) i_beg = i_end # then we fit to the values of c_bar for k in range ( degree ): c [:, k ] = ncheb . chebfit ( nodes1m1 , c_bar [:, k ], degree - 1 ) return c cheb_get_nodes_1d ( interval , degree ) \u00b6 get the Chebyshev nodes and weights on the interval \\([x0, x1]\\) Parameters: Name Type Description Default interval Interval the Interval \\([x_0, x_1]\\) required degree int the degree of the highest Chebyshev polynomial required Returns: Type Description TwoArrays two degree -vectors of Chebyshev nodes and weights on the interval \\([x_0, x_1]\\) TwoArrays so that g(nodes) @ weights approximates the unweighted integral of \\(g(x)\\) on \\([x_0,x_1]\\) Source code in bs_python_utils/chebyshev.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def cheb_get_nodes_1d ( interval : Interval , degree : int ) -> TwoArrays : \"\"\"get the Chebyshev nodes and weights on the interval $[x0, x1]$ Args: interval: the Interval $[x_0, x_1]$ degree: the degree of the highest Chebyshev polynomial Returns: two `degree`-vectors of Chebyshev nodes and weights on the interval $[x_0, x_1]$ so that `g(nodes) @ weights` approximates the unweighted integral of $g(x)$ on $[x_0,x_1]$ \"\"\" nodes1m1 , weights1m1 = ncheb . chebgauss ( degree ) # om=n [-1, 1] x0 , x1 = interval . bounds () nodes = move_from1m1 ( nodes1m1 , interval ) weights = ( x1 - x0 ) * weights1m1 * np . sqrt ( 1.0 - nodes1m1 * nodes1m1 ) / 2.0 return cast ( np . ndarray , nodes ), cast ( np . ndarray , weights ) cheb_get_nodes_2d ( rectangle , degree ) \u00b6 get the Chebyshev nodes and weights on a rectangle Parameters: Name Type Description Default rectangle Rectangle the Rectangle required degree int the degree of the highest Chebyshev polynomial in each dimension required Returns: Type Description TwoArrays two \\(( ext{degree}^2, 2)\\) `-matrices of Chebyshev nodes and weights TwoArrays on the rectangle \\([x0, x1] imes [y0, y1]\\) Source code in bs_python_utils/chebyshev.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def cheb_get_nodes_2d ( rectangle : Rectangle , degree : int ) -> TwoArrays : \"\"\"get the Chebyshev nodes and weights on a rectangle Args: rectangle: the Rectangle degree: the degree of the highest Chebyshev polynomial in each dimension Returns: two $(\\text{degree}^2, 2)$`-matrices of Chebyshev nodes and weights on the rectangle $[x0, x1]\\times [y0, y1]$ \"\"\" nodes1d_x , weights1d_x = cheb_get_nodes_1d ( rectangle . x_interval , degree ) nodes1d_y , weights1d_y = cheb_get_nodes_1d ( rectangle . y_interval , degree ) nodes2d = bsgrid ( nodes1d_x , nodes1d_y ) weights2d = bsgrid ( weights1d_x , weights1d_y ) return nodes2d , weights2d [:, 0 ] * weights2d [:, 1 ] cheb_integrate_from_coeffs_1d ( c , interval ) \u00b6 integrate a function on an interval using the coefficients of its Chebyshev expansion Parameters: Name Type Description Default c np . ndarray the Chebyshev coefficients for fun required interval Interval the Interval required Returns: Type Description float the value of the integral over the interval Source code in bs_python_utils/chebyshev.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def cheb_integrate_from_coeffs_1d ( c : np . ndarray , interval : Interval ) -> float : \"\"\"integrate a function on an interval using the coefficients of its Chebyshev expansion Args: c: the Chebyshev coefficients for `fun` interval: the Interval Returns: the value of the integral over the interval \"\"\" x0 , x1 = interval . bounds () Mc = check_vector ( c ) k_even = slice ( 0 , Mc , 2 ) k_vals = np . arange ( 0 , Mc , 2 ) val_integral = np . sum ( c [ k_even ] / ( 1.0 - k_vals * k_vals )) * ( x1 - x0 ) return cast ( float , val_integral ) cheb_integrate_from_coeffs_2d ( c , rectangle ) \u00b6 integrate a function on an interval using the coefficients of its Chebyshev expansion Parameters: Name Type Description Default c np . ndarray the Chebyshev coefficients for fun required rectangle Rectangle the Rectangle required Returns: Type Description float the value of the integral over the interval Source code in bs_python_utils/chebyshev.py 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def cheb_integrate_from_coeffs_2d ( c : np . ndarray , rectangle : Rectangle ) -> float : \"\"\"integrate a function on an interval using the coefficients of its Chebyshev expansion Args: c: the Chebyshev coefficients for `fun` rectangle: the Rectangle Returns: the value of the integral over the interval \"\"\" x0 , x1 = rectangle . x_interval . bounds () y0 , y1 = rectangle . y_interval . bounds () M = check_square ( c ) k_even = slice ( 0 , M , 2 ) k_vals = np . arange ( 0 , M , 2 ) denom = 1.0 - k_vals * k_vals val_integral = cast ( float , np . sum ( c [ k_even , k_even ] / np . outer ( denom , denom ))) return cast ( float , val_integral * ( x1 - x0 ) * ( y1 - y0 )) cheb_integrate_from_nodes_1d ( vals_at_nodes , weights ) \u00b6 integrate a function given its values at the Chebyshev nodes Parameters: Name Type Description Default vals_at_nodes np . ndarray the values of the function at these nodes required weights np . ndarray the Chebyshev nodes required Returns: Type Description float the value of the integral Notes this is much less precise than cheb_integrate_from_coeffs_1d Source code in bs_python_utils/chebyshev.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def cheb_integrate_from_nodes_1d ( vals_at_nodes : np . ndarray , weights : np . ndarray , ) -> float : \"\"\"integrate a function given its values at the Chebyshev nodes Args: vals_at_nodes: the values of the function at these nodes weights: the Chebyshev nodes Returns: the value of the integral Notes: this is much less precise than `cheb_integrate_from_coeffs_1d` \"\"\" Mv = check_vector ( vals_at_nodes ) Mw = check_vector ( weights ) if Mv != Mw : bs_error_abort ( f \"weights and vals_at_nodes must have the same length, not { Mw } and { Mv } \" ) return cast ( float , weights @ vals_at_nodes ) cheb_integrate_from_nodes_2d ( vals_at_nodes , weights ) \u00b6 integrate a function given its values at the Chebyshev nodes Parameters: Name Type Description Default vals_at_nodes np . ndarray the values of the function on the grid of 2d nodes required weights np . ndarray the Chebyshev weights in 2d required Returns: Type Description float the value of the integral Warning this is much less precise than cheb_integrate_from_coeffs_2d Source code in bs_python_utils/chebyshev.py 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 def cheb_integrate_from_nodes_2d ( vals_at_nodes : np . ndarray , weights : np . ndarray , ) -> float : \"\"\"integrate a function given its values at the Chebyshev nodes Args: vals_at_nodes: the values of the function on the grid of 2d nodes weights: the Chebyshev weights in 2d Returns: the value of the integral Warning: this is much less precise than `cheb_integrate_from_coeffs_2d` \"\"\" Mv = check_vector ( vals_at_nodes ) Mw = check_vector ( weights ) if Mv != Mw : bs_error_abort ( f \"weights and vals_at_nodes must have the same length, not { Mw } and { Mv } \" ) return cast ( float , vals_at_nodes @ weights ) cheb_integrate_from_nodes_4d ( vals_at_nodes4d , weights2d ) \u00b6 integrate a function on the square of a rectangle given its values at the 4d Chebyshev nodes Parameters: Name Type Description Default vals_at_nodes4d np . ndarray the values of the function on the square of the grid of 2d nodes, an \\((M^2, M^2)\\) matrix required weights2d np . ndarray the Chebyshev weights on the rectangular grid, an \\(M^2\\) -vector required Returns: Type Description float the value of the integral Warning it would be better to have a cheb_integrate_from_coeffs_4d Source code in bs_python_utils/chebyshev.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 def cheb_integrate_from_nodes_4d ( vals_at_nodes4d : np . ndarray , weights2d : np . ndarray ) -> float : \"\"\"integrate a function on the square of a rectangle given its values at the 4d Chebyshev nodes Args: vals_at_nodes4d: the values of the function on the square of the grid of 2d nodes, an $(M^2, M^2)$ matrix weights2d: the Chebyshev weights on the rectangular grid, an $M^2$-vector Returns: the value of the integral Warning: it would be better to have a `cheb_integrate_from_coeffs_4d` \"\"\" Mv2 = check_square ( vals_at_nodes4d ) Mw2 = check_vector ( weights2d ) if Mv2 != Mw2 : bs_error_abort ( \"weights2d and vals_at_nodes4d should have the same number of rows, not\" f \" { Mv2 } and { Mw2 } \" ) return cast ( float , weights2d @ ( vals_at_nodes4d @ weights2d )) cheb_interp_1d ( x_vals , interval , c = None , fun = None , degree = None ) \u00b6 interpolate a function on on interval using Chebyshev polynomials Parameters: Name Type Description Default x_vals np . ndarray the values at which to interpolate required interval Interval the Interval required c np . ndarray | None the Chebyshev coefficients for fun , if already known; otherwise we compute them None fun ArrayFunctionOfArray | None the function to interpolate None degree int | None the degree of the Chebyshev expansion None Notes fun and degree are required if c is not provided Returns: Type Description TwoArrays the values of the interpolation at x_vals and the Chebyshev coefficients c Source code in bs_python_utils/chebyshev.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def cheb_interp_1d ( x_vals : np . ndarray , interval : Interval , c : np . ndarray | None = None , fun : ArrayFunctionOfArray | None = None , degree : int | None = None , ) -> TwoArrays : \"\"\"interpolate a function on on interval using Chebyshev polynomials Args: x_vals: the values at which to interpolate interval: the Interval c: the Chebyshev coefficients for `fun`, if already known; otherwise we compute them fun: the function to interpolate degree: the degree of the Chebyshev expansion Notes: `fun` and `degree` are required if `c` is not provided Returns: the values of the interpolation at `x_vals` and the Chebyshev coefficients `c` \"\"\" if c is None : if degree is None or fun is None : bs_error_abort ( \"since c is not provided, fun and degree must be.\" ) degree = cast ( int , degree ) fun = cast ( ArrayFunctionOfArray , fun ) c = cheb_get_coefficients_1d ( fun , interval , degree ) y_vals = ncheb . chebval ( move_to1m1 ( x_vals , interval ), c ) return y_vals , c cheb_interp_1d_from_nodes ( f_vals_at_nodes , x , interval = None ) \u00b6 interpolate \\(f(x)\\) given the values \\(f(x_m)\\) for \\(m=1,\\ldots,M^2\\) at the Chebyshev nodes on an intervak Parameters: Name Type Description Default f_vals_at_nodes np . ndarray an \\(M^2\\) vector of values \\(f(x_m)\\) required x np . ndarray a scalar where we want \\(f(x)\\) required interval Interval | None the interval on which the function acts; by default, \\([0,1]\\) None Returns: Type Description float the interpolated value of \\(f(x)\\) . Source code in bs_python_utils/chebyshev.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def cheb_interp_1d_from_nodes ( f_vals_at_nodes : np . ndarray , x : np . ndarray , interval : Interval | None = None ) -> float : \"\"\"interpolate $f(x)$ given the values $f(x_m)$ for $m=1,\\\\ldots,M^2$ at the Chebyshev nodes on an intervak Args: f_vals_at_nodes: an $M^2$ vector of values $f(x_m)$ x: a scalar where we want $f(x)$ interval: the interval on which the function acts; by default, $[0,1]$ Returns: the interpolated value of $f(x)$. \"\"\" if interval is None : interval = Interval ( x0 = 0.0 , x1 = 1.0 ) # we need the nodes on $[-1, 1]$ interval_1m1 = Interval ( x0 =- 1.0 , x1 = 1.0 ) degree = f_vals_at_nodes . size nodes1m1 , _ = cheb_get_nodes_1d ( interval_1m1 , degree ) coeffs_f = ncheb . chebfit ( nodes1m1 , f_vals_at_nodes , degree - 1 ) f_x , _ = cheb_interp_1d ( x , interval , c = coeffs_f ) return cast ( float , f_x ) cheb_interp_2d ( xy_vals , rectangle , c = None , fun = None , degree = None , vals_at_nodes = None ) \u00b6 interpolate a function on a rectangle using Chebyshev polynomials Parameters: Name Type Description Default xy_vals np . ndarray the values at which to interpolate, an (n,2) matrix or a 2-vector required rectangle Rectangle the Rectangle required c np . ndarray | None the Chebyshev coefficients for fun , if already known; otherwise we compute them None fun ArrayFunctionOfArray | None the function to interpolate None degree int | None the degree of the Chebyshev expansion None vals_at_nodes np . ndarray | None the values on the grid, if precomputed None Notes degree is required if c is not provided, as well as either fun or vals_at_nodes Returns: Type Description TwoArrays | tuple [ float , np . ndarray ] the n -vector of values of the interpolation at xy_vals and the Chebyshev coefficients c Source code in bs_python_utils/chebyshev.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def cheb_interp_2d ( xy_vals : np . ndarray , rectangle : Rectangle , c : np . ndarray | None = None , fun : ArrayFunctionOfArray | None = None , degree : int | None = None , vals_at_nodes : np . ndarray | None = None , ) -> TwoArrays | tuple [ float , np . ndarray ]: \"\"\"interpolate a function on a rectangle using Chebyshev polynomials Args: xy_vals: the values at which to interpolate, an `(n,2)` matrix or a 2-vector rectangle: the Rectangle c: the Chebyshev coefficients for `fun`, if already known; otherwise we compute them fun: the function to interpolate degree: the degree of the Chebyshev expansion vals_at_nodes: the values on the grid, if precomputed Notes: `degree` is required if `c` is not provided, as well as either `fun` or `vals_at_nodes` Returns: the `n`-vector of values of the interpolation at `xy_vals` and the Chebyshev coefficients `c` \"\"\" if c is None : if degree is None : bs_error_abort ( \"either c or degree must be provided\" ) degree = cast ( int , degree ) c = cheb_get_coefficients_2d ( rectangle , degree , vals_at_nodes = vals_at_nodes , fun = fun ) # transform xy_vals to $[-1,1]\\times [-1,1]$ xy_vals1 = np . zeros_like ( xy_vals ) if xy_vals . ndim == 2 : xy_vals1 [:, 0 ] = move_to1m1 ( xy_vals [:, 0 ], rectangle . x_interval ) xy_vals1 [:, 1 ] = move_to1m1 ( xy_vals [:, 1 ], rectangle . y_interval ) n_vals = xy_vals . shape [ 0 ] deg = c . shape [ 0 ] c2 = np . zeros (( deg , n_vals )) for k , c_k in enumerate ( c ): c2 [ k , :] = ncheb . chebval ( xy_vals1 [:, 1 ], c_k ) f_vals = np . zeros ( n_vals ) for i in range ( n_vals ): f_vals [ i ] = ncheb . chebval ( xy_vals1 [ i , 0 ], c2 [:, i ]) else : xy_vals1 [ 0 ] = move_to1m1 ( xy_vals [ 0 ], rectangle . x_interval ) xy_vals1 [ 1 ] = move_to1m1 ( xy_vals [ 1 ], rectangle . y_interval ) deg = c . shape [ 0 ] c2 = np . zeros ( deg ) for k , c_k in enumerate ( c ): c2 [ k ] = ncheb . chebval ( xy_vals1 [ 1 ], c_k ) f_vals = ncheb . chebval ( xy_vals1 [ 0 ], c2 ) return f_vals , c cheb_interp_2d_from_nodes ( f_vals_at_nodes , x , rectangle = None ) \u00b6 interpolate \\(f(x)\\) given the values \\(f(x_m)\\) for \\(m=1,\\ldots,M^2\\) at the Chebyshev nodes on a rectangle Parameters: Name Type Description Default f_vals_at_nodes np . ndarray an \\(M^2\\) vector of values \\(f(x_m)\\) required x np . ndarray a 2-vector where we want \\(f(x)\\) required rectangle Rectangle | None the rectangle on which the function acts; by default, \\([0,1]^2\\) None Returns: Type Description float the interpolated value of \\(f(x)\\) . Source code in bs_python_utils/chebyshev.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 def cheb_interp_2d_from_nodes ( f_vals_at_nodes : np . ndarray , x : np . ndarray , rectangle : Rectangle | None = None ) -> float : \"\"\"interpolate $f(x)$ given the values $f(x_m)$ for $m=1,\\\\ldots,M^2$ at the Chebyshev nodes on a rectangle Args: f_vals_at_nodes: an $M^2$ vector of values $f(x_m)$ x: a 2-vector where we want $f(x)$ rectangle: the rectangle on which the function acts; by default, $[0,1]^2$ Returns: the interpolated value of $f(x)$. \"\"\" if rectangle is None : interval01 = Interval ( x0 = 0.0 , x1 = 1.0 ) rectangle = Rectangle ( x_interval = interval01 , y_interval = interval01 ) degree = round ( sqrt ( f_vals_at_nodes . size )) coeffs_f = cheb_get_coefficients_2d ( rectangle , degree , vals_at_nodes = f_vals_at_nodes ) f_x , _ = cheb_interp_2d ( x , rectangle , c = coeffs_f ) return cast ( float , f_x ) move_from1m1 ( t , interval ) \u00b6 get the position of t in \\([-1,1]\\) and move it in the same position in interval Parameters: Name Type Description Default t FloatOrArray position(s) within ` \\([-1,1]\\) required interval Interval the Interval required Returns: Type Description FloatOrArray x rescaled to \\([-1,1]\\) Source code in bs_python_utils/chebyshev.py 65 66 67 68 69 70 71 72 73 74 75 76 def move_from1m1 ( t : FloatOrArray , interval : Interval ) -> FloatOrArray : \"\"\"get the position of `t` in $[-1,1]$ and move it in the same position in `interval` Args: t: position(s) within `$[-1,1]$ interval: the Interval Returns: `x` rescaled to $[-1,1]$ \"\"\" x0 , x1 = interval . bounds () return cast ( FloatOrArray , (( x1 - x0 ) * t + ( x0 + x1 )) / 2.0 ) move_to1m1 ( x , interval ) \u00b6 get the position of x in interval and move it to the same position in \\([-1,1]\\) Parameters: Name Type Description Default x FloatOrArray position(s) within interval required interval Interval the Interval required Returns: Type Description FloatOrArray x rescaled to \\([-1,1]\\) Source code in bs_python_utils/chebyshev.py 79 80 81 82 83 84 85 86 87 88 89 90 def move_to1m1 ( x : FloatOrArray , interval : Interval ) -> FloatOrArray : \"\"\"get the position of `x` in `interval` and move it to the same position in $[-1,1]$ Args: x: position(s) within `interval` interval: the Interval Returns: `x` rescaled to $[-1,1]$ \"\"\" x0 , x1 = interval . bounds () return cast ( FloatOrArray , ( 2.0 * x - ( x0 + x1 )) / ( x1 - x0 ))","title":"Chebyshev interpolation and integration"},{"location":"chebyshev.html#chebyshev-module","text":"Chebyshev interpolation and integration in 1, 2, and 4 dimensions Note if the math looks strange in the documentation, just reload the page. Interval , Rectangle : basic classes to define the integration domain move_from1m1, move_to1m1 : rescale to and from the \\([-1,1]\\) interval cheb_get_nodes_1d : get Chebyshev nodes and weights on an interval cheb_eval_fun_at_nodes_1d : evaluates a function at is nodes on an interval cheb_get_coefficients_1d : get the Chebyshev coefficients for a function cheb_interp_1d : interpolate a function on an interval given its definition or its coefficients cheb_interp_1d_from_nodes : interpolate a function on an interval given its values at the nodes cheb_find_root : finds the roots of a function in an interval cheb_integrate_from_coeffs_1d : integrates a function given its coefficients cheb_integrate_from_nodes_1d : integrates a function given its values at the nodes (less precise) cheb_get_nodes_2d : get Chebyshev nodes and weights on a rectangle cheb_eval_fun_at_nodes_2d : evaluates a function at is nodes on a rectangle cheb_get_coefficients_2d : get the Chebyshev coefficients for a function of 2 arguments cheb_interp_2d : interpolate a function on a rectangle given its definition or its coefficients cheb_interp_2d_from_nodes : interpolate a function on a rectangle given its values at the nodes cheb_integrate_from_nodes_4d : integrate over a product of rectangles given values at the tensor products of the 2d nodes.","title":"chebyshev module"},{"location":"chebyshev.html#bs_python_utils.chebyshev.Interval","text":"a real interval \\([x_0,x_1]\\) Source code in bs_python_utils/chebyshev.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @dataclass class Interval : \"\"\"a real interval $[x_0,x_1]$\"\"\" x0 : float x1 : float def __post_init__ ( self ): x0 , x1 = self . x0 , self . x1 if x0 > x1 : bs_error_abort ( f \"x0 = { x0 } is larger than x1 = { x1 } \" ) def bounds ( self ): return self . x0 , self . x1","title":"Interval"},{"location":"chebyshev.html#bs_python_utils.chebyshev.Rectangle","text":"a product interval \\([x_0,x_1] imes [y_0, y_1]\\) Source code in bs_python_utils/chebyshev.py 57 58 59 60 61 62 @dataclass class Rectangle : \"\"\"a product interval $[x_0,x_1] \\times [y_0, y_1]$\"\"\" x_interval : Interval y_interval : Interval","title":"Rectangle"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_eval_fun_at_nodes_1d","text":"evaluate a function at the Chebyshev nodes on an interval Parameters: Name Type Description Default fun ArrayFunctionOfArray the function to evaluate on an interval required nodes np . ndarray | None the Chebyshev nodes on that interval, if precomputed None interval Interval | None the Interval None degree int | None the degree of the Chebyshev expansion None Notes interval , degree are required if nodes is not provided Returns: Type Description np . ndarray the values of the function at the Chebyshev nodes Source code in bs_python_utils/chebyshev.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def cheb_eval_fun_at_nodes_1d ( fun : ArrayFunctionOfArray , nodes : np . ndarray | None = None , interval : Interval | None = None , degree : int | None = None , ) -> np . ndarray : \"\"\"evaluate a function at the Chebyshev nodes on an interval Args: fun: the function to evaluate on an interval nodes: the Chebyshev nodes on that interval, if precomputed interval: the Interval degree: the degree of the Chebyshev expansion Notes: `interval`, `degree` are required if `nodes` is not provided Returns: the values of the function at the Chebyshev nodes \"\"\" if nodes is None : if degree is None or interval is None : bs_error_abort ( \"if nodes is not provided, then degree and interval must be\" ) degree = cast ( int , degree ) interval = cast ( Interval , interval ) nodes , _ = cheb_get_nodes_1d ( interval , degree ) vals_at_nodes = fun ( nodes ) return vals_at_nodes","title":"cheb_eval_fun_at_nodes_1d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_eval_fun_at_nodes_2d","text":"evaluate a function at the Chebyshev nodes on a rectangle $ Parameters: Name Type Description Default fun ArrayFunctionOfArray the function to evaluate on the rectangle required nodes np . ndarray | None the Chebyshev nodes on that rectangle, if precomputed None rectangle Rectangle | None the Rectangle None degree int | None the degree of the Chebyshev expansion in each dimension None Notes rectangle and degree are required if nodes is not provided Returns: Type Description np . ndarray the values of the function at the Chebyshev nodes Source code in bs_python_utils/chebyshev.py 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def cheb_eval_fun_at_nodes_2d ( fun : ArrayFunctionOfArray , nodes : np . ndarray | None = None , rectangle : Rectangle | None = None , degree : int | None = None , ) -> np . ndarray : \"\"\"evaluate a function at the Chebyshev nodes on a rectangle $ Args: fun: the function to evaluate on the rectangle nodes: the Chebyshev nodes on that rectangle, if precomputed rectangle: the Rectangle degree: the degree of the Chebyshev expansion in each dimension Notes: `rectangle` and `degree` are required if `nodes` is not provided Returns: the values of the function at the Chebyshev nodes \"\"\" if nodes is None : if degree is None or rectangle is None : bs_error_abort ( \"if nodes is not provided, then degree and rectangle must be\" ) degree = cast ( int , degree ) rectangle = cast ( Rectangle , rectangle ) nodes , _ = cheb_get_nodes_2d ( rectangle , degree ) vals_at_nodes = fun ( nodes ) return vals_at_nodes","title":"cheb_eval_fun_at_nodes_2d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_find_root","text":"find the roots of \\(f(x)=0\\) in \\([0,1]\\) ; also return the one(s) within the interval, if given Parameters: Name Type Description Default f ArrayFunctionOfArray the function required degree int the degree of the Chebyshev expansion required interval Interval | None the interval where we want the root None Returns: Type Description np . ndarray | tuple [ np . ndarray , np . ndarray | float | None] the roots in \\([0,1]\\) ; and the one(s) in interval , if specified Source code in bs_python_utils/chebyshev.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def cheb_find_root ( f : ArrayFunctionOfArray , degree : int , interval : Interval | None = None ) -> np . ndarray | tuple [ np . ndarray , np . ndarray | float | None ]: \"\"\"find the roots of $f(x)=0$ in $[0,1]$; also return the one(s) within the interval, if given Args: f: the function degree: the degree of the Chebyshev expansion interval: the interval where we want the root Returns: the roots in $[0,1]$; and the one(s) in `interval`, if specified \"\"\" interval01 = Interval ( 0.0 , 1.0 ) coeffs_f = cheb_get_coefficients_1d ( f , interval01 , degree ) roots = cast ( np . ndarray , move_from1m1 ( ncheb . chebroots ( coeffs_f ), interval01 )) if interval : roots_in_interval = roots [( roots >= interval . x0 ) & ( roots <= interval . x1 )] if len ( roots_in_interval ) == 0 : return roots , None elif len ( roots_in_interval ) == 1 : return roots , roots_in_interval [ 0 ] else : return roots , roots_in_interval else : return roots","title":"cheb_find_root()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_get_coefficients_1d","text":"get the Chebyshev coefficients for fun on an interval Parameters: Name Type Description Default fun ArrayFunctionOfArray the function required interval Interval the Interval required degree int the degree of the Chebyshev expansion required Returns: Type Description np . ndarray a degree -vector of coefficients Source code in bs_python_utils/chebyshev.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def cheb_get_coefficients_1d ( fun : ArrayFunctionOfArray , interval : Interval , degree : int ) -> np . ndarray : \"\"\"get the Chebyshev coefficients for `fun` on an interval Args: fun: the function interval: the Interval degree: the degree of the Chebyshev expansion Returns: a `degree`-vector of coefficients \"\"\" def fun_t ( t : np . ndarray ) -> np . ndarray : x = cast ( np . ndarray , move_from1m1 ( t , interval )) return fun ( x ) c = ncheb . chebinterpolate ( fun_t , degree ) return cast ( np . ndarray , c )","title":"cheb_get_coefficients_1d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_get_coefficients_2d","text":"get the Chebyshev coefficients for fun on a rectangle, using an OLS fit on the values on the grid of nodes Parameters: Name Type Description Default rectangle Rectangle the Rectangle required degree int the degree of the Chebyshev expansion in each dimension required vals_at_nodes np . ndarray | None the values on the grid, if precomputed None fun ArrayFunctionOfArray | None the function None Notes if vals_at-nodes is not provided then fun must be. Returns: Type Description np . ndarray the Chebyshev coefficients of the OLS Chebyshev fit, an (M,M) matrix np . ndarray the approximation is \\(f(x_1,x_2) = \\sum_{k,l} c_{kl} T_k(x_1)T_l(x_2)\\) Source code in bs_python_utils/chebyshev.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 def cheb_get_coefficients_2d ( rectangle : Rectangle , degree : int , vals_at_nodes : np . ndarray | None = None , fun : ArrayFunctionOfArray | None = None , ) -> np . ndarray : \"\"\"get the Chebyshev coefficients for `fun` on a rectangle, using an OLS fit on the values on the grid of nodes Args: rectangle: the Rectangle degree: the degree of the Chebyshev expansion in each dimension vals_at_nodes: the values on the grid, if precomputed fun: the function Notes: if `vals_at-nodes` is not provided then `fun` must be. Returns: the Chebyshev coefficients of the OLS Chebyshev fit, an `(M,M)` matrix the approximation is $f(x_1,x_2) = \\\\sum_{k,l} c_{kl} T_k(x_1)T_l(x_2)$ \"\"\" if vals_at_nodes is None : if fun is None : bs_error_abort ( \"vals_at_nodes was not provided, so fun must be.\" ) fun = cast ( ArrayFunctionOfArray , fun ) vals_at_nodes = cheb_eval_fun_at_nodes_2d ( fun , rectangle = rectangle , degree = degree ) # we need the nodes on $[-1, 1]$ interval_1m1 = Interval ( x0 =- 1.0 , x1 = 1.0 ) nodes1m1 , _ = cheb_get_nodes_1d ( interval_1m1 , degree ) # first we fit fixing the node on the first dimension c = np . zeros (( degree , degree )) c_bar = np . zeros (( degree , degree )) i_beg = 0 for i in range ( degree ): i_end = i_beg + degree vals_for_i = vals_at_nodes [ i_beg : i_end ] c_bar [ i , :] = ncheb . chebfit ( nodes1m1 , vals_for_i , degree - 1 ) i_beg = i_end # then we fit to the values of c_bar for k in range ( degree ): c [:, k ] = ncheb . chebfit ( nodes1m1 , c_bar [:, k ], degree - 1 ) return c","title":"cheb_get_coefficients_2d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_get_nodes_1d","text":"get the Chebyshev nodes and weights on the interval \\([x0, x1]\\) Parameters: Name Type Description Default interval Interval the Interval \\([x_0, x_1]\\) required degree int the degree of the highest Chebyshev polynomial required Returns: Type Description TwoArrays two degree -vectors of Chebyshev nodes and weights on the interval \\([x_0, x_1]\\) TwoArrays so that g(nodes) @ weights approximates the unweighted integral of \\(g(x)\\) on \\([x_0,x_1]\\) Source code in bs_python_utils/chebyshev.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def cheb_get_nodes_1d ( interval : Interval , degree : int ) -> TwoArrays : \"\"\"get the Chebyshev nodes and weights on the interval $[x0, x1]$ Args: interval: the Interval $[x_0, x_1]$ degree: the degree of the highest Chebyshev polynomial Returns: two `degree`-vectors of Chebyshev nodes and weights on the interval $[x_0, x_1]$ so that `g(nodes) @ weights` approximates the unweighted integral of $g(x)$ on $[x_0,x_1]$ \"\"\" nodes1m1 , weights1m1 = ncheb . chebgauss ( degree ) # om=n [-1, 1] x0 , x1 = interval . bounds () nodes = move_from1m1 ( nodes1m1 , interval ) weights = ( x1 - x0 ) * weights1m1 * np . sqrt ( 1.0 - nodes1m1 * nodes1m1 ) / 2.0 return cast ( np . ndarray , nodes ), cast ( np . ndarray , weights )","title":"cheb_get_nodes_1d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_get_nodes_2d","text":"get the Chebyshev nodes and weights on a rectangle Parameters: Name Type Description Default rectangle Rectangle the Rectangle required degree int the degree of the highest Chebyshev polynomial in each dimension required Returns: Type Description TwoArrays two \\(( ext{degree}^2, 2)\\) `-matrices of Chebyshev nodes and weights TwoArrays on the rectangle \\([x0, x1] imes [y0, y1]\\) Source code in bs_python_utils/chebyshev.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def cheb_get_nodes_2d ( rectangle : Rectangle , degree : int ) -> TwoArrays : \"\"\"get the Chebyshev nodes and weights on a rectangle Args: rectangle: the Rectangle degree: the degree of the highest Chebyshev polynomial in each dimension Returns: two $(\\text{degree}^2, 2)$`-matrices of Chebyshev nodes and weights on the rectangle $[x0, x1]\\times [y0, y1]$ \"\"\" nodes1d_x , weights1d_x = cheb_get_nodes_1d ( rectangle . x_interval , degree ) nodes1d_y , weights1d_y = cheb_get_nodes_1d ( rectangle . y_interval , degree ) nodes2d = bsgrid ( nodes1d_x , nodes1d_y ) weights2d = bsgrid ( weights1d_x , weights1d_y ) return nodes2d , weights2d [:, 0 ] * weights2d [:, 1 ]","title":"cheb_get_nodes_2d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_integrate_from_coeffs_1d","text":"integrate a function on an interval using the coefficients of its Chebyshev expansion Parameters: Name Type Description Default c np . ndarray the Chebyshev coefficients for fun required interval Interval the Interval required Returns: Type Description float the value of the integral over the interval Source code in bs_python_utils/chebyshev.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def cheb_integrate_from_coeffs_1d ( c : np . ndarray , interval : Interval ) -> float : \"\"\"integrate a function on an interval using the coefficients of its Chebyshev expansion Args: c: the Chebyshev coefficients for `fun` interval: the Interval Returns: the value of the integral over the interval \"\"\" x0 , x1 = interval . bounds () Mc = check_vector ( c ) k_even = slice ( 0 , Mc , 2 ) k_vals = np . arange ( 0 , Mc , 2 ) val_integral = np . sum ( c [ k_even ] / ( 1.0 - k_vals * k_vals )) * ( x1 - x0 ) return cast ( float , val_integral )","title":"cheb_integrate_from_coeffs_1d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_integrate_from_coeffs_2d","text":"integrate a function on an interval using the coefficients of its Chebyshev expansion Parameters: Name Type Description Default c np . ndarray the Chebyshev coefficients for fun required rectangle Rectangle the Rectangle required Returns: Type Description float the value of the integral over the interval Source code in bs_python_utils/chebyshev.py 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def cheb_integrate_from_coeffs_2d ( c : np . ndarray , rectangle : Rectangle ) -> float : \"\"\"integrate a function on an interval using the coefficients of its Chebyshev expansion Args: c: the Chebyshev coefficients for `fun` rectangle: the Rectangle Returns: the value of the integral over the interval \"\"\" x0 , x1 = rectangle . x_interval . bounds () y0 , y1 = rectangle . y_interval . bounds () M = check_square ( c ) k_even = slice ( 0 , M , 2 ) k_vals = np . arange ( 0 , M , 2 ) denom = 1.0 - k_vals * k_vals val_integral = cast ( float , np . sum ( c [ k_even , k_even ] / np . outer ( denom , denom ))) return cast ( float , val_integral * ( x1 - x0 ) * ( y1 - y0 ))","title":"cheb_integrate_from_coeffs_2d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_integrate_from_nodes_1d","text":"integrate a function given its values at the Chebyshev nodes Parameters: Name Type Description Default vals_at_nodes np . ndarray the values of the function at these nodes required weights np . ndarray the Chebyshev nodes required Returns: Type Description float the value of the integral Notes this is much less precise than cheb_integrate_from_coeffs_1d Source code in bs_python_utils/chebyshev.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def cheb_integrate_from_nodes_1d ( vals_at_nodes : np . ndarray , weights : np . ndarray , ) -> float : \"\"\"integrate a function given its values at the Chebyshev nodes Args: vals_at_nodes: the values of the function at these nodes weights: the Chebyshev nodes Returns: the value of the integral Notes: this is much less precise than `cheb_integrate_from_coeffs_1d` \"\"\" Mv = check_vector ( vals_at_nodes ) Mw = check_vector ( weights ) if Mv != Mw : bs_error_abort ( f \"weights and vals_at_nodes must have the same length, not { Mw } and { Mv } \" ) return cast ( float , weights @ vals_at_nodes )","title":"cheb_integrate_from_nodes_1d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_integrate_from_nodes_2d","text":"integrate a function given its values at the Chebyshev nodes Parameters: Name Type Description Default vals_at_nodes np . ndarray the values of the function on the grid of 2d nodes required weights np . ndarray the Chebyshev weights in 2d required Returns: Type Description float the value of the integral Warning this is much less precise than cheb_integrate_from_coeffs_2d Source code in bs_python_utils/chebyshev.py 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 def cheb_integrate_from_nodes_2d ( vals_at_nodes : np . ndarray , weights : np . ndarray , ) -> float : \"\"\"integrate a function given its values at the Chebyshev nodes Args: vals_at_nodes: the values of the function on the grid of 2d nodes weights: the Chebyshev weights in 2d Returns: the value of the integral Warning: this is much less precise than `cheb_integrate_from_coeffs_2d` \"\"\" Mv = check_vector ( vals_at_nodes ) Mw = check_vector ( weights ) if Mv != Mw : bs_error_abort ( f \"weights and vals_at_nodes must have the same length, not { Mw } and { Mv } \" ) return cast ( float , vals_at_nodes @ weights )","title":"cheb_integrate_from_nodes_2d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_integrate_from_nodes_4d","text":"integrate a function on the square of a rectangle given its values at the 4d Chebyshev nodes Parameters: Name Type Description Default vals_at_nodes4d np . ndarray the values of the function on the square of the grid of 2d nodes, an \\((M^2, M^2)\\) matrix required weights2d np . ndarray the Chebyshev weights on the rectangular grid, an \\(M^2\\) -vector required Returns: Type Description float the value of the integral Warning it would be better to have a cheb_integrate_from_coeffs_4d Source code in bs_python_utils/chebyshev.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 def cheb_integrate_from_nodes_4d ( vals_at_nodes4d : np . ndarray , weights2d : np . ndarray ) -> float : \"\"\"integrate a function on the square of a rectangle given its values at the 4d Chebyshev nodes Args: vals_at_nodes4d: the values of the function on the square of the grid of 2d nodes, an $(M^2, M^2)$ matrix weights2d: the Chebyshev weights on the rectangular grid, an $M^2$-vector Returns: the value of the integral Warning: it would be better to have a `cheb_integrate_from_coeffs_4d` \"\"\" Mv2 = check_square ( vals_at_nodes4d ) Mw2 = check_vector ( weights2d ) if Mv2 != Mw2 : bs_error_abort ( \"weights2d and vals_at_nodes4d should have the same number of rows, not\" f \" { Mv2 } and { Mw2 } \" ) return cast ( float , weights2d @ ( vals_at_nodes4d @ weights2d ))","title":"cheb_integrate_from_nodes_4d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_interp_1d","text":"interpolate a function on on interval using Chebyshev polynomials Parameters: Name Type Description Default x_vals np . ndarray the values at which to interpolate required interval Interval the Interval required c np . ndarray | None the Chebyshev coefficients for fun , if already known; otherwise we compute them None fun ArrayFunctionOfArray | None the function to interpolate None degree int | None the degree of the Chebyshev expansion None Notes fun and degree are required if c is not provided Returns: Type Description TwoArrays the values of the interpolation at x_vals and the Chebyshev coefficients c Source code in bs_python_utils/chebyshev.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def cheb_interp_1d ( x_vals : np . ndarray , interval : Interval , c : np . ndarray | None = None , fun : ArrayFunctionOfArray | None = None , degree : int | None = None , ) -> TwoArrays : \"\"\"interpolate a function on on interval using Chebyshev polynomials Args: x_vals: the values at which to interpolate interval: the Interval c: the Chebyshev coefficients for `fun`, if already known; otherwise we compute them fun: the function to interpolate degree: the degree of the Chebyshev expansion Notes: `fun` and `degree` are required if `c` is not provided Returns: the values of the interpolation at `x_vals` and the Chebyshev coefficients `c` \"\"\" if c is None : if degree is None or fun is None : bs_error_abort ( \"since c is not provided, fun and degree must be.\" ) degree = cast ( int , degree ) fun = cast ( ArrayFunctionOfArray , fun ) c = cheb_get_coefficients_1d ( fun , interval , degree ) y_vals = ncheb . chebval ( move_to1m1 ( x_vals , interval ), c ) return y_vals , c","title":"cheb_interp_1d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_interp_1d_from_nodes","text":"interpolate \\(f(x)\\) given the values \\(f(x_m)\\) for \\(m=1,\\ldots,M^2\\) at the Chebyshev nodes on an intervak Parameters: Name Type Description Default f_vals_at_nodes np . ndarray an \\(M^2\\) vector of values \\(f(x_m)\\) required x np . ndarray a scalar where we want \\(f(x)\\) required interval Interval | None the interval on which the function acts; by default, \\([0,1]\\) None Returns: Type Description float the interpolated value of \\(f(x)\\) . Source code in bs_python_utils/chebyshev.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def cheb_interp_1d_from_nodes ( f_vals_at_nodes : np . ndarray , x : np . ndarray , interval : Interval | None = None ) -> float : \"\"\"interpolate $f(x)$ given the values $f(x_m)$ for $m=1,\\\\ldots,M^2$ at the Chebyshev nodes on an intervak Args: f_vals_at_nodes: an $M^2$ vector of values $f(x_m)$ x: a scalar where we want $f(x)$ interval: the interval on which the function acts; by default, $[0,1]$ Returns: the interpolated value of $f(x)$. \"\"\" if interval is None : interval = Interval ( x0 = 0.0 , x1 = 1.0 ) # we need the nodes on $[-1, 1]$ interval_1m1 = Interval ( x0 =- 1.0 , x1 = 1.0 ) degree = f_vals_at_nodes . size nodes1m1 , _ = cheb_get_nodes_1d ( interval_1m1 , degree ) coeffs_f = ncheb . chebfit ( nodes1m1 , f_vals_at_nodes , degree - 1 ) f_x , _ = cheb_interp_1d ( x , interval , c = coeffs_f ) return cast ( float , f_x )","title":"cheb_interp_1d_from_nodes()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_interp_2d","text":"interpolate a function on a rectangle using Chebyshev polynomials Parameters: Name Type Description Default xy_vals np . ndarray the values at which to interpolate, an (n,2) matrix or a 2-vector required rectangle Rectangle the Rectangle required c np . ndarray | None the Chebyshev coefficients for fun , if already known; otherwise we compute them None fun ArrayFunctionOfArray | None the function to interpolate None degree int | None the degree of the Chebyshev expansion None vals_at_nodes np . ndarray | None the values on the grid, if precomputed None Notes degree is required if c is not provided, as well as either fun or vals_at_nodes Returns: Type Description TwoArrays | tuple [ float , np . ndarray ] the n -vector of values of the interpolation at xy_vals and the Chebyshev coefficients c Source code in bs_python_utils/chebyshev.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def cheb_interp_2d ( xy_vals : np . ndarray , rectangle : Rectangle , c : np . ndarray | None = None , fun : ArrayFunctionOfArray | None = None , degree : int | None = None , vals_at_nodes : np . ndarray | None = None , ) -> TwoArrays | tuple [ float , np . ndarray ]: \"\"\"interpolate a function on a rectangle using Chebyshev polynomials Args: xy_vals: the values at which to interpolate, an `(n,2)` matrix or a 2-vector rectangle: the Rectangle c: the Chebyshev coefficients for `fun`, if already known; otherwise we compute them fun: the function to interpolate degree: the degree of the Chebyshev expansion vals_at_nodes: the values on the grid, if precomputed Notes: `degree` is required if `c` is not provided, as well as either `fun` or `vals_at_nodes` Returns: the `n`-vector of values of the interpolation at `xy_vals` and the Chebyshev coefficients `c` \"\"\" if c is None : if degree is None : bs_error_abort ( \"either c or degree must be provided\" ) degree = cast ( int , degree ) c = cheb_get_coefficients_2d ( rectangle , degree , vals_at_nodes = vals_at_nodes , fun = fun ) # transform xy_vals to $[-1,1]\\times [-1,1]$ xy_vals1 = np . zeros_like ( xy_vals ) if xy_vals . ndim == 2 : xy_vals1 [:, 0 ] = move_to1m1 ( xy_vals [:, 0 ], rectangle . x_interval ) xy_vals1 [:, 1 ] = move_to1m1 ( xy_vals [:, 1 ], rectangle . y_interval ) n_vals = xy_vals . shape [ 0 ] deg = c . shape [ 0 ] c2 = np . zeros (( deg , n_vals )) for k , c_k in enumerate ( c ): c2 [ k , :] = ncheb . chebval ( xy_vals1 [:, 1 ], c_k ) f_vals = np . zeros ( n_vals ) for i in range ( n_vals ): f_vals [ i ] = ncheb . chebval ( xy_vals1 [ i , 0 ], c2 [:, i ]) else : xy_vals1 [ 0 ] = move_to1m1 ( xy_vals [ 0 ], rectangle . x_interval ) xy_vals1 [ 1 ] = move_to1m1 ( xy_vals [ 1 ], rectangle . y_interval ) deg = c . shape [ 0 ] c2 = np . zeros ( deg ) for k , c_k in enumerate ( c ): c2 [ k ] = ncheb . chebval ( xy_vals1 [ 1 ], c_k ) f_vals = ncheb . chebval ( xy_vals1 [ 0 ], c2 ) return f_vals , c","title":"cheb_interp_2d()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.cheb_interp_2d_from_nodes","text":"interpolate \\(f(x)\\) given the values \\(f(x_m)\\) for \\(m=1,\\ldots,M^2\\) at the Chebyshev nodes on a rectangle Parameters: Name Type Description Default f_vals_at_nodes np . ndarray an \\(M^2\\) vector of values \\(f(x_m)\\) required x np . ndarray a 2-vector where we want \\(f(x)\\) required rectangle Rectangle | None the rectangle on which the function acts; by default, \\([0,1]^2\\) None Returns: Type Description float the interpolated value of \\(f(x)\\) . Source code in bs_python_utils/chebyshev.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 def cheb_interp_2d_from_nodes ( f_vals_at_nodes : np . ndarray , x : np . ndarray , rectangle : Rectangle | None = None ) -> float : \"\"\"interpolate $f(x)$ given the values $f(x_m)$ for $m=1,\\\\ldots,M^2$ at the Chebyshev nodes on a rectangle Args: f_vals_at_nodes: an $M^2$ vector of values $f(x_m)$ x: a 2-vector where we want $f(x)$ rectangle: the rectangle on which the function acts; by default, $[0,1]^2$ Returns: the interpolated value of $f(x)$. \"\"\" if rectangle is None : interval01 = Interval ( x0 = 0.0 , x1 = 1.0 ) rectangle = Rectangle ( x_interval = interval01 , y_interval = interval01 ) degree = round ( sqrt ( f_vals_at_nodes . size )) coeffs_f = cheb_get_coefficients_2d ( rectangle , degree , vals_at_nodes = f_vals_at_nodes ) f_x , _ = cheb_interp_2d ( x , rectangle , c = coeffs_f ) return cast ( float , f_x )","title":"cheb_interp_2d_from_nodes()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.move_from1m1","text":"get the position of t in \\([-1,1]\\) and move it in the same position in interval Parameters: Name Type Description Default t FloatOrArray position(s) within ` \\([-1,1]\\) required interval Interval the Interval required Returns: Type Description FloatOrArray x rescaled to \\([-1,1]\\) Source code in bs_python_utils/chebyshev.py 65 66 67 68 69 70 71 72 73 74 75 76 def move_from1m1 ( t : FloatOrArray , interval : Interval ) -> FloatOrArray : \"\"\"get the position of `t` in $[-1,1]$ and move it in the same position in `interval` Args: t: position(s) within `$[-1,1]$ interval: the Interval Returns: `x` rescaled to $[-1,1]$ \"\"\" x0 , x1 = interval . bounds () return cast ( FloatOrArray , (( x1 - x0 ) * t + ( x0 + x1 )) / 2.0 )","title":"move_from1m1()"},{"location":"chebyshev.html#bs_python_utils.chebyshev.move_to1m1","text":"get the position of x in interval and move it to the same position in \\([-1,1]\\) Parameters: Name Type Description Default x FloatOrArray position(s) within interval required interval Interval the Interval required Returns: Type Description FloatOrArray x rescaled to \\([-1,1]\\) Source code in bs_python_utils/chebyshev.py 79 80 81 82 83 84 85 86 87 88 89 90 def move_to1m1 ( x : FloatOrArray , interval : Interval ) -> FloatOrArray : \"\"\"get the position of `x` in `interval` and move it to the same position in $[-1,1]$ Args: x: position(s) within `interval` interval: the Interval Returns: `x` rescaled to $[-1,1]$ \"\"\" x0 , x1 = interval . bounds () return cast ( FloatOrArray , ( 2.0 * x - ( x0 + x1 )) / ( x1 - x0 ))","title":"move_to1m1()"},{"location":"distance_covariances.html","text":"distance_covariances module \u00b6 Distance covariance and partial distance covariance \u00e0 la Szekely and Rizzo; evaluation and tests of independence and conditional independence: DcovResults , PdcovResults : classes for distance covariances dcov_dcor : `evaluates the distance covariance and correlation of two random variables pdcov_pdcor : evaluates the partial distance covariance and correlation of X and Y given Z pvalue_dcov : test of no dependence between X and Y given Z . dcov_dcor ( X , Y , unbiased = False ) \u00b6 evaluate the distance covariance and correlation of X and Y Parameters: Name Type Description Default X np . ndarray n observations of a random variable or vector required Y np . ndarray n observations of a random variable or vector required unbiased bool if True , we use the Szekely and Rizzo 2014 formula False Returns: Type Description DcovResults dCov^2(X,Y) and dCor^2(X,Y) Source code in bs_python_utils/distance_covariances.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def dcov_dcor ( X : np . ndarray , Y : np . ndarray , unbiased : bool = False ) -> DcovResults : \"\"\" evaluate the distance covariance and correlation of `X` and `Y` Args: X: `n` observations of a random variable or vector Y: `n` observations of a random variable or vector unbiased: if `True`, we use the Szekely and Rizzo 2014 formula Returns: `dCov^2(X,Y)` and `dCor^2(X,Y)` \"\"\" X_dist = _compute_distances ( X ) n = X_dist . shape [ 0 ] X_dd = _double_decenter ( X_dist , unbiased ) Y_dist = _compute_distances ( Y ) Y_dd = _double_decenter ( Y_dist , unbiased ) dcov2 = _dcov_prod ( X_dd , Y_dd , unbiased ) dcor2 = dcov2 / sqrt ( _dcov_prod ( X_dd , X_dd , unbiased ) * _dcov_prod ( Y_dd , Y_dd , unbiased ) ) return DcovResults ( dcov = dcov2 , dcor = dcor2 , X_dd = X_dd , Y_dd = Y_dd , unbiased = unbiased , dcov_stat = n * dcov2 , ) pdcov_pdcor ( X , Y , Z ) \u00b6 evaluate the partial distance covariance and correlation of X and Y given Z Parameters: Name Type Description Default X np . ndarray n observations of a random variable or vector required Y np . ndarray n observations of a random variable or vector required Z np . ndarray n observations of a random variable or vector required Returns: Type Description PdcovResults a PdcovResults instance Source code in bs_python_utils/distance_covariances.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def pdcov_pdcor ( X : np . ndarray , Y : np . ndarray , Z : np . ndarray ) -> PdcovResults : \"\"\" evaluate the partial distance covariance and correlation of `X` and `Y` given `Z` Args: X: `n` observations of a random variable or vector Y: `n` observations of a random variable or vector Z: `n` observations of a random variable or vector Returns: a `PdcovResults` instance \"\"\" unbiased = True X_dist = _compute_distances ( X ) X_dd = _double_decenter ( X_dist , unbiased ) Y_dist = _compute_distances ( Y ) Y_dd = _double_decenter ( Y_dist , unbiased ) Z_dist = _compute_distances ( Z ) Z_dd = _double_decenter ( Z_dist , unbiased ) C_XX = _dcov_prod ( X_dd , X_dd , unbiased ) C_XY = _dcov_prod ( X_dd , Y_dd , unbiased ) C_YY = _dcov_prod ( Y_dd , Y_dd , unbiased ) C_XZ = _dcov_prod ( X_dd , Z_dd , unbiased ) C_YZ = _dcov_prod ( Y_dd , Z_dd , unbiased ) C_ZZ = _dcov_prod ( Z_dd , Z_dd , unbiased ) pdcov = C_XY - ( C_XZ * C_YZ ) / C_ZZ pdcor = pdcov / sqrt (( C_XX - C_XZ * C_XZ / C_ZZ ) * ( C_YY - C_YZ * C_YZ / C_ZZ )) n = X . shape [ 0 ] return PdcovResults ( pdcov = pdcov , pdcor = pdcor , pdcov_stat = n * pdcov , X_dd = X_dd , Y_dd = Y_dd , Z_dd = Z_dd ) pvalue_dcov ( dcov_results , ndraws = 199 ) \u00b6 test of no dependence between X and Y given Z Parameters: Name Type Description Default dcov_results DcovResults results from dcov_dcor required ndraws int the number of draws we use 199 Returns: Type Description float the bootstrapped p-value of the test Source code in bs_python_utils/distance_covariances.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def pvalue_dcov ( dcov_results : DcovResults , ndraws : int = 199 ) -> float : \"\"\" test of no dependence between `X` and `Y` given `Z` Args: dcov_results: results from `dcov_dcor` ndraws: the number of draws we use Returns: the bootstrapped p-value of the test \"\"\" X_dd = dcov_results . X_dd Y_dd = dcov_results . Y_dd dcov_stat = dcov_results . dcov_stat unbiased = dcov_results . unbiased dcov_stats_boot = _dcov_bootstrap ( X_dd , Y_dd , unbiased , ndraws ) sum_small = cast ( int , np . sum ( dcov_stat < dcov_stats_boot )) return ( 1.0 + sum_small ) / ( 1.0 + ndraws ) pvalue_pdcov ( pdcov_results , ndraws = 199 ) \u00b6 test of no dependence between X and Y given Z Parameters: Name Type Description Default pdcov_results PdcovResults the results of pdcov_pdcor required ndraws int the number of draws we use 199 Returns: Type Description float the bootstrapped p-value of the test Source code in bs_python_utils/distance_covariances.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def pvalue_pdcov ( pdcov_results : PdcovResults , ndraws : int = 199 ) -> float : \"\"\" test of no dependence between `X` and `Y` given `Z` Args: pdcov_results: the results of `pdcov_pdcor` ndraws: the number of draws we use Returns: the bootstrapped p-value of the test \"\"\" X_dd = pdcov_results . X_dd Y_dd = pdcov_results . Y_dd Z_dd = pdcov_results . Z_dd pdcov_stat = pdcov_results . pdcov_stat pdcov_stats_boot = _pdcovs_bootstrap ( X_dd , Y_dd , Z_dd , ndraws ) sum_small = cast ( int , np . sum ( pdcov_stat < pdcov_stats_boot )) return ( 1.0 + sum_small ) / ( 1.0 + ndraws )","title":"nonlinear dependence"},{"location":"distance_covariances.html#distance_covariances-module","text":"Distance covariance and partial distance covariance \u00e0 la Szekely and Rizzo; evaluation and tests of independence and conditional independence: DcovResults , PdcovResults : classes for distance covariances dcov_dcor : `evaluates the distance covariance and correlation of two random variables pdcov_pdcor : evaluates the partial distance covariance and correlation of X and Y given Z pvalue_dcov : test of no dependence between X and Y given Z .","title":"distance_covariances module"},{"location":"distance_covariances.html#bs_python_utils.distance_covariances.dcov_dcor","text":"evaluate the distance covariance and correlation of X and Y Parameters: Name Type Description Default X np . ndarray n observations of a random variable or vector required Y np . ndarray n observations of a random variable or vector required unbiased bool if True , we use the Szekely and Rizzo 2014 formula False Returns: Type Description DcovResults dCov^2(X,Y) and dCor^2(X,Y) Source code in bs_python_utils/distance_covariances.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def dcov_dcor ( X : np . ndarray , Y : np . ndarray , unbiased : bool = False ) -> DcovResults : \"\"\" evaluate the distance covariance and correlation of `X` and `Y` Args: X: `n` observations of a random variable or vector Y: `n` observations of a random variable or vector unbiased: if `True`, we use the Szekely and Rizzo 2014 formula Returns: `dCov^2(X,Y)` and `dCor^2(X,Y)` \"\"\" X_dist = _compute_distances ( X ) n = X_dist . shape [ 0 ] X_dd = _double_decenter ( X_dist , unbiased ) Y_dist = _compute_distances ( Y ) Y_dd = _double_decenter ( Y_dist , unbiased ) dcov2 = _dcov_prod ( X_dd , Y_dd , unbiased ) dcor2 = dcov2 / sqrt ( _dcov_prod ( X_dd , X_dd , unbiased ) * _dcov_prod ( Y_dd , Y_dd , unbiased ) ) return DcovResults ( dcov = dcov2 , dcor = dcor2 , X_dd = X_dd , Y_dd = Y_dd , unbiased = unbiased , dcov_stat = n * dcov2 , )","title":"dcov_dcor()"},{"location":"distance_covariances.html#bs_python_utils.distance_covariances.pdcov_pdcor","text":"evaluate the partial distance covariance and correlation of X and Y given Z Parameters: Name Type Description Default X np . ndarray n observations of a random variable or vector required Y np . ndarray n observations of a random variable or vector required Z np . ndarray n observations of a random variable or vector required Returns: Type Description PdcovResults a PdcovResults instance Source code in bs_python_utils/distance_covariances.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def pdcov_pdcor ( X : np . ndarray , Y : np . ndarray , Z : np . ndarray ) -> PdcovResults : \"\"\" evaluate the partial distance covariance and correlation of `X` and `Y` given `Z` Args: X: `n` observations of a random variable or vector Y: `n` observations of a random variable or vector Z: `n` observations of a random variable or vector Returns: a `PdcovResults` instance \"\"\" unbiased = True X_dist = _compute_distances ( X ) X_dd = _double_decenter ( X_dist , unbiased ) Y_dist = _compute_distances ( Y ) Y_dd = _double_decenter ( Y_dist , unbiased ) Z_dist = _compute_distances ( Z ) Z_dd = _double_decenter ( Z_dist , unbiased ) C_XX = _dcov_prod ( X_dd , X_dd , unbiased ) C_XY = _dcov_prod ( X_dd , Y_dd , unbiased ) C_YY = _dcov_prod ( Y_dd , Y_dd , unbiased ) C_XZ = _dcov_prod ( X_dd , Z_dd , unbiased ) C_YZ = _dcov_prod ( Y_dd , Z_dd , unbiased ) C_ZZ = _dcov_prod ( Z_dd , Z_dd , unbiased ) pdcov = C_XY - ( C_XZ * C_YZ ) / C_ZZ pdcor = pdcov / sqrt (( C_XX - C_XZ * C_XZ / C_ZZ ) * ( C_YY - C_YZ * C_YZ / C_ZZ )) n = X . shape [ 0 ] return PdcovResults ( pdcov = pdcov , pdcor = pdcor , pdcov_stat = n * pdcov , X_dd = X_dd , Y_dd = Y_dd , Z_dd = Z_dd )","title":"pdcov_pdcor()"},{"location":"distance_covariances.html#bs_python_utils.distance_covariances.pvalue_dcov","text":"test of no dependence between X and Y given Z Parameters: Name Type Description Default dcov_results DcovResults results from dcov_dcor required ndraws int the number of draws we use 199 Returns: Type Description float the bootstrapped p-value of the test Source code in bs_python_utils/distance_covariances.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def pvalue_dcov ( dcov_results : DcovResults , ndraws : int = 199 ) -> float : \"\"\" test of no dependence between `X` and `Y` given `Z` Args: dcov_results: results from `dcov_dcor` ndraws: the number of draws we use Returns: the bootstrapped p-value of the test \"\"\" X_dd = dcov_results . X_dd Y_dd = dcov_results . Y_dd dcov_stat = dcov_results . dcov_stat unbiased = dcov_results . unbiased dcov_stats_boot = _dcov_bootstrap ( X_dd , Y_dd , unbiased , ndraws ) sum_small = cast ( int , np . sum ( dcov_stat < dcov_stats_boot )) return ( 1.0 + sum_small ) / ( 1.0 + ndraws )","title":"pvalue_dcov()"},{"location":"distance_covariances.html#bs_python_utils.distance_covariances.pvalue_pdcov","text":"test of no dependence between X and Y given Z Parameters: Name Type Description Default pdcov_results PdcovResults the results of pdcov_pdcor required ndraws int the number of draws we use 199 Returns: Type Description float the bootstrapped p-value of the test Source code in bs_python_utils/distance_covariances.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def pvalue_pdcov ( pdcov_results : PdcovResults , ndraws : int = 199 ) -> float : \"\"\" test of no dependence between `X` and `Y` given `Z` Args: pdcov_results: the results of `pdcov_pdcor` ndraws: the number of draws we use Returns: the bootstrapped p-value of the test \"\"\" X_dd = pdcov_results . X_dd Y_dd = pdcov_results . Y_dd Z_dd = pdcov_results . Z_dd pdcov_stat = pdcov_results . pdcov_stat pdcov_stats_boot = _pdcovs_bootstrap ( X_dd , Y_dd , Z_dd , ndraws ) sum_small = cast ( int , np . sum ( pdcov_stat < pdcov_stats_boot )) return ( 1.0 + sum_small ) / ( 1.0 + ndraws )","title":"pvalue_pdcov()"},{"location":"pandas_utils.html","text":"pandas_utils module \u00b6 Utility functions for pandas: bspd_print : pretty-prints a data frame bspd_cross_products : generates cross-products of variables bspd_statsdf : makes a dataframe with columns from an array specified column names. bspd_prepareplot : prepares a dataframe for plotting (very specific). bspd_cross_products ( df , l1 , l2 = None , with_squares = True ) \u00b6 Returns a DataFrame with cross-products of the variables of df whose names are in l1 and l2 . Parameters: Name Type Description Default df pd . DataFrame any data frame required l1 list [ str ] a list of names of variables that belong to df required l2 list [ str ] | None ibidem; l1 by default None with_squares bool | None if False , we drop the squares. True by default. True Returns: Type Description pd . DataFrame the data frame of cross-products with concatenated names. Source code in bs_python_utils/pandas_utils.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def bspd_cross_products ( df : pd . DataFrame , l1 : list [ str ], l2 : list [ str ] | None = None , with_squares : bool | None = True , ) -> pd . DataFrame : \"\"\"Returns a DataFrame with cross-products of the variables of `df` whose names are in `l1` and `l2`. Args: df: any data frame l1: a list of names of variables that belong to `df` l2: ibidem; `l1` by default with_squares: if `False`, we drop the squares. `True` by default. Returns: the data frame of cross-products with concatenated names. \"\"\" lp2 = l1 if l2 is None else l2 l12 = list ( product ( l1 , lp2 )) cross_pairs = [[ x [ 0 ], x [ 1 ]] for x in l12 if x [ 0 ] != x [ 1 ]] unique_pairs = [] for _i , c in enumerate ( cross_pairs ): print ( c ) c_ordered = c if c [ 0 ] < c [ 1 ] else list ( reversed ( c )) print ( c_ordered ) if c_ordered not in unique_pairs : unique_pairs . append ( c_ordered ) print ( unique_pairs ) col_names = sorted ([( x [ 0 ], x [ 1 ], f \" { x [ 0 ] } * { x [ 1 ] } \" ) for x in unique_pairs ]) if with_squares : col_names_squares = sorted ( [( x [ 0 ], x [ 1 ], f \" { x [ 0 ] } **2\" ) for x in l12 if x [ 0 ] == x [ 1 ]] ) col_names += col_names_squares df_cprods = pd . DataFrame ( { col_name : df [ x0 ] * df [ x1 ] for ( x0 , x1 , col_name ) in col_names } ) return df_cprods bspd_prepareplot ( df ) \u00b6 Parameters: Name Type Description Default df pd . DataFrame any dataframe whose column names either all end in '_n' for n an integer, or none does required Returns: Type Description pd . DataFrame a properly melted dataframe for plotting, with columns 'Sample', 'Statistic', 'Value', pd . DataFrame and 'Group' if there are several integers. Source code in bs_python_utils/pandas_utils.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def bspd_prepareplot ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Args: df: any dataframe whose column names either all end in '_n' for n an integer, or none does Returns: a properly melted dataframe for plotting, with columns 'Sample', 'Statistic', 'Value', and 'Group' if there are several integers. \"\"\" # check the names of the columns values_integers = _check_names_n ( df . columns ) n_values_integers = len ( values_integers ) df2 = df . copy () df2 [ \"Sample\" ] = np . arange ( df . shape [ 0 ]) dfm = pd . melt ( df2 , id_vars = \"Sample\" , value_vars = list ( df . columns ), var_name = \"Statistic\" , value_name = \"Value\" , ) if n_values_integers in [ 0 , 1 ]: return dfm else : # at least two different groups of statistics stat_group = dfm [ \"Statistic\" ] . str . split ( \"_\" , n = 1 , expand = True ) dfm . drop ( columns = [ \"Statistic\" ], inplace = True ) dfm [ \"Statistic\" ] = stat_group [ 0 ] dfm [ \"Group\" ] = stat_group [ 1 ] return dfm bspd_print ( df , s = '' , max_rows = None , max_cols = None , precision = None ) \u00b6 Pretty-prints a data frame Parameters: Name Type Description Default df pd . DataFrame any data frame required s str | None an optional title string '' max_rows int | None maximum number of rows to print (all by default) None max_cols int | None maximum number of columns to print (all by default) None precision int | None of numbers. 3 digits by default. None Returns: Type Description None nothing. Source code in bs_python_utils/pandas_utils.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def bspd_print ( df : pd . DataFrame , s : str | None = \"\" , max_rows : int | None = None , max_cols : int | None = None , precision : int | None = None , ) -> None : \"\"\"Pretty-prints a data frame Args: df: any data frame s: an optional title string max_rows: maximum number of rows to print (all by default) max_cols: maximum number of columns to print (all by default) precision: of numbers. 3 digits by default. Returns: nothing. \"\"\" print_stars ( s ) with pd . option_context ( \"display.max_rows\" , max_rows , \"display.max_columns\" , max_cols , \"display.precision\" , precision , ): print ( df ) bspd_statsdf ( T , col_names ) \u00b6 Make a dataframe with columns from the array(s) in T and names from col_names . Parameters: Name Type Description Default T np . ndarray | list [ np . ndarray ] a list of n_T matrices or vectors with N rows, or a matrix or a vector with N rows required col_names str | list [ str ] | list [ str | list [ str ]] a list of n_T name objects; a name object must be a string or a list of strings, with the names for the column(s) of the corresponding T matrix required Returns: Type Description pd . DataFrame a dataframe with the named columns. Source code in bs_python_utils/pandas_utils.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def bspd_statsdf ( T : np . ndarray | list [ np . ndarray ], col_names : str | list [ str ] | list [ str | list [ str ]], ) -> pd . DataFrame : \"\"\" Make a dataframe with columns from the array(s) in `T` and names from `col_names`. Args: T: a list of n_T matrices or vectors with N rows, or a matrix or a vector with N rows col_names: a list of n_T name objects; a name object must be a string or a list of strings, with the names for the column(s) of the corresponding T matrix Returns: a dataframe with the named columns. \"\"\" if isinstance ( T , list ): n_T = len ( T ) _check_colnames ( col_names , n_T ) shape_T = [] for i in range ( n_T ): shape_T . append ( T [ i ] . shape ) set_nrows = { shape_i [ 0 ] for shape_i in shape_T } if len ( set_nrows ) > 1 : bs_error_abort ( \"All T arrays should have the same number of rows.\" ) big_T = T [ 0 ] big_names = _list_str ( col_names [ 0 ], suffix = \"_1\" ) for i in range ( 1 , n_T ): big_T = np . column_stack (( big_T , T [ i ])) big_names . extend ( _list_str ( col_names [ i ], suffix = f \"_ { i + 1 } \" )) df = pd . DataFrame ( big_T , columns = big_names , copy = True ) else : # only one element in T ndims_T = check_vector_or_matrix ( T ) if ndims_T == 1 : if not isinstance ( col_names , str ): bs_error_abort ( f \"T is a vector but col_names is { col_names } \" ) df = pd . DataFrame ( T , columns = [ col_names ], copy = True ) elif ndims_T == 2 : N , K = T . shape K2 = len ( col_names ) if K2 != K : bs_error_abort ( f \"T is { T . shape } but col_names has { K2 } elements\" ) df = pd . DataFrame ( T , columns = col_names , copy = True ) return df","title":"Pandas"},{"location":"pandas_utils.html#pandas_utils-module","text":"Utility functions for pandas: bspd_print : pretty-prints a data frame bspd_cross_products : generates cross-products of variables bspd_statsdf : makes a dataframe with columns from an array specified column names. bspd_prepareplot : prepares a dataframe for plotting (very specific).","title":"pandas_utils module"},{"location":"pandas_utils.html#bs_python_utils.pandas_utils.bspd_cross_products","text":"Returns a DataFrame with cross-products of the variables of df whose names are in l1 and l2 . Parameters: Name Type Description Default df pd . DataFrame any data frame required l1 list [ str ] a list of names of variables that belong to df required l2 list [ str ] | None ibidem; l1 by default None with_squares bool | None if False , we drop the squares. True by default. True Returns: Type Description pd . DataFrame the data frame of cross-products with concatenated names. Source code in bs_python_utils/pandas_utils.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def bspd_cross_products ( df : pd . DataFrame , l1 : list [ str ], l2 : list [ str ] | None = None , with_squares : bool | None = True , ) -> pd . DataFrame : \"\"\"Returns a DataFrame with cross-products of the variables of `df` whose names are in `l1` and `l2`. Args: df: any data frame l1: a list of names of variables that belong to `df` l2: ibidem; `l1` by default with_squares: if `False`, we drop the squares. `True` by default. Returns: the data frame of cross-products with concatenated names. \"\"\" lp2 = l1 if l2 is None else l2 l12 = list ( product ( l1 , lp2 )) cross_pairs = [[ x [ 0 ], x [ 1 ]] for x in l12 if x [ 0 ] != x [ 1 ]] unique_pairs = [] for _i , c in enumerate ( cross_pairs ): print ( c ) c_ordered = c if c [ 0 ] < c [ 1 ] else list ( reversed ( c )) print ( c_ordered ) if c_ordered not in unique_pairs : unique_pairs . append ( c_ordered ) print ( unique_pairs ) col_names = sorted ([( x [ 0 ], x [ 1 ], f \" { x [ 0 ] } * { x [ 1 ] } \" ) for x in unique_pairs ]) if with_squares : col_names_squares = sorted ( [( x [ 0 ], x [ 1 ], f \" { x [ 0 ] } **2\" ) for x in l12 if x [ 0 ] == x [ 1 ]] ) col_names += col_names_squares df_cprods = pd . DataFrame ( { col_name : df [ x0 ] * df [ x1 ] for ( x0 , x1 , col_name ) in col_names } ) return df_cprods","title":"bspd_cross_products()"},{"location":"pandas_utils.html#bs_python_utils.pandas_utils.bspd_prepareplot","text":"Parameters: Name Type Description Default df pd . DataFrame any dataframe whose column names either all end in '_n' for n an integer, or none does required Returns: Type Description pd . DataFrame a properly melted dataframe for plotting, with columns 'Sample', 'Statistic', 'Value', pd . DataFrame and 'Group' if there are several integers. Source code in bs_python_utils/pandas_utils.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def bspd_prepareplot ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Args: df: any dataframe whose column names either all end in '_n' for n an integer, or none does Returns: a properly melted dataframe for plotting, with columns 'Sample', 'Statistic', 'Value', and 'Group' if there are several integers. \"\"\" # check the names of the columns values_integers = _check_names_n ( df . columns ) n_values_integers = len ( values_integers ) df2 = df . copy () df2 [ \"Sample\" ] = np . arange ( df . shape [ 0 ]) dfm = pd . melt ( df2 , id_vars = \"Sample\" , value_vars = list ( df . columns ), var_name = \"Statistic\" , value_name = \"Value\" , ) if n_values_integers in [ 0 , 1 ]: return dfm else : # at least two different groups of statistics stat_group = dfm [ \"Statistic\" ] . str . split ( \"_\" , n = 1 , expand = True ) dfm . drop ( columns = [ \"Statistic\" ], inplace = True ) dfm [ \"Statistic\" ] = stat_group [ 0 ] dfm [ \"Group\" ] = stat_group [ 1 ] return dfm","title":"bspd_prepareplot()"},{"location":"pandas_utils.html#bs_python_utils.pandas_utils.bspd_print","text":"Pretty-prints a data frame Parameters: Name Type Description Default df pd . DataFrame any data frame required s str | None an optional title string '' max_rows int | None maximum number of rows to print (all by default) None max_cols int | None maximum number of columns to print (all by default) None precision int | None of numbers. 3 digits by default. None Returns: Type Description None nothing. Source code in bs_python_utils/pandas_utils.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def bspd_print ( df : pd . DataFrame , s : str | None = \"\" , max_rows : int | None = None , max_cols : int | None = None , precision : int | None = None , ) -> None : \"\"\"Pretty-prints a data frame Args: df: any data frame s: an optional title string max_rows: maximum number of rows to print (all by default) max_cols: maximum number of columns to print (all by default) precision: of numbers. 3 digits by default. Returns: nothing. \"\"\" print_stars ( s ) with pd . option_context ( \"display.max_rows\" , max_rows , \"display.max_columns\" , max_cols , \"display.precision\" , precision , ): print ( df )","title":"bspd_print()"},{"location":"pandas_utils.html#bs_python_utils.pandas_utils.bspd_statsdf","text":"Make a dataframe with columns from the array(s) in T and names from col_names . Parameters: Name Type Description Default T np . ndarray | list [ np . ndarray ] a list of n_T matrices or vectors with N rows, or a matrix or a vector with N rows required col_names str | list [ str ] | list [ str | list [ str ]] a list of n_T name objects; a name object must be a string or a list of strings, with the names for the column(s) of the corresponding T matrix required Returns: Type Description pd . DataFrame a dataframe with the named columns. Source code in bs_python_utils/pandas_utils.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def bspd_statsdf ( T : np . ndarray | list [ np . ndarray ], col_names : str | list [ str ] | list [ str | list [ str ]], ) -> pd . DataFrame : \"\"\" Make a dataframe with columns from the array(s) in `T` and names from `col_names`. Args: T: a list of n_T matrices or vectors with N rows, or a matrix or a vector with N rows col_names: a list of n_T name objects; a name object must be a string or a list of strings, with the names for the column(s) of the corresponding T matrix Returns: a dataframe with the named columns. \"\"\" if isinstance ( T , list ): n_T = len ( T ) _check_colnames ( col_names , n_T ) shape_T = [] for i in range ( n_T ): shape_T . append ( T [ i ] . shape ) set_nrows = { shape_i [ 0 ] for shape_i in shape_T } if len ( set_nrows ) > 1 : bs_error_abort ( \"All T arrays should have the same number of rows.\" ) big_T = T [ 0 ] big_names = _list_str ( col_names [ 0 ], suffix = \"_1\" ) for i in range ( 1 , n_T ): big_T = np . column_stack (( big_T , T [ i ])) big_names . extend ( _list_str ( col_names [ i ], suffix = f \"_ { i + 1 } \" )) df = pd . DataFrame ( big_T , columns = big_names , copy = True ) else : # only one element in T ndims_T = check_vector_or_matrix ( T ) if ndims_T == 1 : if not isinstance ( col_names , str ): bs_error_abort ( f \"T is a vector but col_names is { col_names } \" ) df = pd . DataFrame ( T , columns = [ col_names ], copy = True ) elif ndims_T == 2 : N , K = T . shape K2 = len ( col_names ) if K2 != K : bs_error_abort ( f \"T is { T . shape } but col_names has { K2 } elements\" ) df = pd . DataFrame ( T , columns = col_names , copy = True ) return df","title":"bspd_statsdf()"},{"location":"sklearn_utils.html","text":"sklearn_utils module \u00b6 Contains Lasso scikit-learn utility programs: skl_npreg_lasso : Lasso regression on polynomial interactions of the covariates plot_lasso_path : plots the Lasso coefficient paths. plot_lasso_path ( y , X , eps = 0.001 ) \u00b6 plot Lasso coefficient paths Parameters: Name Type Description Default y np . ndarray shape (nobs) required X np . ndarray shape (nobs, nfeatures) required eps float length of path 0.001 Returns: Type Description None plots the paths. Source code in bs_python_utils/sklearn_utils.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def plot_lasso_path ( y : np . ndarray , X : np . ndarray , eps : float = 1e-3 ) -> None : \"\"\" plot Lasso coefficient paths Args: y: shape `(nobs)` X: shape `(nobs, nfeatures)` eps: length of path Returns: plots the paths. \"\"\" # Compute paths print ( \"Computing regularization path using the lasso...\" ) alphas_lasso , coefs_lasso , _ = lasso_path ( X , y , eps ) plt . clf () # Display results plt . figure ( 1 ) colors = cycle ([ \"b\" , \"r\" , \"g\" , \"c\" , \"k\" ]) neg_log_alphas_lasso = - np . log10 ( alphas_lasso ) for coef_l , c in zip ( coefs_lasso , colors , strict = True ): plt . plot ( neg_log_alphas_lasso , coef_l , c = c ) plt . xlabel ( \"-Log(alpha)\" ) plt . ylabel ( \"coefficients\" ) plt . title ( \"Lasso Paths\" ) plt . axis ( \"tight\" ) plt . show () return skl_npreg_lasso ( y , X , alpha , degree = 4 ) \u00b6 Lasso nonparametric regression of y over polynomials of X Parameters: Name Type Description Default y np . ndarray shape (nobs) required X np . ndarray shape (nobs, nfeatures) required alpha float Lasso penalty parameter required degree int highest total degree 4 Returns: Type Description np . ndarray the (nobs) array E(y\\vert X) over the sample Source code in bs_python_utils/sklearn_utils.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def skl_npreg_lasso ( y : np . ndarray , X : np . ndarray , alpha : float , degree : int = 4 ) -> np . ndarray : \"\"\" Lasso nonparametric regression of `y` over polynomials of `X` Args: y: shape `(nobs)` X: shape `(nobs, nfeatures)` alpha: Lasso penalty parameter degree: highest total degree Returns: the `(nobs)` array `E(y\\\\vert X)` over the sample \"\"\" # first scale the X variables stdsc = StandardScaler () sfit = stdsc . fit ( X ) X_scaled = sfit . transform ( X ) pf = PolynomialFeatures ( degree ) # Create the features and fit X_poly = pf . fit_transform ( X_scaled ) # now run Lasso reg = Lasso ( alpha = alpha ) . fit ( X_poly , y ) expy_X = reg . predict ( X_poly ) return cast ( np . ndarray , expy_X )","title":"Sklearn"},{"location":"sklearn_utils.html#sklearn_utils-module","text":"Contains Lasso scikit-learn utility programs: skl_npreg_lasso : Lasso regression on polynomial interactions of the covariates plot_lasso_path : plots the Lasso coefficient paths.","title":"sklearn_utils module"},{"location":"sklearn_utils.html#bs_python_utils.sklearn_utils.plot_lasso_path","text":"plot Lasso coefficient paths Parameters: Name Type Description Default y np . ndarray shape (nobs) required X np . ndarray shape (nobs, nfeatures) required eps float length of path 0.001 Returns: Type Description None plots the paths. Source code in bs_python_utils/sklearn_utils.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def plot_lasso_path ( y : np . ndarray , X : np . ndarray , eps : float = 1e-3 ) -> None : \"\"\" plot Lasso coefficient paths Args: y: shape `(nobs)` X: shape `(nobs, nfeatures)` eps: length of path Returns: plots the paths. \"\"\" # Compute paths print ( \"Computing regularization path using the lasso...\" ) alphas_lasso , coefs_lasso , _ = lasso_path ( X , y , eps ) plt . clf () # Display results plt . figure ( 1 ) colors = cycle ([ \"b\" , \"r\" , \"g\" , \"c\" , \"k\" ]) neg_log_alphas_lasso = - np . log10 ( alphas_lasso ) for coef_l , c in zip ( coefs_lasso , colors , strict = True ): plt . plot ( neg_log_alphas_lasso , coef_l , c = c ) plt . xlabel ( \"-Log(alpha)\" ) plt . ylabel ( \"coefficients\" ) plt . title ( \"Lasso Paths\" ) plt . axis ( \"tight\" ) plt . show () return","title":"plot_lasso_path()"},{"location":"sklearn_utils.html#bs_python_utils.sklearn_utils.skl_npreg_lasso","text":"Lasso nonparametric regression of y over polynomials of X Parameters: Name Type Description Default y np . ndarray shape (nobs) required X np . ndarray shape (nobs, nfeatures) required alpha float Lasso penalty parameter required degree int highest total degree 4 Returns: Type Description np . ndarray the (nobs) array E(y\\vert X) over the sample Source code in bs_python_utils/sklearn_utils.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def skl_npreg_lasso ( y : np . ndarray , X : np . ndarray , alpha : float , degree : int = 4 ) -> np . ndarray : \"\"\" Lasso nonparametric regression of `y` over polynomials of `X` Args: y: shape `(nobs)` X: shape `(nobs, nfeatures)` alpha: Lasso penalty parameter degree: highest total degree Returns: the `(nobs)` array `E(y\\\\vert X)` over the sample \"\"\" # first scale the X variables stdsc = StandardScaler () sfit = stdsc . fit ( X ) X_scaled = sfit . transform ( X ) pf = PolynomialFeatures ( degree ) # Create the features and fit X_poly = pf . fit_transform ( X_scaled ) # now run Lasso reg = Lasso ( alpha = alpha ) . fit ( X_poly , y ) expy_X = reg . predict ( X_poly ) return cast ( np . ndarray , expy_X )","title":"skl_npreg_lasso()"}]}